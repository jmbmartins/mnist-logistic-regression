{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbmartins/LogisticRegressionProject/blob/main/Pratical_Project_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R0VZWpJll4l2",
        "outputId": "6eb42288-d7cf-4671-e805-6a0398b60565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 10/10000 - Loss: 0.6431\n",
            "Iteration 20/10000 - Loss: 0.5968\n",
            "Iteration 30/10000 - Loss: 0.5587\n",
            "Iteration 40/10000 - Loss: 0.5269\n",
            "Iteration 50/10000 - Loss: 0.5004\n",
            "Iteration 60/10000 - Loss: 0.4781\n",
            "Iteration 70/10000 - Loss: 0.4592\n",
            "Iteration 80/10000 - Loss: 0.4430\n",
            "Iteration 90/10000 - Loss: 0.4291\n",
            "Iteration 100/10000 - Loss: 0.4170\n",
            "Iteration 110/10000 - Loss: 0.4065\n",
            "Iteration 120/10000 - Loss: 0.3973\n",
            "Iteration 130/10000 - Loss: 0.3892\n",
            "Iteration 140/10000 - Loss: 0.3819\n",
            "Iteration 150/10000 - Loss: 0.3755\n",
            "Iteration 160/10000 - Loss: 0.3697\n",
            "Iteration 170/10000 - Loss: 0.3645\n",
            "Iteration 180/10000 - Loss: 0.3598\n",
            "Iteration 190/10000 - Loss: 0.3555\n",
            "Iteration 200/10000 - Loss: 0.3515\n",
            "Iteration 210/10000 - Loss: 0.3479\n",
            "Iteration 220/10000 - Loss: 0.3446\n",
            "Iteration 230/10000 - Loss: 0.3415\n",
            "Iteration 240/10000 - Loss: 0.3387\n",
            "Iteration 250/10000 - Loss: 0.3360\n",
            "Iteration 260/10000 - Loss: 0.3335\n",
            "Iteration 270/10000 - Loss: 0.3312\n",
            "Iteration 280/10000 - Loss: 0.3290\n",
            "Iteration 290/10000 - Loss: 0.3270\n",
            "Iteration 300/10000 - Loss: 0.3250\n",
            "Iteration 310/10000 - Loss: 0.3232\n",
            "Iteration 320/10000 - Loss: 0.3214\n",
            "Iteration 330/10000 - Loss: 0.3197\n",
            "Iteration 340/10000 - Loss: 0.3181\n",
            "Iteration 350/10000 - Loss: 0.3166\n",
            "Iteration 360/10000 - Loss: 0.3151\n",
            "Iteration 370/10000 - Loss: 0.3137\n",
            "Iteration 380/10000 - Loss: 0.3123\n",
            "Iteration 390/10000 - Loss: 0.3110\n",
            "Iteration 400/10000 - Loss: 0.3097\n",
            "Iteration 410/10000 - Loss: 0.3085\n",
            "Iteration 420/10000 - Loss: 0.3073\n",
            "Iteration 430/10000 - Loss: 0.3061\n",
            "Iteration 440/10000 - Loss: 0.3050\n",
            "Iteration 450/10000 - Loss: 0.3039\n",
            "Iteration 460/10000 - Loss: 0.3028\n",
            "Iteration 470/10000 - Loss: 0.3018\n",
            "Iteration 480/10000 - Loss: 0.3007\n",
            "Iteration 490/10000 - Loss: 0.2997\n",
            "Iteration 500/10000 - Loss: 0.2988\n",
            "Iteration 510/10000 - Loss: 0.2978\n",
            "Iteration 520/10000 - Loss: 0.2968\n",
            "Iteration 530/10000 - Loss: 0.2959\n",
            "Iteration 540/10000 - Loss: 0.2950\n",
            "Iteration 550/10000 - Loss: 0.2941\n",
            "Iteration 560/10000 - Loss: 0.2932\n",
            "Iteration 570/10000 - Loss: 0.2923\n",
            "Iteration 580/10000 - Loss: 0.2915\n",
            "Iteration 590/10000 - Loss: 0.2906\n",
            "Iteration 600/10000 - Loss: 0.2898\n",
            "Iteration 610/10000 - Loss: 0.2890\n",
            "Iteration 620/10000 - Loss: 0.2882\n",
            "Iteration 630/10000 - Loss: 0.2874\n",
            "Iteration 640/10000 - Loss: 0.2866\n",
            "Iteration 650/10000 - Loss: 0.2858\n",
            "Iteration 660/10000 - Loss: 0.2850\n",
            "Iteration 670/10000 - Loss: 0.2842\n",
            "Iteration 680/10000 - Loss: 0.2835\n",
            "Iteration 690/10000 - Loss: 0.2827\n",
            "Iteration 700/10000 - Loss: 0.2820\n",
            "Iteration 710/10000 - Loss: 0.2813\n",
            "Iteration 720/10000 - Loss: 0.2805\n",
            "Iteration 730/10000 - Loss: 0.2798\n",
            "Iteration 740/10000 - Loss: 0.2791\n",
            "Iteration 750/10000 - Loss: 0.2784\n",
            "Iteration 760/10000 - Loss: 0.2777\n",
            "Iteration 770/10000 - Loss: 0.2770\n",
            "Iteration 780/10000 - Loss: 0.2763\n",
            "Iteration 790/10000 - Loss: 0.2756\n",
            "Iteration 800/10000 - Loss: 0.2749\n",
            "Iteration 810/10000 - Loss: 0.2743\n",
            "Iteration 820/10000 - Loss: 0.2736\n",
            "Iteration 830/10000 - Loss: 0.2729\n",
            "Iteration 840/10000 - Loss: 0.2723\n",
            "Iteration 850/10000 - Loss: 0.2716\n",
            "Iteration 860/10000 - Loss: 0.2710\n",
            "Iteration 870/10000 - Loss: 0.2703\n",
            "Iteration 880/10000 - Loss: 0.2697\n",
            "Iteration 890/10000 - Loss: 0.2691\n",
            "Iteration 900/10000 - Loss: 0.2684\n",
            "Iteration 910/10000 - Loss: 0.2678\n",
            "Iteration 920/10000 - Loss: 0.2672\n",
            "Iteration 930/10000 - Loss: 0.2666\n",
            "Iteration 940/10000 - Loss: 0.2660\n",
            "Iteration 950/10000 - Loss: 0.2654\n",
            "Iteration 960/10000 - Loss: 0.2648\n",
            "Iteration 970/10000 - Loss: 0.2642\n",
            "Iteration 980/10000 - Loss: 0.2636\n",
            "Iteration 990/10000 - Loss: 0.2630\n",
            "Iteration 1000/10000 - Loss: 0.2624\n",
            "Iteration 1010/10000 - Loss: 0.2618\n",
            "Iteration 1020/10000 - Loss: 0.2613\n",
            "Iteration 1030/10000 - Loss: 0.2607\n",
            "Iteration 1040/10000 - Loss: 0.2601\n",
            "Iteration 1050/10000 - Loss: 0.2596\n",
            "Iteration 1060/10000 - Loss: 0.2590\n",
            "Iteration 1070/10000 - Loss: 0.2584\n",
            "Iteration 1080/10000 - Loss: 0.2579\n",
            "Iteration 1090/10000 - Loss: 0.2573\n",
            "Iteration 1100/10000 - Loss: 0.2568\n",
            "Iteration 1110/10000 - Loss: 0.2562\n",
            "Iteration 1120/10000 - Loss: 0.2557\n",
            "Iteration 1130/10000 - Loss: 0.2552\n",
            "Iteration 1140/10000 - Loss: 0.2546\n",
            "Iteration 1150/10000 - Loss: 0.2541\n",
            "Iteration 1160/10000 - Loss: 0.2536\n",
            "Iteration 1170/10000 - Loss: 0.2531\n",
            "Iteration 1180/10000 - Loss: 0.2525\n",
            "Iteration 1190/10000 - Loss: 0.2520\n",
            "Iteration 1200/10000 - Loss: 0.2515\n",
            "Iteration 1210/10000 - Loss: 0.2510\n",
            "Iteration 1220/10000 - Loss: 0.2505\n",
            "Iteration 1230/10000 - Loss: 0.2500\n",
            "Iteration 1240/10000 - Loss: 0.2495\n",
            "Iteration 1250/10000 - Loss: 0.2490\n",
            "Iteration 1260/10000 - Loss: 0.2485\n",
            "Iteration 1270/10000 - Loss: 0.2480\n",
            "Iteration 1280/10000 - Loss: 0.2475\n",
            "Iteration 1290/10000 - Loss: 0.2470\n",
            "Iteration 1300/10000 - Loss: 0.2466\n",
            "Iteration 1310/10000 - Loss: 0.2461\n",
            "Iteration 1320/10000 - Loss: 0.2456\n",
            "Iteration 1330/10000 - Loss: 0.2451\n",
            "Iteration 1340/10000 - Loss: 0.2447\n",
            "Iteration 1350/10000 - Loss: 0.2442\n",
            "Iteration 1360/10000 - Loss: 0.2437\n",
            "Iteration 1370/10000 - Loss: 0.2433\n",
            "Iteration 1380/10000 - Loss: 0.2428\n",
            "Iteration 1390/10000 - Loss: 0.2424\n",
            "Iteration 1400/10000 - Loss: 0.2419\n",
            "Iteration 1410/10000 - Loss: 0.2415\n",
            "Iteration 1420/10000 - Loss: 0.2410\n",
            "Iteration 1430/10000 - Loss: 0.2406\n",
            "Iteration 1440/10000 - Loss: 0.2401\n",
            "Iteration 1450/10000 - Loss: 0.2397\n",
            "Iteration 1460/10000 - Loss: 0.2392\n",
            "Iteration 1470/10000 - Loss: 0.2388\n",
            "Iteration 1480/10000 - Loss: 0.2384\n",
            "Iteration 1490/10000 - Loss: 0.2379\n",
            "Iteration 1500/10000 - Loss: 0.2375\n",
            "Iteration 1510/10000 - Loss: 0.2371\n",
            "Iteration 1520/10000 - Loss: 0.2367\n",
            "Iteration 1530/10000 - Loss: 0.2362\n",
            "Iteration 1540/10000 - Loss: 0.2358\n",
            "Iteration 1550/10000 - Loss: 0.2354\n",
            "Iteration 1560/10000 - Loss: 0.2350\n",
            "Iteration 1570/10000 - Loss: 0.2346\n",
            "Iteration 1580/10000 - Loss: 0.2342\n",
            "Iteration 1590/10000 - Loss: 0.2338\n",
            "Iteration 1600/10000 - Loss: 0.2334\n",
            "Iteration 1610/10000 - Loss: 0.2330\n",
            "Iteration 1620/10000 - Loss: 0.2326\n",
            "Iteration 1630/10000 - Loss: 0.2322\n",
            "Iteration 1640/10000 - Loss: 0.2318\n",
            "Iteration 1650/10000 - Loss: 0.2314\n",
            "Iteration 1660/10000 - Loss: 0.2310\n",
            "Iteration 1670/10000 - Loss: 0.2306\n",
            "Iteration 1680/10000 - Loss: 0.2302\n",
            "Iteration 1690/10000 - Loss: 0.2298\n",
            "Iteration 1700/10000 - Loss: 0.2295\n",
            "Iteration 1710/10000 - Loss: 0.2291\n",
            "Iteration 1720/10000 - Loss: 0.2287\n",
            "Iteration 1730/10000 - Loss: 0.2283\n",
            "Iteration 1740/10000 - Loss: 0.2280\n",
            "Iteration 1750/10000 - Loss: 0.2276\n",
            "Iteration 1760/10000 - Loss: 0.2272\n",
            "Iteration 1770/10000 - Loss: 0.2268\n",
            "Iteration 1780/10000 - Loss: 0.2265\n",
            "Iteration 1790/10000 - Loss: 0.2261\n",
            "Iteration 1800/10000 - Loss: 0.2258\n",
            "Iteration 1810/10000 - Loss: 0.2254\n",
            "Iteration 1820/10000 - Loss: 0.2250\n",
            "Iteration 1830/10000 - Loss: 0.2247\n",
            "Iteration 1840/10000 - Loss: 0.2243\n",
            "Iteration 1850/10000 - Loss: 0.2240\n",
            "Iteration 1860/10000 - Loss: 0.2236\n",
            "Iteration 1870/10000 - Loss: 0.2233\n",
            "Iteration 1880/10000 - Loss: 0.2229\n",
            "Iteration 1890/10000 - Loss: 0.2226\n",
            "Iteration 1900/10000 - Loss: 0.2223\n",
            "Iteration 1910/10000 - Loss: 0.2219\n",
            "Iteration 1920/10000 - Loss: 0.2216\n",
            "Iteration 1930/10000 - Loss: 0.2212\n",
            "Iteration 1940/10000 - Loss: 0.2209\n",
            "Iteration 1950/10000 - Loss: 0.2206\n",
            "Iteration 1960/10000 - Loss: 0.2202\n",
            "Iteration 1970/10000 - Loss: 0.2199\n",
            "Iteration 1980/10000 - Loss: 0.2196\n",
            "Iteration 1990/10000 - Loss: 0.2193\n",
            "Iteration 2000/10000 - Loss: 0.2189\n",
            "Iteration 2010/10000 - Loss: 0.2186\n",
            "Iteration 2020/10000 - Loss: 0.2183\n",
            "Iteration 2030/10000 - Loss: 0.2180\n",
            "Iteration 2040/10000 - Loss: 0.2177\n",
            "Iteration 2050/10000 - Loss: 0.2173\n",
            "Iteration 2060/10000 - Loss: 0.2170\n",
            "Iteration 2070/10000 - Loss: 0.2167\n",
            "Iteration 2080/10000 - Loss: 0.2164\n",
            "Iteration 2090/10000 - Loss: 0.2161\n",
            "Iteration 2100/10000 - Loss: 0.2158\n",
            "Iteration 2110/10000 - Loss: 0.2155\n",
            "Iteration 2120/10000 - Loss: 0.2152\n",
            "Iteration 2130/10000 - Loss: 0.2149\n",
            "Iteration 2140/10000 - Loss: 0.2146\n",
            "Iteration 2150/10000 - Loss: 0.2143\n",
            "Iteration 2160/10000 - Loss: 0.2140\n",
            "Iteration 2170/10000 - Loss: 0.2137\n",
            "Iteration 2180/10000 - Loss: 0.2134\n",
            "Iteration 2190/10000 - Loss: 0.2131\n",
            "Iteration 2200/10000 - Loss: 0.2128\n",
            "Iteration 2210/10000 - Loss: 0.2125\n",
            "Iteration 2220/10000 - Loss: 0.2122\n",
            "Iteration 2230/10000 - Loss: 0.2119\n",
            "Iteration 2240/10000 - Loss: 0.2116\n",
            "Iteration 2250/10000 - Loss: 0.2113\n",
            "Iteration 2260/10000 - Loss: 0.2111\n",
            "Iteration 2270/10000 - Loss: 0.2108\n",
            "Iteration 2280/10000 - Loss: 0.2105\n",
            "Iteration 2290/10000 - Loss: 0.2102\n",
            "Iteration 2300/10000 - Loss: 0.2099\n",
            "Iteration 2310/10000 - Loss: 0.2097\n",
            "Iteration 2320/10000 - Loss: 0.2094\n",
            "Iteration 2330/10000 - Loss: 0.2091\n",
            "Iteration 2340/10000 - Loss: 0.2088\n",
            "Iteration 2350/10000 - Loss: 0.2086\n",
            "Iteration 2360/10000 - Loss: 0.2083\n",
            "Iteration 2370/10000 - Loss: 0.2080\n",
            "Iteration 2380/10000 - Loss: 0.2077\n",
            "Iteration 2390/10000 - Loss: 0.2075\n",
            "Iteration 2400/10000 - Loss: 0.2072\n",
            "Iteration 2410/10000 - Loss: 0.2069\n",
            "Iteration 2420/10000 - Loss: 0.2067\n",
            "Iteration 2430/10000 - Loss: 0.2064\n",
            "Iteration 2440/10000 - Loss: 0.2062\n",
            "Iteration 2450/10000 - Loss: 0.2059\n",
            "Iteration 2460/10000 - Loss: 0.2056\n",
            "Iteration 2470/10000 - Loss: 0.2054\n",
            "Iteration 2480/10000 - Loss: 0.2051\n",
            "Iteration 2490/10000 - Loss: 0.2049\n",
            "Iteration 2500/10000 - Loss: 0.2046\n",
            "Iteration 2510/10000 - Loss: 0.2044\n",
            "Iteration 2520/10000 - Loss: 0.2041\n",
            "Iteration 2530/10000 - Loss: 0.2039\n",
            "Iteration 2540/10000 - Loss: 0.2036\n",
            "Iteration 2550/10000 - Loss: 0.2034\n",
            "Iteration 2560/10000 - Loss: 0.2031\n",
            "Iteration 2570/10000 - Loss: 0.2029\n",
            "Iteration 2580/10000 - Loss: 0.2026\n",
            "Iteration 2590/10000 - Loss: 0.2024\n",
            "Iteration 2600/10000 - Loss: 0.2021\n",
            "Iteration 2610/10000 - Loss: 0.2019\n",
            "Iteration 2620/10000 - Loss: 0.2017\n",
            "Iteration 2630/10000 - Loss: 0.2014\n",
            "Iteration 2640/10000 - Loss: 0.2012\n",
            "Iteration 2650/10000 - Loss: 0.2009\n",
            "Iteration 2660/10000 - Loss: 0.2007\n",
            "Iteration 2670/10000 - Loss: 0.2005\n",
            "Iteration 2680/10000 - Loss: 0.2002\n",
            "Iteration 2690/10000 - Loss: 0.2000\n",
            "Iteration 2700/10000 - Loss: 0.1998\n",
            "Iteration 2710/10000 - Loss: 0.1995\n",
            "Iteration 2720/10000 - Loss: 0.1993\n",
            "Iteration 2730/10000 - Loss: 0.1991\n",
            "Iteration 2740/10000 - Loss: 0.1989\n",
            "Iteration 2750/10000 - Loss: 0.1986\n",
            "Iteration 2760/10000 - Loss: 0.1984\n",
            "Iteration 2770/10000 - Loss: 0.1982\n",
            "Iteration 2780/10000 - Loss: 0.1980\n",
            "Iteration 2790/10000 - Loss: 0.1977\n",
            "Iteration 2800/10000 - Loss: 0.1975\n",
            "Iteration 2810/10000 - Loss: 0.1973\n",
            "Iteration 2820/10000 - Loss: 0.1971\n",
            "Iteration 2830/10000 - Loss: 0.1969\n",
            "Iteration 2840/10000 - Loss: 0.1966\n",
            "Iteration 2850/10000 - Loss: 0.1964\n",
            "Iteration 2860/10000 - Loss: 0.1962\n",
            "Iteration 2870/10000 - Loss: 0.1960\n",
            "Iteration 2880/10000 - Loss: 0.1958\n",
            "Iteration 2890/10000 - Loss: 0.1956\n",
            "Iteration 2900/10000 - Loss: 0.1953\n",
            "Iteration 2910/10000 - Loss: 0.1951\n",
            "Iteration 2920/10000 - Loss: 0.1949\n",
            "Iteration 2930/10000 - Loss: 0.1947\n",
            "Iteration 2940/10000 - Loss: 0.1945\n",
            "Iteration 2950/10000 - Loss: 0.1943\n",
            "Iteration 2960/10000 - Loss: 0.1941\n",
            "Iteration 2970/10000 - Loss: 0.1939\n",
            "Iteration 2980/10000 - Loss: 0.1937\n",
            "Iteration 2990/10000 - Loss: 0.1935\n",
            "Iteration 3000/10000 - Loss: 0.1933\n",
            "Iteration 3010/10000 - Loss: 0.1931\n",
            "Iteration 3020/10000 - Loss: 0.1929\n",
            "Iteration 3030/10000 - Loss: 0.1927\n",
            "Iteration 3040/10000 - Loss: 0.1925\n",
            "Iteration 3050/10000 - Loss: 0.1923\n",
            "Iteration 3060/10000 - Loss: 0.1921\n",
            "Iteration 3070/10000 - Loss: 0.1919\n",
            "Iteration 3080/10000 - Loss: 0.1917\n",
            "Iteration 3090/10000 - Loss: 0.1915\n",
            "Iteration 3100/10000 - Loss: 0.1913\n",
            "Iteration 3110/10000 - Loss: 0.1911\n",
            "Iteration 3120/10000 - Loss: 0.1909\n",
            "Iteration 3130/10000 - Loss: 0.1907\n",
            "Iteration 3140/10000 - Loss: 0.1905\n",
            "Iteration 3150/10000 - Loss: 0.1903\n",
            "Iteration 3160/10000 - Loss: 0.1901\n",
            "Iteration 3170/10000 - Loss: 0.1899\n",
            "Iteration 3180/10000 - Loss: 0.1897\n",
            "Iteration 3190/10000 - Loss: 0.1895\n",
            "Iteration 3200/10000 - Loss: 0.1894\n",
            "Iteration 3210/10000 - Loss: 0.1892\n",
            "Iteration 3220/10000 - Loss: 0.1890\n",
            "Iteration 3230/10000 - Loss: 0.1888\n",
            "Iteration 3240/10000 - Loss: 0.1886\n",
            "Iteration 3250/10000 - Loss: 0.1884\n",
            "Iteration 3260/10000 - Loss: 0.1882\n",
            "Iteration 3270/10000 - Loss: 0.1881\n",
            "Iteration 3280/10000 - Loss: 0.1879\n",
            "Iteration 3290/10000 - Loss: 0.1877\n",
            "Iteration 3300/10000 - Loss: 0.1875\n",
            "Iteration 3310/10000 - Loss: 0.1873\n",
            "Iteration 3320/10000 - Loss: 0.1872\n",
            "Iteration 3330/10000 - Loss: 0.1870\n",
            "Iteration 3340/10000 - Loss: 0.1868\n",
            "Iteration 3350/10000 - Loss: 0.1866\n",
            "Iteration 3360/10000 - Loss: 0.1864\n",
            "Iteration 3370/10000 - Loss: 0.1863\n",
            "Iteration 3380/10000 - Loss: 0.1861\n",
            "Iteration 3390/10000 - Loss: 0.1859\n",
            "Iteration 3400/10000 - Loss: 0.1857\n",
            "Iteration 3410/10000 - Loss: 0.1856\n",
            "Iteration 3420/10000 - Loss: 0.1854\n",
            "Iteration 3430/10000 - Loss: 0.1852\n",
            "Iteration 3440/10000 - Loss: 0.1851\n",
            "Iteration 3450/10000 - Loss: 0.1849\n",
            "Iteration 3460/10000 - Loss: 0.1847\n",
            "Iteration 3470/10000 - Loss: 0.1845\n",
            "Iteration 3480/10000 - Loss: 0.1844\n",
            "Iteration 3490/10000 - Loss: 0.1842\n",
            "Iteration 3500/10000 - Loss: 0.1840\n",
            "Iteration 3510/10000 - Loss: 0.1839\n",
            "Iteration 3520/10000 - Loss: 0.1837\n",
            "Iteration 3530/10000 - Loss: 0.1835\n",
            "Iteration 3540/10000 - Loss: 0.1834\n",
            "Iteration 3550/10000 - Loss: 0.1832\n",
            "Iteration 3560/10000 - Loss: 0.1831\n",
            "Iteration 3570/10000 - Loss: 0.1829\n",
            "Iteration 3580/10000 - Loss: 0.1827\n",
            "Iteration 3590/10000 - Loss: 0.1826\n",
            "Iteration 3600/10000 - Loss: 0.1824\n",
            "Iteration 3610/10000 - Loss: 0.1822\n",
            "Iteration 3620/10000 - Loss: 0.1821\n",
            "Iteration 3630/10000 - Loss: 0.1819\n",
            "Iteration 3640/10000 - Loss: 0.1818\n",
            "Iteration 3650/10000 - Loss: 0.1816\n",
            "Iteration 3660/10000 - Loss: 0.1814\n",
            "Iteration 3670/10000 - Loss: 0.1813\n",
            "Iteration 3680/10000 - Loss: 0.1811\n",
            "Iteration 3690/10000 - Loss: 0.1810\n",
            "Iteration 3700/10000 - Loss: 0.1808\n",
            "Iteration 3710/10000 - Loss: 0.1807\n",
            "Iteration 3720/10000 - Loss: 0.1805\n",
            "Iteration 3730/10000 - Loss: 0.1804\n",
            "Iteration 3740/10000 - Loss: 0.1802\n",
            "Iteration 3750/10000 - Loss: 0.1800\n",
            "Iteration 3760/10000 - Loss: 0.1799\n",
            "Iteration 3770/10000 - Loss: 0.1797\n",
            "Iteration 3780/10000 - Loss: 0.1796\n",
            "Iteration 3790/10000 - Loss: 0.1794\n",
            "Iteration 3800/10000 - Loss: 0.1793\n",
            "Iteration 3810/10000 - Loss: 0.1791\n",
            "Iteration 3820/10000 - Loss: 0.1790\n",
            "Iteration 3830/10000 - Loss: 0.1788\n",
            "Iteration 3840/10000 - Loss: 0.1787\n",
            "Iteration 3850/10000 - Loss: 0.1785\n",
            "Iteration 3860/10000 - Loss: 0.1784\n",
            "Iteration 3870/10000 - Loss: 0.1782\n",
            "Iteration 3880/10000 - Loss: 0.1781\n",
            "Iteration 3890/10000 - Loss: 0.1780\n",
            "Iteration 3900/10000 - Loss: 0.1778\n",
            "Iteration 3910/10000 - Loss: 0.1777\n",
            "Iteration 3920/10000 - Loss: 0.1775\n",
            "Iteration 3930/10000 - Loss: 0.1774\n",
            "Iteration 3940/10000 - Loss: 0.1772\n",
            "Iteration 3950/10000 - Loss: 0.1771\n",
            "Iteration 3960/10000 - Loss: 0.1769\n",
            "Iteration 3970/10000 - Loss: 0.1768\n",
            "Iteration 3980/10000 - Loss: 0.1767\n",
            "Iteration 3990/10000 - Loss: 0.1765\n",
            "Iteration 4000/10000 - Loss: 0.1764\n",
            "Iteration 4010/10000 - Loss: 0.1762\n",
            "Iteration 4020/10000 - Loss: 0.1761\n",
            "Iteration 4030/10000 - Loss: 0.1760\n",
            "Iteration 4040/10000 - Loss: 0.1758\n",
            "Iteration 4050/10000 - Loss: 0.1757\n",
            "Iteration 4060/10000 - Loss: 0.1755\n",
            "Iteration 4070/10000 - Loss: 0.1754\n",
            "Iteration 4080/10000 - Loss: 0.1753\n",
            "Iteration 4090/10000 - Loss: 0.1751\n",
            "Iteration 4100/10000 - Loss: 0.1750\n",
            "Iteration 4110/10000 - Loss: 0.1749\n",
            "Iteration 4120/10000 - Loss: 0.1747\n",
            "Iteration 4130/10000 - Loss: 0.1746\n",
            "Iteration 4140/10000 - Loss: 0.1745\n",
            "Iteration 4150/10000 - Loss: 0.1743\n",
            "Iteration 4160/10000 - Loss: 0.1742\n",
            "Iteration 4170/10000 - Loss: 0.1741\n",
            "Iteration 4180/10000 - Loss: 0.1739\n",
            "Iteration 4190/10000 - Loss: 0.1738\n",
            "Iteration 4200/10000 - Loss: 0.1737\n",
            "Iteration 4210/10000 - Loss: 0.1735\n",
            "Iteration 4220/10000 - Loss: 0.1734\n",
            "Iteration 4230/10000 - Loss: 0.1733\n",
            "Iteration 4240/10000 - Loss: 0.1731\n",
            "Iteration 4250/10000 - Loss: 0.1730\n",
            "Iteration 4260/10000 - Loss: 0.1729\n",
            "Iteration 4270/10000 - Loss: 0.1728\n",
            "Iteration 4280/10000 - Loss: 0.1726\n",
            "Iteration 4290/10000 - Loss: 0.1725\n",
            "Iteration 4300/10000 - Loss: 0.1724\n",
            "Iteration 4310/10000 - Loss: 0.1722\n",
            "Iteration 4320/10000 - Loss: 0.1721\n",
            "Iteration 4330/10000 - Loss: 0.1720\n",
            "Iteration 4340/10000 - Loss: 0.1719\n",
            "Iteration 4350/10000 - Loss: 0.1717\n",
            "Iteration 4360/10000 - Loss: 0.1716\n",
            "Iteration 4370/10000 - Loss: 0.1715\n",
            "Iteration 4380/10000 - Loss: 0.1714\n",
            "Iteration 4390/10000 - Loss: 0.1712\n",
            "Iteration 4400/10000 - Loss: 0.1711\n",
            "Iteration 4410/10000 - Loss: 0.1710\n",
            "Iteration 4420/10000 - Loss: 0.1709\n",
            "Iteration 4430/10000 - Loss: 0.1708\n",
            "Iteration 4440/10000 - Loss: 0.1706\n",
            "Iteration 4450/10000 - Loss: 0.1705\n",
            "Iteration 4460/10000 - Loss: 0.1704\n",
            "Iteration 4470/10000 - Loss: 0.1703\n",
            "Iteration 4480/10000 - Loss: 0.1701\n",
            "Iteration 4490/10000 - Loss: 0.1700\n",
            "Iteration 4500/10000 - Loss: 0.1699\n",
            "Iteration 4510/10000 - Loss: 0.1698\n",
            "Iteration 4520/10000 - Loss: 0.1697\n",
            "Iteration 4530/10000 - Loss: 0.1695\n",
            "Iteration 4540/10000 - Loss: 0.1694\n",
            "Iteration 4550/10000 - Loss: 0.1693\n",
            "Iteration 4560/10000 - Loss: 0.1692\n",
            "Iteration 4570/10000 - Loss: 0.1691\n",
            "Iteration 4580/10000 - Loss: 0.1690\n",
            "Iteration 4590/10000 - Loss: 0.1688\n",
            "Iteration 4600/10000 - Loss: 0.1687\n",
            "Iteration 4610/10000 - Loss: 0.1686\n",
            "Iteration 4620/10000 - Loss: 0.1685\n",
            "Iteration 4630/10000 - Loss: 0.1684\n",
            "Iteration 4640/10000 - Loss: 0.1683\n",
            "Iteration 4650/10000 - Loss: 0.1681\n",
            "Iteration 4660/10000 - Loss: 0.1680\n",
            "Iteration 4670/10000 - Loss: 0.1679\n",
            "Iteration 4680/10000 - Loss: 0.1678\n",
            "Iteration 4690/10000 - Loss: 0.1677\n",
            "Iteration 4700/10000 - Loss: 0.1676\n",
            "Iteration 4710/10000 - Loss: 0.1675\n",
            "Iteration 4720/10000 - Loss: 0.1674\n",
            "Iteration 4730/10000 - Loss: 0.1672\n",
            "Iteration 4740/10000 - Loss: 0.1671\n",
            "Iteration 4750/10000 - Loss: 0.1670\n",
            "Iteration 4760/10000 - Loss: 0.1669\n",
            "Iteration 4770/10000 - Loss: 0.1668\n",
            "Iteration 4780/10000 - Loss: 0.1667\n",
            "Iteration 4790/10000 - Loss: 0.1666\n",
            "Iteration 4800/10000 - Loss: 0.1665\n",
            "Iteration 4810/10000 - Loss: 0.1664\n",
            "Iteration 4820/10000 - Loss: 0.1663\n",
            "Iteration 4830/10000 - Loss: 0.1661\n",
            "Iteration 4840/10000 - Loss: 0.1660\n",
            "Iteration 4850/10000 - Loss: 0.1659\n",
            "Iteration 4860/10000 - Loss: 0.1658\n",
            "Iteration 4870/10000 - Loss: 0.1657\n",
            "Iteration 4880/10000 - Loss: 0.1656\n",
            "Iteration 4890/10000 - Loss: 0.1655\n",
            "Iteration 4900/10000 - Loss: 0.1654\n",
            "Iteration 4910/10000 - Loss: 0.1653\n",
            "Iteration 4920/10000 - Loss: 0.1652\n",
            "Iteration 4930/10000 - Loss: 0.1651\n",
            "Iteration 4940/10000 - Loss: 0.1650\n",
            "Iteration 4950/10000 - Loss: 0.1649\n",
            "Iteration 4960/10000 - Loss: 0.1648\n",
            "Iteration 4970/10000 - Loss: 0.1647\n",
            "Iteration 4980/10000 - Loss: 0.1646\n",
            "Iteration 4990/10000 - Loss: 0.1644\n",
            "Iteration 5000/10000 - Loss: 0.1643\n",
            "Iteration 5010/10000 - Loss: 0.1642\n",
            "Iteration 5020/10000 - Loss: 0.1641\n",
            "Iteration 5030/10000 - Loss: 0.1640\n",
            "Iteration 5040/10000 - Loss: 0.1639\n",
            "Iteration 5050/10000 - Loss: 0.1638\n",
            "Iteration 5060/10000 - Loss: 0.1637\n",
            "Iteration 5070/10000 - Loss: 0.1636\n",
            "Iteration 5080/10000 - Loss: 0.1635\n",
            "Iteration 5090/10000 - Loss: 0.1634\n",
            "Iteration 5100/10000 - Loss: 0.1633\n",
            "Iteration 5110/10000 - Loss: 0.1632\n",
            "Iteration 5120/10000 - Loss: 0.1631\n",
            "Iteration 5130/10000 - Loss: 0.1630\n",
            "Iteration 5140/10000 - Loss: 0.1629\n",
            "Iteration 5150/10000 - Loss: 0.1628\n",
            "Iteration 5160/10000 - Loss: 0.1627\n",
            "Iteration 5170/10000 - Loss: 0.1626\n",
            "Iteration 5180/10000 - Loss: 0.1625\n",
            "Iteration 5190/10000 - Loss: 0.1624\n",
            "Iteration 5200/10000 - Loss: 0.1623\n",
            "Iteration 5210/10000 - Loss: 0.1622\n",
            "Iteration 5220/10000 - Loss: 0.1621\n",
            "Iteration 5230/10000 - Loss: 0.1620\n",
            "Iteration 5240/10000 - Loss: 0.1619\n",
            "Iteration 5250/10000 - Loss: 0.1618\n",
            "Iteration 5260/10000 - Loss: 0.1617\n",
            "Iteration 5270/10000 - Loss: 0.1617\n",
            "Iteration 5280/10000 - Loss: 0.1616\n",
            "Iteration 5290/10000 - Loss: 0.1615\n",
            "Iteration 5300/10000 - Loss: 0.1614\n",
            "Iteration 5310/10000 - Loss: 0.1613\n",
            "Iteration 5320/10000 - Loss: 0.1612\n",
            "Iteration 5330/10000 - Loss: 0.1611\n",
            "Iteration 5340/10000 - Loss: 0.1610\n",
            "Iteration 5350/10000 - Loss: 0.1609\n",
            "Iteration 5360/10000 - Loss: 0.1608\n",
            "Iteration 5370/10000 - Loss: 0.1607\n",
            "Iteration 5380/10000 - Loss: 0.1606\n",
            "Iteration 5390/10000 - Loss: 0.1605\n",
            "Iteration 5400/10000 - Loss: 0.1604\n",
            "Iteration 5410/10000 - Loss: 0.1603\n",
            "Iteration 5420/10000 - Loss: 0.1602\n",
            "Iteration 5430/10000 - Loss: 0.1601\n",
            "Iteration 5440/10000 - Loss: 0.1601\n",
            "Iteration 5450/10000 - Loss: 0.1600\n",
            "Iteration 5460/10000 - Loss: 0.1599\n",
            "Iteration 5470/10000 - Loss: 0.1598\n",
            "Iteration 5480/10000 - Loss: 0.1597\n",
            "Iteration 5490/10000 - Loss: 0.1596\n",
            "Iteration 5500/10000 - Loss: 0.1595\n",
            "Iteration 5510/10000 - Loss: 0.1594\n",
            "Iteration 5520/10000 - Loss: 0.1593\n",
            "Iteration 5530/10000 - Loss: 0.1592\n",
            "Iteration 5540/10000 - Loss: 0.1591\n",
            "Iteration 5550/10000 - Loss: 0.1591\n",
            "Iteration 5560/10000 - Loss: 0.1590\n",
            "Iteration 5570/10000 - Loss: 0.1589\n",
            "Iteration 5580/10000 - Loss: 0.1588\n",
            "Iteration 5590/10000 - Loss: 0.1587\n",
            "Iteration 5600/10000 - Loss: 0.1586\n",
            "Iteration 5610/10000 - Loss: 0.1585\n",
            "Iteration 5620/10000 - Loss: 0.1584\n",
            "Iteration 5630/10000 - Loss: 0.1583\n",
            "Iteration 5640/10000 - Loss: 0.1583\n",
            "Iteration 5650/10000 - Loss: 0.1582\n",
            "Iteration 5660/10000 - Loss: 0.1581\n",
            "Iteration 5670/10000 - Loss: 0.1580\n",
            "Iteration 5680/10000 - Loss: 0.1579\n",
            "Iteration 5690/10000 - Loss: 0.1578\n",
            "Iteration 5700/10000 - Loss: 0.1577\n",
            "Iteration 5710/10000 - Loss: 0.1577\n",
            "Iteration 5720/10000 - Loss: 0.1576\n",
            "Iteration 5730/10000 - Loss: 0.1575\n",
            "Iteration 5740/10000 - Loss: 0.1574\n",
            "Iteration 5750/10000 - Loss: 0.1573\n",
            "Iteration 5760/10000 - Loss: 0.1572\n",
            "Iteration 5770/10000 - Loss: 0.1571\n",
            "Iteration 5780/10000 - Loss: 0.1571\n",
            "Iteration 5790/10000 - Loss: 0.1570\n",
            "Iteration 5800/10000 - Loss: 0.1569\n",
            "Iteration 5810/10000 - Loss: 0.1568\n",
            "Iteration 5820/10000 - Loss: 0.1567\n",
            "Iteration 5830/10000 - Loss: 0.1566\n",
            "Iteration 5840/10000 - Loss: 0.1566\n",
            "Iteration 5850/10000 - Loss: 0.1565\n",
            "Iteration 5860/10000 - Loss: 0.1564\n",
            "Iteration 5870/10000 - Loss: 0.1563\n",
            "Iteration 5880/10000 - Loss: 0.1562\n",
            "Iteration 5890/10000 - Loss: 0.1561\n",
            "Iteration 5900/10000 - Loss: 0.1561\n",
            "Iteration 5910/10000 - Loss: 0.1560\n",
            "Iteration 5920/10000 - Loss: 0.1559\n",
            "Iteration 5930/10000 - Loss: 0.1558\n",
            "Iteration 5940/10000 - Loss: 0.1557\n",
            "Iteration 5950/10000 - Loss: 0.1557\n",
            "Iteration 5960/10000 - Loss: 0.1556\n",
            "Iteration 5970/10000 - Loss: 0.1555\n",
            "Iteration 5980/10000 - Loss: 0.1554\n",
            "Iteration 5990/10000 - Loss: 0.1553\n",
            "Iteration 6000/10000 - Loss: 0.1553\n",
            "Iteration 6010/10000 - Loss: 0.1552\n",
            "Iteration 6020/10000 - Loss: 0.1551\n",
            "Iteration 6030/10000 - Loss: 0.1550\n",
            "Iteration 6040/10000 - Loss: 0.1549\n",
            "Iteration 6050/10000 - Loss: 0.1549\n",
            "Iteration 6060/10000 - Loss: 0.1548\n",
            "Iteration 6070/10000 - Loss: 0.1547\n",
            "Iteration 6080/10000 - Loss: 0.1546\n",
            "Iteration 6090/10000 - Loss: 0.1545\n",
            "Iteration 6100/10000 - Loss: 0.1545\n",
            "Iteration 6110/10000 - Loss: 0.1544\n",
            "Iteration 6120/10000 - Loss: 0.1543\n",
            "Iteration 6130/10000 - Loss: 0.1542\n",
            "Iteration 6140/10000 - Loss: 0.1542\n",
            "Iteration 6150/10000 - Loss: 0.1541\n",
            "Iteration 6160/10000 - Loss: 0.1540\n",
            "Iteration 6170/10000 - Loss: 0.1539\n",
            "Iteration 6180/10000 - Loss: 0.1538\n",
            "Iteration 6190/10000 - Loss: 0.1538\n",
            "Iteration 6200/10000 - Loss: 0.1537\n",
            "Iteration 6210/10000 - Loss: 0.1536\n",
            "Iteration 6220/10000 - Loss: 0.1535\n",
            "Iteration 6230/10000 - Loss: 0.1535\n",
            "Iteration 6240/10000 - Loss: 0.1534\n",
            "Iteration 6250/10000 - Loss: 0.1533\n",
            "Iteration 6260/10000 - Loss: 0.1532\n",
            "Iteration 6270/10000 - Loss: 0.1532\n",
            "Iteration 6280/10000 - Loss: 0.1531\n",
            "Iteration 6290/10000 - Loss: 0.1530\n",
            "Iteration 6300/10000 - Loss: 0.1529\n",
            "Iteration 6310/10000 - Loss: 0.1529\n",
            "Iteration 6320/10000 - Loss: 0.1528\n",
            "Iteration 6330/10000 - Loss: 0.1527\n",
            "Iteration 6340/10000 - Loss: 0.1526\n",
            "Iteration 6350/10000 - Loss: 0.1526\n",
            "Iteration 6360/10000 - Loss: 0.1525\n",
            "Iteration 6370/10000 - Loss: 0.1524\n",
            "Iteration 6380/10000 - Loss: 0.1523\n",
            "Iteration 6390/10000 - Loss: 0.1523\n",
            "Iteration 6400/10000 - Loss: 0.1522\n",
            "Iteration 6410/10000 - Loss: 0.1521\n",
            "Iteration 6420/10000 - Loss: 0.1521\n",
            "Iteration 6430/10000 - Loss: 0.1520\n",
            "Iteration 6440/10000 - Loss: 0.1519\n",
            "Iteration 6450/10000 - Loss: 0.1518\n",
            "Iteration 6460/10000 - Loss: 0.1518\n",
            "Iteration 6470/10000 - Loss: 0.1517\n",
            "Iteration 6480/10000 - Loss: 0.1516\n",
            "Iteration 6490/10000 - Loss: 0.1515\n",
            "Iteration 6500/10000 - Loss: 0.1515\n",
            "Iteration 6510/10000 - Loss: 0.1514\n",
            "Iteration 6520/10000 - Loss: 0.1513\n",
            "Iteration 6530/10000 - Loss: 0.1513\n",
            "Iteration 6540/10000 - Loss: 0.1512\n",
            "Iteration 6550/10000 - Loss: 0.1511\n",
            "Iteration 6560/10000 - Loss: 0.1511\n",
            "Iteration 6570/10000 - Loss: 0.1510\n",
            "Iteration 6580/10000 - Loss: 0.1509\n",
            "Iteration 6590/10000 - Loss: 0.1508\n",
            "Iteration 6600/10000 - Loss: 0.1508\n",
            "Iteration 6610/10000 - Loss: 0.1507\n",
            "Iteration 6620/10000 - Loss: 0.1506\n",
            "Iteration 6630/10000 - Loss: 0.1506\n",
            "Iteration 6640/10000 - Loss: 0.1505\n",
            "Iteration 6650/10000 - Loss: 0.1504\n",
            "Iteration 6660/10000 - Loss: 0.1504\n",
            "Iteration 6670/10000 - Loss: 0.1503\n",
            "Iteration 6680/10000 - Loss: 0.1502\n",
            "Iteration 6690/10000 - Loss: 0.1501\n",
            "Iteration 6700/10000 - Loss: 0.1501\n",
            "Iteration 6710/10000 - Loss: 0.1500\n",
            "Iteration 6720/10000 - Loss: 0.1499\n",
            "Iteration 6730/10000 - Loss: 0.1499\n",
            "Iteration 6740/10000 - Loss: 0.1498\n",
            "Iteration 6750/10000 - Loss: 0.1497\n",
            "Iteration 6760/10000 - Loss: 0.1497\n",
            "Iteration 6770/10000 - Loss: 0.1496\n",
            "Iteration 6780/10000 - Loss: 0.1495\n",
            "Iteration 6790/10000 - Loss: 0.1495\n",
            "Iteration 6800/10000 - Loss: 0.1494\n",
            "Iteration 6810/10000 - Loss: 0.1493\n",
            "Iteration 6820/10000 - Loss: 0.1493\n",
            "Iteration 6830/10000 - Loss: 0.1492\n",
            "Iteration 6840/10000 - Loss: 0.1491\n",
            "Iteration 6850/10000 - Loss: 0.1491\n",
            "Iteration 6860/10000 - Loss: 0.1490\n",
            "Iteration 6870/10000 - Loss: 0.1489\n",
            "Iteration 6880/10000 - Loss: 0.1489\n",
            "Iteration 6890/10000 - Loss: 0.1488\n",
            "Iteration 6900/10000 - Loss: 0.1487\n",
            "Iteration 6910/10000 - Loss: 0.1487\n",
            "Iteration 6920/10000 - Loss: 0.1486\n",
            "Iteration 6930/10000 - Loss: 0.1485\n",
            "Iteration 6940/10000 - Loss: 0.1485\n",
            "Iteration 6950/10000 - Loss: 0.1484\n",
            "Iteration 6960/10000 - Loss: 0.1484\n",
            "Iteration 6970/10000 - Loss: 0.1483\n",
            "Iteration 6980/10000 - Loss: 0.1482\n",
            "Iteration 6990/10000 - Loss: 0.1482\n",
            "Iteration 7000/10000 - Loss: 0.1481\n",
            "Iteration 7010/10000 - Loss: 0.1480\n",
            "Iteration 7020/10000 - Loss: 0.1480\n",
            "Iteration 7030/10000 - Loss: 0.1479\n",
            "Iteration 7040/10000 - Loss: 0.1478\n",
            "Iteration 7050/10000 - Loss: 0.1478\n",
            "Iteration 7060/10000 - Loss: 0.1477\n",
            "Iteration 7070/10000 - Loss: 0.1477\n",
            "Iteration 7080/10000 - Loss: 0.1476\n",
            "Iteration 7090/10000 - Loss: 0.1475\n",
            "Iteration 7100/10000 - Loss: 0.1475\n",
            "Iteration 7110/10000 - Loss: 0.1474\n",
            "Iteration 7120/10000 - Loss: 0.1473\n",
            "Iteration 7130/10000 - Loss: 0.1473\n",
            "Iteration 7140/10000 - Loss: 0.1472\n",
            "Iteration 7150/10000 - Loss: 0.1471\n",
            "Iteration 7160/10000 - Loss: 0.1471\n",
            "Iteration 7170/10000 - Loss: 0.1470\n",
            "Iteration 7180/10000 - Loss: 0.1470\n",
            "Iteration 7190/10000 - Loss: 0.1469\n",
            "Iteration 7200/10000 - Loss: 0.1468\n",
            "Iteration 7210/10000 - Loss: 0.1468\n",
            "Iteration 7220/10000 - Loss: 0.1467\n",
            "Iteration 7230/10000 - Loss: 0.1467\n",
            "Iteration 7240/10000 - Loss: 0.1466\n",
            "Iteration 7250/10000 - Loss: 0.1465\n",
            "Iteration 7260/10000 - Loss: 0.1465\n",
            "Iteration 7270/10000 - Loss: 0.1464\n",
            "Iteration 7280/10000 - Loss: 0.1464\n",
            "Iteration 7290/10000 - Loss: 0.1463\n",
            "Iteration 7300/10000 - Loss: 0.1462\n",
            "Iteration 7310/10000 - Loss: 0.1462\n",
            "Iteration 7320/10000 - Loss: 0.1461\n",
            "Iteration 7330/10000 - Loss: 0.1460\n",
            "Iteration 7340/10000 - Loss: 0.1460\n",
            "Iteration 7350/10000 - Loss: 0.1459\n",
            "Iteration 7360/10000 - Loss: 0.1459\n",
            "Iteration 7370/10000 - Loss: 0.1458\n",
            "Iteration 7380/10000 - Loss: 0.1458\n",
            "Iteration 7390/10000 - Loss: 0.1457\n",
            "Iteration 7400/10000 - Loss: 0.1456\n",
            "Iteration 7410/10000 - Loss: 0.1456\n",
            "Iteration 7420/10000 - Loss: 0.1455\n",
            "Iteration 7430/10000 - Loss: 0.1455\n",
            "Iteration 7440/10000 - Loss: 0.1454\n",
            "Iteration 7450/10000 - Loss: 0.1453\n",
            "Iteration 7460/10000 - Loss: 0.1453\n",
            "Iteration 7470/10000 - Loss: 0.1452\n",
            "Iteration 7480/10000 - Loss: 0.1452\n",
            "Iteration 7490/10000 - Loss: 0.1451\n",
            "Iteration 7500/10000 - Loss: 0.1450\n",
            "Iteration 7510/10000 - Loss: 0.1450\n",
            "Iteration 7520/10000 - Loss: 0.1449\n",
            "Iteration 7530/10000 - Loss: 0.1449\n",
            "Iteration 7540/10000 - Loss: 0.1448\n",
            "Iteration 7550/10000 - Loss: 0.1448\n",
            "Iteration 7560/10000 - Loss: 0.1447\n",
            "Iteration 7570/10000 - Loss: 0.1446\n",
            "Iteration 7580/10000 - Loss: 0.1446\n",
            "Iteration 7590/10000 - Loss: 0.1445\n",
            "Iteration 7600/10000 - Loss: 0.1445\n",
            "Iteration 7610/10000 - Loss: 0.1444\n",
            "Iteration 7620/10000 - Loss: 0.1444\n",
            "Iteration 7630/10000 - Loss: 0.1443\n",
            "Iteration 7640/10000 - Loss: 0.1442\n",
            "Iteration 7650/10000 - Loss: 0.1442\n",
            "Iteration 7660/10000 - Loss: 0.1441\n",
            "Iteration 7670/10000 - Loss: 0.1441\n",
            "Iteration 7680/10000 - Loss: 0.1440\n",
            "Iteration 7690/10000 - Loss: 0.1440\n",
            "Iteration 7700/10000 - Loss: 0.1439\n",
            "Iteration 7710/10000 - Loss: 0.1439\n",
            "Iteration 7720/10000 - Loss: 0.1438\n",
            "Iteration 7730/10000 - Loss: 0.1437\n",
            "Iteration 7740/10000 - Loss: 0.1437\n",
            "Iteration 7750/10000 - Loss: 0.1436\n",
            "Iteration 7760/10000 - Loss: 0.1436\n",
            "Iteration 7770/10000 - Loss: 0.1435\n",
            "Iteration 7780/10000 - Loss: 0.1435\n",
            "Iteration 7790/10000 - Loss: 0.1434\n",
            "Iteration 7800/10000 - Loss: 0.1434\n",
            "Iteration 7810/10000 - Loss: 0.1433\n",
            "Iteration 7820/10000 - Loss: 0.1432\n",
            "Iteration 7830/10000 - Loss: 0.1432\n",
            "Iteration 7840/10000 - Loss: 0.1431\n",
            "Iteration 7850/10000 - Loss: 0.1431\n",
            "Iteration 7860/10000 - Loss: 0.1430\n",
            "Iteration 7870/10000 - Loss: 0.1430\n",
            "Iteration 7880/10000 - Loss: 0.1429\n",
            "Iteration 7890/10000 - Loss: 0.1429\n",
            "Iteration 7900/10000 - Loss: 0.1428\n",
            "Iteration 7910/10000 - Loss: 0.1428\n",
            "Iteration 7920/10000 - Loss: 0.1427\n",
            "Iteration 7930/10000 - Loss: 0.1427\n",
            "Iteration 7940/10000 - Loss: 0.1426\n",
            "Iteration 7950/10000 - Loss: 0.1425\n",
            "Iteration 7960/10000 - Loss: 0.1425\n",
            "Iteration 7970/10000 - Loss: 0.1424\n",
            "Iteration 7980/10000 - Loss: 0.1424\n",
            "Iteration 7990/10000 - Loss: 0.1423\n",
            "Iteration 8000/10000 - Loss: 0.1423\n",
            "Iteration 8010/10000 - Loss: 0.1422\n",
            "Iteration 8020/10000 - Loss: 0.1422\n",
            "Iteration 8030/10000 - Loss: 0.1421\n",
            "Iteration 8040/10000 - Loss: 0.1421\n",
            "Iteration 8050/10000 - Loss: 0.1420\n",
            "Iteration 8060/10000 - Loss: 0.1420\n",
            "Iteration 8070/10000 - Loss: 0.1419\n",
            "Iteration 8080/10000 - Loss: 0.1419\n",
            "Iteration 8090/10000 - Loss: 0.1418\n",
            "Iteration 8100/10000 - Loss: 0.1418\n",
            "Iteration 8110/10000 - Loss: 0.1417\n",
            "Iteration 8120/10000 - Loss: 0.1417\n",
            "Iteration 8130/10000 - Loss: 0.1416\n",
            "Iteration 8140/10000 - Loss: 0.1415\n",
            "Iteration 8150/10000 - Loss: 0.1415\n",
            "Iteration 8160/10000 - Loss: 0.1414\n",
            "Iteration 8170/10000 - Loss: 0.1414\n",
            "Iteration 8180/10000 - Loss: 0.1413\n",
            "Iteration 8190/10000 - Loss: 0.1413\n",
            "Iteration 8200/10000 - Loss: 0.1412\n",
            "Iteration 8210/10000 - Loss: 0.1412\n",
            "Iteration 8220/10000 - Loss: 0.1411\n",
            "Iteration 8230/10000 - Loss: 0.1411\n",
            "Iteration 8240/10000 - Loss: 0.1410\n",
            "Iteration 8250/10000 - Loss: 0.1410\n",
            "Iteration 8260/10000 - Loss: 0.1409\n",
            "Iteration 8270/10000 - Loss: 0.1409\n",
            "Iteration 8280/10000 - Loss: 0.1408\n",
            "Iteration 8290/10000 - Loss: 0.1408\n",
            "Iteration 8300/10000 - Loss: 0.1407\n",
            "Iteration 8310/10000 - Loss: 0.1407\n",
            "Iteration 8320/10000 - Loss: 0.1406\n",
            "Iteration 8330/10000 - Loss: 0.1406\n",
            "Iteration 8340/10000 - Loss: 0.1405\n",
            "Iteration 8350/10000 - Loss: 0.1405\n",
            "Iteration 8360/10000 - Loss: 0.1404\n",
            "Iteration 8370/10000 - Loss: 0.1404\n",
            "Iteration 8380/10000 - Loss: 0.1403\n",
            "Iteration 8390/10000 - Loss: 0.1403\n",
            "Iteration 8400/10000 - Loss: 0.1402\n",
            "Iteration 8410/10000 - Loss: 0.1402\n",
            "Iteration 8420/10000 - Loss: 0.1401\n",
            "Iteration 8430/10000 - Loss: 0.1401\n",
            "Iteration 8440/10000 - Loss: 0.1400\n",
            "Iteration 8450/10000 - Loss: 0.1400\n",
            "Iteration 8460/10000 - Loss: 0.1399\n",
            "Iteration 8470/10000 - Loss: 0.1399\n",
            "Iteration 8480/10000 - Loss: 0.1398\n",
            "Iteration 8490/10000 - Loss: 0.1398\n",
            "Iteration 8500/10000 - Loss: 0.1398\n",
            "Iteration 8510/10000 - Loss: 0.1397\n",
            "Iteration 8520/10000 - Loss: 0.1397\n",
            "Iteration 8530/10000 - Loss: 0.1396\n",
            "Iteration 8540/10000 - Loss: 0.1396\n",
            "Iteration 8550/10000 - Loss: 0.1395\n",
            "Iteration 8560/10000 - Loss: 0.1395\n",
            "Iteration 8570/10000 - Loss: 0.1394\n",
            "Iteration 8580/10000 - Loss: 0.1394\n",
            "Iteration 8590/10000 - Loss: 0.1393\n",
            "Iteration 8600/10000 - Loss: 0.1393\n",
            "Iteration 8610/10000 - Loss: 0.1392\n",
            "Iteration 8620/10000 - Loss: 0.1392\n",
            "Iteration 8630/10000 - Loss: 0.1391\n",
            "Iteration 8640/10000 - Loss: 0.1391\n",
            "Iteration 8650/10000 - Loss: 0.1390\n",
            "Iteration 8660/10000 - Loss: 0.1390\n",
            "Iteration 8670/10000 - Loss: 0.1389\n",
            "Iteration 8680/10000 - Loss: 0.1389\n",
            "Iteration 8690/10000 - Loss: 0.1388\n",
            "Iteration 8700/10000 - Loss: 0.1388\n",
            "Iteration 8710/10000 - Loss: 0.1388\n",
            "Iteration 8720/10000 - Loss: 0.1387\n",
            "Iteration 8730/10000 - Loss: 0.1387\n",
            "Iteration 8740/10000 - Loss: 0.1386\n",
            "Iteration 8750/10000 - Loss: 0.1386\n",
            "Iteration 8760/10000 - Loss: 0.1385\n",
            "Iteration 8770/10000 - Loss: 0.1385\n",
            "Iteration 8780/10000 - Loss: 0.1384\n",
            "Iteration 8790/10000 - Loss: 0.1384\n",
            "Iteration 8800/10000 - Loss: 0.1383\n",
            "Iteration 8810/10000 - Loss: 0.1383\n",
            "Iteration 8820/10000 - Loss: 0.1382\n",
            "Iteration 8830/10000 - Loss: 0.1382\n",
            "Iteration 8840/10000 - Loss: 0.1382\n",
            "Iteration 8850/10000 - Loss: 0.1381\n",
            "Iteration 8860/10000 - Loss: 0.1381\n",
            "Iteration 8870/10000 - Loss: 0.1380\n",
            "Iteration 8880/10000 - Loss: 0.1380\n",
            "Iteration 8890/10000 - Loss: 0.1379\n",
            "Iteration 8900/10000 - Loss: 0.1379\n",
            "Iteration 8910/10000 - Loss: 0.1378\n",
            "Iteration 8920/10000 - Loss: 0.1378\n",
            "Iteration 8930/10000 - Loss: 0.1377\n",
            "Iteration 8940/10000 - Loss: 0.1377\n",
            "Iteration 8950/10000 - Loss: 0.1377\n",
            "Iteration 8960/10000 - Loss: 0.1376\n",
            "Iteration 8970/10000 - Loss: 0.1376\n",
            "Iteration 8980/10000 - Loss: 0.1375\n",
            "Iteration 8990/10000 - Loss: 0.1375\n",
            "Iteration 9000/10000 - Loss: 0.1374\n",
            "Iteration 9010/10000 - Loss: 0.1374\n",
            "Iteration 9020/10000 - Loss: 0.1373\n",
            "Iteration 9030/10000 - Loss: 0.1373\n",
            "Iteration 9040/10000 - Loss: 0.1373\n",
            "Iteration 9050/10000 - Loss: 0.1372\n",
            "Iteration 9060/10000 - Loss: 0.1372\n",
            "Iteration 9070/10000 - Loss: 0.1371\n",
            "Iteration 9080/10000 - Loss: 0.1371\n",
            "Iteration 9090/10000 - Loss: 0.1370\n",
            "Iteration 9100/10000 - Loss: 0.1370\n",
            "Iteration 9110/10000 - Loss: 0.1369\n",
            "Iteration 9120/10000 - Loss: 0.1369\n",
            "Iteration 9130/10000 - Loss: 0.1369\n",
            "Iteration 9140/10000 - Loss: 0.1368\n",
            "Iteration 9150/10000 - Loss: 0.1368\n",
            "Iteration 9160/10000 - Loss: 0.1367\n",
            "Iteration 9170/10000 - Loss: 0.1367\n",
            "Iteration 9180/10000 - Loss: 0.1366\n",
            "Iteration 9190/10000 - Loss: 0.1366\n",
            "Iteration 9200/10000 - Loss: 0.1366\n",
            "Iteration 9210/10000 - Loss: 0.1365\n",
            "Iteration 9220/10000 - Loss: 0.1365\n",
            "Iteration 9230/10000 - Loss: 0.1364\n",
            "Iteration 9240/10000 - Loss: 0.1364\n",
            "Iteration 9250/10000 - Loss: 0.1363\n",
            "Iteration 9260/10000 - Loss: 0.1363\n",
            "Iteration 9270/10000 - Loss: 0.1363\n",
            "Iteration 9280/10000 - Loss: 0.1362\n",
            "Iteration 9290/10000 - Loss: 0.1362\n",
            "Iteration 9300/10000 - Loss: 0.1361\n",
            "Iteration 9310/10000 - Loss: 0.1361\n",
            "Iteration 9320/10000 - Loss: 0.1360\n",
            "Iteration 9330/10000 - Loss: 0.1360\n",
            "Iteration 9340/10000 - Loss: 0.1360\n",
            "Iteration 9350/10000 - Loss: 0.1359\n",
            "Iteration 9360/10000 - Loss: 0.1359\n",
            "Iteration 9370/10000 - Loss: 0.1358\n",
            "Iteration 9380/10000 - Loss: 0.1358\n",
            "Iteration 9390/10000 - Loss: 0.1358\n",
            "Iteration 9400/10000 - Loss: 0.1357\n",
            "Iteration 9410/10000 - Loss: 0.1357\n",
            "Iteration 9420/10000 - Loss: 0.1356\n",
            "Iteration 9430/10000 - Loss: 0.1356\n",
            "Iteration 9440/10000 - Loss: 0.1355\n",
            "Iteration 9450/10000 - Loss: 0.1355\n",
            "Iteration 9460/10000 - Loss: 0.1355\n",
            "Iteration 9470/10000 - Loss: 0.1354\n",
            "Iteration 9480/10000 - Loss: 0.1354\n",
            "Iteration 9490/10000 - Loss: 0.1353\n",
            "Iteration 9500/10000 - Loss: 0.1353\n",
            "Iteration 9510/10000 - Loss: 0.1353\n",
            "Iteration 9520/10000 - Loss: 0.1352\n",
            "Iteration 9530/10000 - Loss: 0.1352\n",
            "Iteration 9540/10000 - Loss: 0.1351\n",
            "Iteration 9550/10000 - Loss: 0.1351\n",
            "Iteration 9560/10000 - Loss: 0.1351\n",
            "Iteration 9570/10000 - Loss: 0.1350\n",
            "Iteration 9580/10000 - Loss: 0.1350\n",
            "Iteration 9590/10000 - Loss: 0.1349\n",
            "Iteration 9600/10000 - Loss: 0.1349\n",
            "Iteration 9610/10000 - Loss: 0.1348\n",
            "Iteration 9620/10000 - Loss: 0.1348\n",
            "Iteration 9630/10000 - Loss: 0.1348\n",
            "Iteration 9640/10000 - Loss: 0.1347\n",
            "Iteration 9650/10000 - Loss: 0.1347\n",
            "Iteration 9660/10000 - Loss: 0.1346\n",
            "Iteration 9670/10000 - Loss: 0.1346\n",
            "Iteration 9680/10000 - Loss: 0.1346\n",
            "Iteration 9690/10000 - Loss: 0.1345\n",
            "Iteration 9700/10000 - Loss: 0.1345\n",
            "Iteration 9710/10000 - Loss: 0.1344\n",
            "Iteration 9720/10000 - Loss: 0.1344\n",
            "Iteration 9730/10000 - Loss: 0.1344\n",
            "Iteration 9740/10000 - Loss: 0.1343\n",
            "Iteration 9750/10000 - Loss: 0.1343\n",
            "Iteration 9760/10000 - Loss: 0.1343\n",
            "Iteration 9770/10000 - Loss: 0.1342\n",
            "Iteration 9780/10000 - Loss: 0.1342\n",
            "Iteration 9790/10000 - Loss: 0.1341\n",
            "Iteration 9800/10000 - Loss: 0.1341\n",
            "Iteration 9810/10000 - Loss: 0.1341\n",
            "Iteration 9820/10000 - Loss: 0.1340\n",
            "Iteration 9830/10000 - Loss: 0.1340\n",
            "Iteration 9840/10000 - Loss: 0.1339\n",
            "Iteration 9850/10000 - Loss: 0.1339\n",
            "Iteration 9860/10000 - Loss: 0.1339\n",
            "Iteration 9870/10000 - Loss: 0.1338\n",
            "Iteration 9880/10000 - Loss: 0.1338\n",
            "Iteration 9890/10000 - Loss: 0.1337\n",
            "Iteration 9900/10000 - Loss: 0.1337\n",
            "Iteration 9910/10000 - Loss: 0.1337\n",
            "Iteration 9920/10000 - Loss: 0.1336\n",
            "Iteration 9930/10000 - Loss: 0.1336\n",
            "Iteration 9940/10000 - Loss: 0.1335\n",
            "Iteration 9950/10000 - Loss: 0.1335\n",
            "Iteration 9960/10000 - Loss: 0.1335\n",
            "Iteration 9970/10000 - Loss: 0.1334\n",
            "Iteration 9980/10000 - Loss: 0.1334\n",
            "Iteration 9990/10000 - Loss: 0.1334\n",
            "Iteration 10000/10000 - Loss: 0.1333\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.858\n",
            "Confusion Matrix:\n",
            "[[ 942    0    3    4    0    2   16    1   12    0]\n",
            " [   0 1088    8    3    1    1    4    0   30    0]\n",
            " [  16   29  830   31   19    0   31   22   48    6]\n",
            " [   5    3   22  885    1   20    8   20   35   11]\n",
            " [   3    9    5    1  853    1   18    2   16   74]\n",
            " [  34   20    7  123   25  567   32   15   51   18]\n",
            " [  20    6   13    2   12   17  881    0    7    0]\n",
            " [   5   39   27    1   13    0    4  892   11   36]\n",
            " [   8   18   13   45    9   18   19   16  814   14]\n",
            " [  16   14   14   15   55    8    3   42   14  828]]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Read, Normalize and Split Data\n",
        "#########################################################################\n",
        "\n",
        "def load_and_process_data(file_name):\n",
        "    # Read data from CSV file and preprocess it\n",
        "    with open(file_name) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        X = []  # Initialize a list to store input data (pixel values)\n",
        "        y = []  # Initialize a list to store labels\n",
        "        for row in csv_reader:\n",
        "            y.append(int(row[0]))\n",
        "            temp = [float(i) / 255.0 for i in row[1:]]  # Normalize pixel values\n",
        "            X.append(temp)\n",
        "\n",
        "    # Convert data to NumPy arrays for further processing\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    # Normalize the pixel values using Min-Max scaling\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "\n",
        "    # Add a bias term (1) to the input data\n",
        "    X = np.append(X, np.ones((X.shape[0], 1), np.float64), axis=1)\n",
        "\n",
        "    # Convert labels (y) to one-hot encoding\n",
        "    num_classes = len(np.unique(y))\n",
        "    y = np.eye(num_classes)[y]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Read Training Data\n",
        "X_train, y_train = load_and_process_data('mnist_train.csv')\n",
        "\n",
        "# Read Test Data\n",
        "X_test, y_test = load_and_process_data('mnist_test.csv')\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Logistic regression\n",
        "#########################################################################\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "    # Compute the binary cross-entropy loss\n",
        "    epsilon = 1e-9\n",
        "    y1 = y_true * np.log(y_pred + epsilon)\n",
        "    y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
        "    return -np.mean(y1 + y2)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Sigmoid activation function\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def feed_forward(X, weights, bias):\n",
        "    # Perform feedforward operation\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = sigmoid(z)\n",
        "    return A\n",
        "\n",
        "\n",
        "def fit(X, y, n_iters, lr):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    weights = np.zeros((n_features, n_classes))\n",
        "    bias = np.zeros(n_classes)\n",
        "    losses = []  # To store loss at each iteration\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "        A = feed_forward(X, weights, bias)\n",
        "        loss = compute_loss(y, A)\n",
        "        losses.append(loss)\n",
        "        dz = A - y\n",
        "        dw = (1 / n_samples) * np.dot(X.T, dz)\n",
        "        db = (1 / n_samples) * np.sum(dz, axis=0)\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        if (iteration + 1) % 10 == 0:\n",
        "            print(f\"Iteration {iteration + 1}/{n_iters} - Loss: {loss}\")\n",
        "\n",
        "    return weights, bias, losses\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_iters = 10000\n",
        "\n",
        "weights, bias, losses = fit(X_train, y_train, n_iters, learning_rate)\n",
        "\n",
        "# Plot the loss over iterations\n",
        "plt.figure(1)\n",
        "plt.plot(range(n_iters), losses, '-g', label='Logistic Regression')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = feed_forward(X_test, weights, bias)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute and display the confusion matrix and test accuracy\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), predicted_classes)\n",
        "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm)) / np.sum(cm)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(np.array(cm))\n",
        "\n",
        "input('Close app?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs6WWh4co_jE"
      },
      "source": [
        "Added L2 (Ridge) Regularization\n",
        "\n",
        "Added a dynamic stopping criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zpsL9g9pGlS",
        "outputId": "be0a1119-887c-4f52-999d-2c82a58c0b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 10 - Loss: 0.6430686132659823\n",
            "Iteration 20 - Loss: 0.5968352474567212\n",
            "Iteration 30 - Loss: 0.5586541455461445\n",
            "Iteration 40 - Loss: 0.5269439492602223\n",
            "Iteration 50 - Loss: 0.500433001210704\n",
            "Iteration 60 - Loss: 0.47811065333527525\n",
            "Iteration 70 - Loss: 0.45917786109216535\n",
            "Iteration 80 - Loss: 0.44300336501204113\n",
            "Iteration 90 - Loss: 0.4290873826703438\n",
            "Iteration 100 - Loss: 0.41703262641473454\n",
            "Iteration 110 - Loss: 0.4065216416521198\n",
            "Iteration 120 - Loss: 0.3972992767551955\n",
            "Iteration 130 - Loss: 0.3891591845699873\n",
            "Iteration 140 - Loss: 0.3819334349841873\n",
            "Iteration 150 - Loss: 0.3754845062593486\n",
            "Iteration 160 - Loss: 0.3696990883829596\n",
            "Iteration 170 - Loss: 0.36448326633130756\n",
            "Iteration 180 - Loss: 0.35975875625337517\n",
            "Iteration 190 - Loss: 0.3554599478440737\n",
            "Iteration 200 - Loss: 0.3515315667124671\n",
            "Iteration 210 - Loss: 0.34792681594164687\n",
            "Iteration 220 - Loss: 0.3446058900015334\n",
            "Iteration 230 - Loss: 0.34153477960350764\n",
            "Iteration 240 - Loss: 0.3386843051642793\n",
            "Iteration 250 - Loss: 0.33602933090913706\n",
            "Iteration 260 - Loss: 0.3335481225017913\n",
            "Iteration 270 - Loss: 0.33122181933288514\n",
            "Iteration 280 - Loss: 0.3290339988912293\n",
            "Iteration 290 - Loss: 0.3269703154679106\n",
            "Iteration 300 - Loss: 0.325018199164348\n",
            "Iteration 310 - Loss: 0.323166604059092\n",
            "Iteration 320 - Loss: 0.3214057966346143\n",
            "Iteration 330 - Loss: 0.3197271773242314\n",
            "Iteration 340 - Loss: 0.31812312942337556\n",
            "Iteration 350 - Loss: 0.3165868907038737\n",
            "Iteration 360 - Loss: 0.31511244393943433\n",
            "Iteration 370 - Loss: 0.3136944232446338\n",
            "Iteration 380 - Loss: 0.3123280336862096\n",
            "Iteration 390 - Loss: 0.31100898207364397\n",
            "Iteration 400 - Loss: 0.3097334171984903\n",
            "Iteration 410 - Loss: 0.30849787808622126\n",
            "Iteration 420 - Loss: 0.30729924906437195\n",
            "Iteration 430 - Loss: 0.3061347206471464\n",
            "Iteration 440 - Loss: 0.3050017553980129\n",
            "Iteration 450 - Loss: 0.30389805806483927\n",
            "Iteration 460 - Loss: 0.3028215493921798\n",
            "Iteration 470 - Loss: 0.30177034310668893\n",
            "Iteration 480 - Loss: 0.3007427256477403\n",
            "Iteration 490 - Loss: 0.299737138278896\n",
            "Iteration 500 - Loss: 0.2987521612691659\n",
            "Iteration 510 - Loss: 0.29778649987777683\n",
            "Iteration 520 - Loss: 0.29683897191392566\n",
            "Iteration 530 - Loss: 0.2959084966749097\n",
            "Iteration 540 - Loss: 0.2949940850930773\n",
            "Iteration 550 - Loss: 0.29409483094502514\n",
            "Iteration 560 - Loss: 0.29320990299606714\n",
            "Iteration 570 - Loss: 0.29233853796971404\n",
            "Iteration 580 - Loss: 0.2914800342462369\n",
            "Iteration 590 - Loss: 0.2906337462066693\n",
            "Iteration 600 - Loss: 0.28979907914917646\n",
            "Iteration 610 - Loss: 0.2889754847138313\n",
            "Iteration 620 - Loss: 0.2881624567597006\n",
            "Iteration 630 - Loss: 0.2873595276449653\n",
            "Iteration 640 - Loss: 0.2865662648666974\n",
            "Iteration 650 - Loss: 0.2857822680220548\n",
            "Iteration 660 - Loss: 0.28500716605712395\n",
            "Iteration 670 - Loss: 0.28424061477353624\n",
            "Iteration 680 - Loss: 0.28348229456639584\n",
            "Iteration 690 - Loss: 0.2827319083700313\n",
            "Iteration 700 - Loss: 0.2819891797907081\n",
            "Iteration 710 - Loss: 0.28125385140772446\n",
            "Iteration 720 - Loss: 0.2805256832263439\n",
            "Iteration 730 - Loss: 0.27980445126779246\n",
            "Iteration 740 - Loss: 0.27908994628311873\n",
            "Iteration 750 - Loss: 0.27838197257911074\n",
            "Iteration 760 - Loss: 0.27768034694568405\n",
            "Iteration 770 - Loss: 0.27698489767525203\n",
            "Iteration 780 - Loss: 0.27629546366555197\n",
            "Iteration 790 - Loss: 0.27561189359826316\n",
            "Iteration 800 - Loss: 0.27493404518651815\n",
            "Iteration 810 - Loss: 0.2742617844850873\n",
            "Iteration 820 - Loss: 0.27359498525763115\n",
            "Iteration 830 - Loss: 0.27293352839595164\n",
            "Iteration 840 - Loss: 0.2722773013866675\n",
            "Iteration 850 - Loss: 0.27162619782116687\n",
            "Iteration 860 - Loss: 0.27098011694508795\n",
            "Iteration 870 - Loss: 0.2703389632439232\n",
            "Iteration 880 - Loss: 0.2697026460616644\n",
            "Iteration 890 - Loss: 0.26907107924968077\n",
            "Iteration 900 - Loss: 0.2684441808432853\n",
            "Iteration 910 - Loss: 0.2678218727636721\n",
            "Iteration 920 - Loss: 0.2672040805431125\n",
            "Iteration 930 - Loss: 0.2665907330714902\n",
            "Iteration 940 - Loss: 0.26598176236242027\n",
            "Iteration 950 - Loss: 0.26537710333735576\n",
            "Iteration 960 - Loss: 0.2647766936262182\n",
            "Iteration 970 - Loss: 0.264180473383222\n",
            "Iteration 980 - Loss: 0.2635883851166692\n",
            "Iteration 990 - Loss: 0.2630003735316016\n",
            "Iteration 1000 - Loss: 0.26241638538428586\n",
            "Iteration 1010 - Loss: 0.26183636934759896\n",
            "Iteration 1020 - Loss: 0.26126027588645484\n",
            "Iteration 1030 - Loss: 0.260688057142486\n",
            "Iteration 1040 - Loss: 0.2601196668272573\n",
            "Iteration 1050 - Loss: 0.2595550601233535\n",
            "Iteration 1060 - Loss: 0.2589941935927276\n",
            "Iteration 1070 - Loss: 0.25843702509175337\n",
            "Iteration 1080 - Loss: 0.2578835136924687\n",
            "Iteration 1090 - Loss: 0.25733361960953594\n",
            "Iteration 1100 - Loss: 0.25678730413248424\n",
            "Iteration 1110 - Loss: 0.2562445295628337\n",
            "Iteration 1120 - Loss: 0.25570525915573467\n",
            "Iteration 1130 - Loss: 0.2551694570657789\n",
            "Iteration 1140 - Loss: 0.2546370882966711\n",
            "Iteration 1150 - Loss: 0.254108118654474\n",
            "Iteration 1160 - Loss: 0.2535825147041576\n",
            "Iteration 1170 - Loss: 0.2530602437292092\n",
            "Iteration 1180 - Loss: 0.25254127369407564\n",
            "Iteration 1190 - Loss: 0.25202557320923097\n",
            "Iteration 1200 - Loss: 0.2515131114986728\n",
            "Iteration 1210 - Loss: 0.25100385836967176\n",
            "Iteration 1220 - Loss: 0.25049778418460694\n",
            "Iteration 1230 - Loss: 0.24999485983473646\n",
            "Iteration 1240 - Loss: 0.2494950567157593\n",
            "Iteration 1250 - Loss: 0.24899834670504176\n",
            "Iteration 1260 - Loss: 0.24850470214038367\n",
            "Iteration 1270 - Loss: 0.24801409580021644\n",
            "Iteration 1280 - Loss: 0.24752650088512593\n",
            "Iteration 1290 - Loss: 0.2470418910006073\n",
            "Iteration 1300 - Loss: 0.24656024014096115\n",
            "Iteration 1310 - Loss: 0.24608152267425137\n",
            "Iteration 1320 - Loss: 0.24560571332824357\n",
            "Iteration 1330 - Loss: 0.24513278717726086\n",
            "Iteration 1340 - Loss: 0.24466271962988434\n",
            "Iteration 1350 - Loss: 0.24419548641744343\n",
            "Iteration 1360 - Loss: 0.24373106358323526\n",
            "Iteration 1370 - Loss: 0.24326942747242622\n",
            "Iteration 1380 - Loss: 0.24281055472258095\n",
            "Iteration 1390 - Loss: 0.24235442225477988\n",
            "Iteration 1400 - Loss: 0.24190100726528124\n",
            "Iteration 1410 - Loss: 0.24145028721768755\n",
            "Iteration 1420 - Loss: 0.2410022398355834\n",
            "Iteration 1430 - Loss: 0.24055684309561043\n",
            "Iteration 1440 - Loss: 0.24011407522094785\n",
            "Iteration 1450 - Loss: 0.23967391467517102\n",
            "Iteration 1460 - Loss: 0.23923634015646025\n",
            "Iteration 1470 - Loss: 0.2388013305921372\n",
            "Iteration 1480 - Loss: 0.2383688651335032\n",
            "Iteration 1490 - Loss: 0.2379389231509615\n",
            "Iteration 1500 - Loss: 0.2375114842294002\n",
            "Iteration 1510 - Loss: 0.23708652816382011\n",
            "Iteration 1520 - Loss: 0.23666403495519034\n",
            "Iteration 1530 - Loss: 0.23624398480651362\n",
            "Iteration 1540 - Loss: 0.2358263581190893\n",
            "Iteration 1550 - Loss: 0.23541113548895873\n",
            "Iteration 1560 - Loss: 0.23499829770352124\n",
            "Iteration 1570 - Loss: 0.23458782573830841\n",
            "Iteration 1580 - Loss: 0.23417970075390498\n",
            "Iteration 1590 - Loss: 0.23377390409300888\n",
            "Iteration 1600 - Loss: 0.2333704172776177\n",
            "Iteration 1610 - Loss: 0.23296922200633463\n",
            "Iteration 1620 - Loss: 0.23257030015178554\n",
            "Iteration 1630 - Loss: 0.2321736337581402\n",
            "Iteration 1640 - Loss: 0.23177920503872748\n",
            "Iteration 1650 - Loss: 0.23138699637374405\n",
            "Iteration 1660 - Loss: 0.23099699030804338\n",
            "Iteration 1670 - Loss: 0.23060916954900545\n",
            "Iteration 1680 - Loss: 0.2302235169644779\n",
            "Iteration 1690 - Loss: 0.2298400155807861\n",
            "Iteration 1700 - Loss: 0.22945864858080667\n",
            "Iteration 1710 - Loss: 0.22907939930210022\n",
            "Iteration 1720 - Loss: 0.22870225123509944\n",
            "Iteration 1730 - Loss: 0.22832718802135007\n",
            "Iteration 1740 - Loss: 0.22795419345179954\n",
            "Iteration 1750 - Loss: 0.22758325146513156\n",
            "Iteration 1760 - Loss: 0.22721434614614275\n",
            "Iteration 1770 - Loss: 0.22684746172416048\n",
            "Iteration 1780 - Loss: 0.22648258257149712\n",
            "Iteration 1790 - Loss: 0.2261196932019393\n",
            "Iteration 1800 - Loss: 0.22575877826927163\n",
            "Iteration 1810 - Loss: 0.2253998225658293\n",
            "Iteration 1820 - Loss: 0.22504281102108312\n",
            "Iteration 1830 - Loss: 0.22468772870024858\n",
            "Iteration 1840 - Loss: 0.22433456080292322\n",
            "Iteration 1850 - Loss: 0.2239832926617482\n",
            "Iteration 1860 - Loss: 0.22363390974109237\n",
            "Iteration 1870 - Loss: 0.22328639763575883\n",
            "Iteration 1880 - Loss: 0.2229407420697121\n",
            "Iteration 1890 - Loss: 0.22259692889482594\n",
            "Iteration 1900 - Loss: 0.22225494408964755\n",
            "Iteration 1910 - Loss: 0.22191477375818255\n",
            "Iteration 1920 - Loss: 0.22157640412869495\n",
            "Iteration 1930 - Loss: 0.22123982155252392\n",
            "Iteration 1940 - Loss: 0.22090501250291583\n",
            "Iteration 1950 - Loss: 0.22057196357387213\n",
            "Iteration 1960 - Loss: 0.22024066147900956\n",
            "Iteration 1970 - Loss: 0.2199110930504361\n",
            "Iteration 1980 - Loss: 0.21958324523763803\n",
            "Iteration 1990 - Loss: 0.21925710510638094\n",
            "Iteration 2000 - Loss: 0.2189326598376228\n",
            "Iteration 2010 - Loss: 0.21860989672643819\n",
            "Iteration 2020 - Loss: 0.218288803180954\n",
            "Iteration 2030 - Loss: 0.2179693667212963\n",
            "Iteration 2040 - Loss: 0.21765157497854906\n",
            "Iteration 2050 - Loss: 0.21733541569372025\n",
            "Iteration 2060 - Loss: 0.2170208767167215\n",
            "Iteration 2070 - Loss: 0.21670794600535526\n",
            "Iteration 2080 - Loss: 0.21639661162431226\n",
            "Iteration 2090 - Loss: 0.21608686174417907\n",
            "Iteration 2100 - Loss: 0.21577868464045366\n",
            "Iteration 2110 - Loss: 0.21547206869257063\n",
            "Iteration 2120 - Loss: 0.21516700238293518\n",
            "Iteration 2130 - Loss: 0.21486347429596578\n",
            "Iteration 2140 - Loss: 0.2145614731171454\n",
            "Iteration 2150 - Loss: 0.2142609876320809\n",
            "Iteration 2160 - Loss: 0.21396200672557053\n",
            "Iteration 2170 - Loss: 0.21366451938068037\n",
            "Iteration 2180 - Loss: 0.213368514677828\n",
            "Iteration 2190 - Loss: 0.21307398179387407\n",
            "Iteration 2200 - Loss: 0.21278091000122284\n",
            "Iteration 2210 - Loss: 0.21248928866692857\n",
            "Iteration 2220 - Loss: 0.21219910725181138\n",
            "Iteration 2230 - Loss: 0.2119103553095794\n",
            "Iteration 2240 - Loss: 0.21162302248595952\n",
            "Iteration 2250 - Loss: 0.21133709851783455\n",
            "Iteration 2260 - Loss: 0.211052573232388\n",
            "Iteration 2270 - Loss: 0.2107694365462572\n",
            "Iteration 2280 - Loss: 0.21048767846469224\n",
            "Iteration 2290 - Loss: 0.21020728908072264\n",
            "Iteration 2300 - Loss: 0.2099282585743316\n",
            "Iteration 2310 - Loss: 0.209650577211637\n",
            "Iteration 2320 - Loss: 0.209374235344079\n",
            "Iteration 2330 - Loss: 0.20909922340761578\n",
            "Iteration 2340 - Loss: 0.20882553192192457\n",
            "Iteration 2350 - Loss: 0.20855315148961173\n",
            "Iteration 2360 - Loss: 0.20828207279542751\n",
            "Iteration 2370 - Loss: 0.20801228660548923\n",
            "Iteration 2380 - Loss: 0.20774378376651034\n",
            "Iteration 2390 - Loss: 0.20747655520503694\n",
            "Iteration 2400 - Loss: 0.20721059192669009\n",
            "Iteration 2410 - Loss: 0.20694588501541603\n",
            "Iteration 2420 - Loss: 0.20668242563274153\n",
            "Iteration 2430 - Loss: 0.20642020501703712\n",
            "Iteration 2440 - Loss: 0.20615921448278635\n",
            "Iteration 2450 - Loss: 0.2058994454198615\n",
            "Iteration 2460 - Loss: 0.20564088929280577\n",
            "Iteration 2470 - Loss: 0.2053835376401222\n",
            "Iteration 2480 - Loss: 0.20512738207356865\n",
            "Iteration 2490 - Loss: 0.20487241427745975\n",
            "Iteration 2500 - Loss: 0.20461862600797384\n",
            "Iteration 2510 - Loss: 0.20436600909246797\n",
            "Iteration 2520 - Loss: 0.2041145554287979\n",
            "Iteration 2530 - Loss: 0.2038642569846449\n",
            "Iteration 2540 - Loss: 0.20361510579684824\n",
            "Iteration 2550 - Loss: 0.2033670939707444\n",
            "Iteration 2560 - Loss: 0.20312021367951213\n",
            "Iteration 2570 - Loss: 0.20287445716352343\n",
            "Iteration 2580 - Loss: 0.20262981672970096\n",
            "Iteration 2590 - Loss: 0.20238628475088072\n",
            "Iteration 2600 - Loss: 0.20214385366518167\n",
            "Iteration 2610 - Loss: 0.20190251597538042\n",
            "Iteration 2620 - Loss: 0.2016622642482923\n",
            "Iteration 2630 - Loss: 0.20142309111415815\n",
            "Iteration 2640 - Loss: 0.20118498926603656\n",
            "Iteration 2650 - Loss: 0.20094795145920225\n",
            "Iteration 2660 - Loss: 0.20071197051055073\n",
            "Iteration 2670 - Loss: 0.2004770392980068\n",
            "Iteration 2680 - Loss: 0.200243150759941\n",
            "Iteration 2690 - Loss: 0.20001029789459\n",
            "Iteration 2700 - Loss: 0.19977847375948316\n",
            "Iteration 2710 - Loss: 0.1995476714708746\n",
            "Iteration 2720 - Loss: 0.19931788420318056\n",
            "Iteration 2730 - Loss: 0.19908910518842265\n",
            "Iteration 2740 - Loss: 0.19886132771567594\n",
            "Iteration 2750 - Loss: 0.19863454513052267\n",
            "Iteration 2760 - Loss: 0.1984087508345112\n",
            "Iteration 2770 - Loss: 0.19818393828462036\n",
            "Iteration 2780 - Loss: 0.197960100992729\n",
            "Iteration 2790 - Loss: 0.1977372325250907\n",
            "Iteration 2800 - Loss: 0.19751532650181333\n",
            "Iteration 2810 - Loss: 0.1972943765963444\n",
            "Iteration 2820 - Loss: 0.19707437653496102\n",
            "Iteration 2830 - Loss: 0.19685532009626433\n",
            "Iteration 2840 - Loss: 0.19663720111068084\n",
            "Iteration 2850 - Loss: 0.1964200134599657\n",
            "Iteration 2860 - Loss: 0.19620375107671417\n",
            "Iteration 2870 - Loss: 0.1959884079438753\n",
            "Iteration 2880 - Loss: 0.19577397809427177\n",
            "Iteration 2890 - Loss: 0.1955604556101248\n",
            "Iteration 2900 - Loss: 0.19534783462258223\n",
            "Iteration 2910 - Loss: 0.19513610931125275\n",
            "Iteration 2920 - Loss: 0.19492527390374487\n",
            "Iteration 2930 - Loss: 0.1947153226752088\n",
            "Iteration 2940 - Loss: 0.1945062499478848\n",
            "Iteration 2950 - Loss: 0.19429805009065573\n",
            "Iteration 2960 - Loss: 0.19409071751860252\n",
            "Iteration 2970 - Loss: 0.1938842466925662\n",
            "Iteration 2980 - Loss: 0.19367863211871286\n",
            "Iteration 2990 - Loss: 0.193473868348104\n",
            "Iteration 3000 - Loss: 0.19326994997627014\n",
            "Iteration 3010 - Loss: 0.1930668716427898\n",
            "Iteration 3020 - Loss: 0.19286462803087198\n",
            "Iteration 3030 - Loss: 0.19266321386694335\n",
            "Iteration 3040 - Loss: 0.19246262392023855\n",
            "Iteration 3050 - Loss: 0.19226285300239715\n",
            "Iteration 3060 - Loss: 0.19206389596706075\n",
            "Iteration 3070 - Loss: 0.19186574770947856\n",
            "Iteration 3080 - Loss: 0.1916684031661132\n",
            "Iteration 3090 - Loss: 0.1914718573142527\n",
            "Iteration 3100 - Loss: 0.19127610517162638\n",
            "Iteration 3110 - Loss: 0.19108114179602287\n",
            "Iteration 3120 - Loss: 0.1908869622849143\n",
            "Iteration 3130 - Loss: 0.19069356177508284\n",
            "Iteration 3140 - Loss: 0.1905009354422513\n",
            "Iteration 3150 - Loss: 0.19030907850071774\n",
            "Iteration 3160 - Loss: 0.19011798620299353\n",
            "Iteration 3170 - Loss: 0.1899276538394455\n",
            "Iteration 3180 - Loss: 0.18973807673794113\n",
            "Iteration 3190 - Loss: 0.18954925026349778\n",
            "Iteration 3200 - Loss: 0.18936116981793572\n",
            "Iteration 3210 - Loss: 0.18917383083953368\n",
            "Iteration 3220 - Loss: 0.18898722880268903\n",
            "Iteration 3230 - Loss: 0.18880135921758134\n",
            "Iteration 3240 - Loss: 0.18861621762983777\n",
            "Iteration 3250 - Loss: 0.1884317996202044\n",
            "Iteration 3260 - Loss: 0.18824810080421894\n",
            "Iteration 3270 - Loss: 0.1880651168318877\n",
            "Iteration 3280 - Loss: 0.18788284338736527\n",
            "Iteration 3290 - Loss: 0.18770127618863827\n",
            "Iteration 3300 - Loss: 0.18752041098721117\n",
            "Iteration 3310 - Loss: 0.1873402435677968\n",
            "Iteration 3320 - Loss: 0.18716076974800835\n",
            "Iteration 3330 - Loss: 0.18698198537805624\n",
            "Iteration 3340 - Loss: 0.18680388634044623\n",
            "Iteration 3350 - Loss: 0.18662646854968232\n",
            "Iteration 3360 - Loss: 0.18644972795197154\n",
            "Iteration 3370 - Loss: 0.18627366052493247\n",
            "Iteration 3380 - Loss: 0.18609826227730578\n",
            "Iteration 3390 - Loss: 0.18592352924866878\n",
            "Iteration 3400 - Loss: 0.18574945750915256\n",
            "Iteration 3410 - Loss: 0.1855760431591613\n",
            "Iteration 3420 - Loss: 0.18540328232909573\n",
            "Iteration 3430 - Loss: 0.1852311711790778\n",
            "Iteration 3440 - Loss: 0.18505970589867993\n",
            "Iteration 3450 - Loss: 0.1848888827066552\n",
            "Iteration 3460 - Loss: 0.18471869785067205\n",
            "Iteration 3470 - Loss: 0.1845491476070502\n",
            "Iteration 3480 - Loss: 0.18438022828050027\n",
            "Iteration 3490 - Loss: 0.18421193620386497\n",
            "Iteration 3500 - Loss: 0.18404426773786461\n",
            "Iteration 3510 - Loss: 0.18387721927084325\n",
            "Iteration 3520 - Loss: 0.1837107872185188\n",
            "Iteration 3530 - Loss: 0.18354496802373502\n",
            "Iteration 3540 - Loss: 0.18337975815621613\n",
            "Iteration 3550 - Loss: 0.1832151541123245\n",
            "Iteration 3560 - Loss: 0.1830511524148195\n",
            "Iteration 3570 - Loss: 0.18288774961262008\n",
            "Iteration 3580 - Loss: 0.18272494228056893\n",
            "Iteration 3590 - Loss: 0.18256272701919968\n",
            "Iteration 3600 - Loss: 0.1824011004545056\n",
            "Iteration 3610 - Loss: 0.18224005923771167\n",
            "Iteration 3620 - Loss: 0.1820796000450482\n",
            "Iteration 3630 - Loss: 0.1819197195775265\n",
            "Iteration 3640 - Loss: 0.181760414560718\n",
            "Iteration 3650 - Loss: 0.18160168174453453\n",
            "Iteration 3660 - Loss: 0.18144351790301064\n",
            "Iteration 3670 - Loss: 0.18128591983408962\n",
            "Iteration 3680 - Loss: 0.18112888435940963\n",
            "Iteration 3690 - Loss: 0.18097240832409373\n",
            "Iteration 3700 - Loss: 0.18081648859654104\n",
            "Iteration 3710 - Loss: 0.18066112206822\n",
            "Iteration 3720 - Loss: 0.18050630565346434\n",
            "Iteration 3730 - Loss: 0.1803520362892706\n",
            "Iteration 3740 - Loss: 0.18019831093509728\n",
            "Iteration 3750 - Loss: 0.18004512657266772\n",
            "Iteration 3760 - Loss: 0.17989248020577228\n",
            "Iteration 3770 - Loss: 0.17974036886007494\n",
            "Iteration 3780 - Loss: 0.17958878958292054\n",
            "Iteration 3790 - Loss: 0.17943773944314415\n",
            "Iteration 3800 - Loss: 0.17928721553088248\n",
            "Iteration 3810 - Loss: 0.17913721495738733\n",
            "Iteration 3820 - Loss: 0.17898773485484068\n",
            "Iteration 3830 - Loss: 0.17883877237617113\n",
            "Iteration 3840 - Loss: 0.17869032469487345\n",
            "Iteration 3850 - Loss: 0.17854238900482847\n",
            "Iteration 3860 - Loss: 0.17839496252012588\n",
            "Iteration 3870 - Loss: 0.17824804247488812\n",
            "Iteration 3880 - Loss: 0.17810162612309635\n",
            "Iteration 3890 - Loss: 0.17795571073841768\n",
            "Iteration 3900 - Loss: 0.17781029361403544\n",
            "Iteration 3910 - Loss: 0.177665372062479\n",
            "Iteration 3920 - Loss: 0.17752094341545677\n",
            "Iteration 3930 - Loss: 0.17737700502369103\n",
            "Iteration 3940 - Loss: 0.1772335542567532\n",
            "Iteration 3950 - Loss: 0.17709058850290155\n",
            "Iteration 3960 - Loss: 0.17694810516892098\n",
            "Iteration 3970 - Loss: 0.17680610167996286\n",
            "Iteration 3980 - Loss: 0.17666457547938763\n",
            "Iteration 3990 - Loss: 0.17652352402860882\n",
            "Iteration 4000 - Loss: 0.17638294480693842\n",
            "Iteration 4010 - Loss: 0.1762428353114337\n",
            "Iteration 4020 - Loss: 0.17610319305674557\n",
            "Iteration 4030 - Loss: 0.17596401557496832\n",
            "Iteration 4040 - Loss: 0.17582530041549166\n",
            "Iteration 4050 - Loss: 0.17568704514485228\n",
            "Iteration 4060 - Loss: 0.17554924734658958\n",
            "Iteration 4070 - Loss: 0.17541190462109996\n",
            "Iteration 4080 - Loss: 0.17527501458549485\n",
            "Iteration 4090 - Loss: 0.1751385748734587\n",
            "Iteration 4100 - Loss: 0.17500258313510894\n",
            "Iteration 4110 - Loss: 0.17486703703685721\n",
            "Iteration 4120 - Loss: 0.17473193426127195\n",
            "Iteration 4130 - Loss: 0.17459727250694193\n",
            "Iteration 4140 - Loss: 0.17446304948834196\n",
            "Iteration 4150 - Loss: 0.17432926293569923\n",
            "Iteration 4160 - Loss: 0.1741959105948606\n",
            "Iteration 4170 - Loss: 0.17406299022716265\n",
            "Iteration 4180 - Loss: 0.17393049960930088\n",
            "Iteration 4190 - Loss: 0.17379843653320226\n",
            "Iteration 4200 - Loss: 0.17366679880589772\n",
            "Iteration 4210 - Loss: 0.17353558424939575\n",
            "Iteration 4220 - Loss: 0.17340479070055842\n",
            "Iteration 4230 - Loss: 0.1732744160109771\n",
            "Iteration 4240 - Loss: 0.1731444580468508\n",
            "Iteration 4250 - Loss: 0.17301491468886454\n",
            "Iteration 4260 - Loss: 0.17288578383206923\n",
            "Iteration 4270 - Loss: 0.17275706338576322\n",
            "Iteration 4280 - Loss: 0.17262875127337443\n",
            "Iteration 4290 - Loss: 0.17250084543234304\n",
            "Iteration 4300 - Loss: 0.17237334381400726\n",
            "Iteration 4310 - Loss: 0.1722462443834877\n",
            "Iteration 4320 - Loss: 0.17211954511957486\n",
            "Iteration 4330 - Loss: 0.1719932440146169\n",
            "Iteration 4340 - Loss: 0.17186733907440724\n",
            "Iteration 4350 - Loss: 0.17174182831807658\n",
            "Iteration 4360 - Loss: 0.17161670977798169\n",
            "Iteration 4370 - Loss: 0.17149198149959882\n",
            "Iteration 4380 - Loss: 0.17136764154141607\n",
            "Iteration 4390 - Loss: 0.1712436879748271\n",
            "Iteration 4400 - Loss: 0.17112011888402712\n",
            "Iteration 4410 - Loss: 0.17099693236590785\n",
            "Iteration 4420 - Loss: 0.17087412652995498\n",
            "Iteration 4430 - Loss: 0.1707516994981461\n",
            "Iteration 4440 - Loss: 0.17062964940484926\n",
            "Iteration 4450 - Loss: 0.17050797439672327\n",
            "Iteration 4460 - Loss: 0.17038667263261817\n",
            "Iteration 4470 - Loss: 0.17026574228347707\n",
            "Iteration 4480 - Loss: 0.17014518153223876\n",
            "Iteration 4490 - Loss: 0.17002498857374107\n",
            "Iteration 4500 - Loss: 0.1699051616146256\n",
            "Iteration 4510 - Loss: 0.16978569887324294\n",
            "Iteration 4520 - Loss: 0.1696665985795587\n",
            "Iteration 4530 - Loss: 0.169547858975061\n",
            "Iteration 4540 - Loss: 0.16942947831266775\n",
            "Iteration 4550 - Loss: 0.16931145485663657\n",
            "Iteration 4560 - Loss: 0.16919378688247308\n",
            "Iteration 4570 - Loss: 0.16907647267684256\n",
            "Iteration 4580 - Loss: 0.16895951053748037\n",
            "Iteration 4590 - Loss: 0.1688428987731047\n",
            "Iteration 4600 - Loss: 0.16872663570332952\n",
            "Iteration 4610 - Loss: 0.16861071965857768\n",
            "Iteration 4620 - Loss: 0.1684951489799962\n",
            "Iteration 4630 - Loss: 0.16837992201937135\n",
            "Iteration 4640 - Loss: 0.1682650371390445\n",
            "Iteration 4650 - Loss: 0.16815049271182936\n",
            "Iteration 4660 - Loss: 0.16803628712092963\n",
            "Iteration 4670 - Loss: 0.16792241875985697\n",
            "Iteration 4680 - Loss: 0.16780888603235053\n",
            "Iteration 4690 - Loss: 0.16769568735229706\n",
            "Iteration 4700 - Loss: 0.16758282114365086\n",
            "Iteration 4710 - Loss: 0.1674702858403557\n",
            "Iteration 4720 - Loss: 0.16735807988626644\n",
            "Iteration 4730 - Loss: 0.1672462017350725\n",
            "Iteration 4740 - Loss: 0.16713464985022028\n",
            "Iteration 4750 - Loss: 0.16702342270483847\n",
            "Iteration 4760 - Loss: 0.1669125187816619\n",
            "Iteration 4770 - Loss: 0.16680193657295783\n",
            "Iteration 4780 - Loss: 0.1666916745804519\n",
            "Iteration 4790 - Loss: 0.16658173131525453\n",
            "Iteration 4800 - Loss: 0.16647210529778955\n",
            "Iteration 4810 - Loss: 0.16636279505772136\n",
            "Iteration 4820 - Loss: 0.1662537991338843\n",
            "Iteration 4830 - Loss: 0.16614511607421198\n",
            "Iteration 4840 - Loss: 0.16603674443566724\n",
            "Iteration 4850 - Loss: 0.16592868278417333\n",
            "Iteration 4860 - Loss: 0.16582092969454498\n",
            "Iteration 4870 - Loss: 0.1657134837504204\n",
            "Iteration 4880 - Loss: 0.16560634354419362\n",
            "Iteration 4890 - Loss: 0.1654995076769483\n",
            "Iteration 4900 - Loss: 0.16539297475839118\n",
            "Iteration 4910 - Loss: 0.1652867434067861\n",
            "Iteration 4920 - Loss: 0.16518081224888975\n",
            "Iteration 4930 - Loss: 0.16507517991988643\n",
            "Iteration 4940 - Loss: 0.1649698450633247\n",
            "Iteration 4950 - Loss: 0.16486480633105396\n",
            "Iteration 4960 - Loss: 0.16476006238316124\n",
            "Iteration 4970 - Loss: 0.1646556118879099\n",
            "Iteration 4980 - Loss: 0.16455145352167713\n",
            "Iteration 4990 - Loss: 0.16444758596889314\n",
            "Iteration 5000 - Loss: 0.16434400792198084\n",
            "Iteration 5010 - Loss: 0.16424071808129562\n",
            "Iteration 5020 - Loss: 0.1641377151550654\n",
            "Iteration 5030 - Loss: 0.16403499785933293\n",
            "Iteration 5040 - Loss: 0.16393256491789543\n",
            "Iteration 5050 - Loss: 0.16383041506224874\n",
            "Iteration 5060 - Loss: 0.1637285470315281\n",
            "Iteration 5070 - Loss: 0.16362695957245235\n",
            "Iteration 5080 - Loss: 0.1635256514392669\n",
            "Iteration 5090 - Loss: 0.16342462139368835\n",
            "Iteration 5100 - Loss: 0.16332386820484804\n",
            "Iteration 5110 - Loss: 0.1632233906492384\n",
            "Iteration 5120 - Loss: 0.1631231875106575\n",
            "Iteration 5130 - Loss: 0.1630232575801552\n",
            "Iteration 5140 - Loss: 0.16292359965597988\n",
            "Iteration 5150 - Loss: 0.16282421254352517\n",
            "Iteration 5160 - Loss: 0.16272509505527724\n",
            "Iteration 5170 - Loss: 0.1626262460107629\n",
            "Iteration 5180 - Loss: 0.1625276642364975\n",
            "Iteration 5190 - Loss: 0.1624293485659341\n",
            "Iteration 5200 - Loss: 0.16233129783941236\n",
            "Iteration 5210 - Loss: 0.16223351090410806\n",
            "Iteration 5220 - Loss: 0.16213598661398365\n",
            "Iteration 5230 - Loss: 0.16203872382973802\n",
            "Iteration 5240 - Loss: 0.16194172141875765\n",
            "Iteration 5250 - Loss: 0.16184497825506852\n",
            "Iteration 5260 - Loss: 0.16174849321928703\n",
            "Iteration 5270 - Loss: 0.16165226519857231\n",
            "Iteration 5280 - Loss: 0.1615562930865793\n",
            "Iteration 5290 - Loss: 0.1614605757834112\n",
            "Iteration 5300 - Loss: 0.16136511219557295\n",
            "Iteration 5310 - Loss: 0.1612699012359251\n",
            "Iteration 5320 - Loss: 0.16117494182363776\n",
            "Iteration 5330 - Loss: 0.16108023288414533\n",
            "Iteration 5340 - Loss: 0.16098577334910172\n",
            "Iteration 5350 - Loss: 0.16089156215633477\n",
            "Iteration 5360 - Loss: 0.1607975982498029\n",
            "Iteration 5370 - Loss: 0.1607038805795505\n",
            "Iteration 5380 - Loss: 0.1606104081016649\n",
            "Iteration 5390 - Loss: 0.16051717977823268\n",
            "Iteration 5400 - Loss: 0.16042419457729712\n",
            "Iteration 5410 - Loss: 0.16033145147281597\n",
            "Iteration 5420 - Loss: 0.16023894944461856\n",
            "Iteration 5430 - Loss: 0.16014668747836486\n",
            "Iteration 5440 - Loss: 0.16005466456550377\n",
            "Iteration 5450 - Loss: 0.15996287970323178\n",
            "Iteration 5460 - Loss: 0.15987133189445266\n",
            "Iteration 5470 - Loss: 0.159780020147737\n",
            "Iteration 5480 - Loss: 0.15968894347728177\n",
            "Iteration 5490 - Loss: 0.15959810090287133\n",
            "Iteration 5500 - Loss: 0.1595074914498373\n",
            "Iteration 5510 - Loss: 0.15941711414902018\n",
            "Iteration 5520 - Loss: 0.15932696803673013\n",
            "Iteration 5530 - Loss: 0.15923705215470868\n",
            "Iteration 5540 - Loss: 0.15914736555009099\n",
            "Iteration 5550 - Loss: 0.15905790727536745\n",
            "Iteration 5560 - Loss: 0.15896867638834686\n",
            "Iteration 5570 - Loss: 0.15887967195211886\n",
            "Iteration 5580 - Loss: 0.1587908930350173\n",
            "Iteration 5590 - Loss: 0.15870233871058356\n",
            "Iteration 5600 - Loss: 0.15861400805753043\n",
            "Iteration 5610 - Loss: 0.15852590015970622\n",
            "Iteration 5620 - Loss: 0.15843801410605898\n",
            "Iteration 5630 - Loss: 0.15835034899060096\n",
            "Iteration 5640 - Loss: 0.15826290391237421\n",
            "Iteration 5650 - Loss: 0.15817567797541512\n",
            "Iteration 5660 - Loss: 0.15808867028872003\n",
            "Iteration 5670 - Loss: 0.1580018799662117\n",
            "Iteration 5680 - Loss: 0.15791530612670437\n",
            "Iteration 5690 - Loss: 0.15782894789387075\n",
            "Iteration 5700 - Loss: 0.15774280439620886\n",
            "Iteration 5710 - Loss: 0.15765687476700793\n",
            "Iteration 5720 - Loss: 0.15757115814431674\n",
            "Iteration 5730 - Loss: 0.1574856536709101\n",
            "Iteration 5740 - Loss: 0.15740036049425735\n",
            "Iteration 5750 - Loss: 0.1573152777664894\n",
            "Iteration 5760 - Loss: 0.15723040464436802\n",
            "Iteration 5770 - Loss: 0.15714574028925332\n",
            "Iteration 5780 - Loss: 0.1570612838670731\n",
            "Iteration 5790 - Loss: 0.15697703454829137\n",
            "Iteration 5800 - Loss: 0.15689299150787822\n",
            "Iteration 5810 - Loss: 0.15680915392527855\n",
            "Iteration 5820 - Loss: 0.15672552098438233\n",
            "Iteration 5830 - Loss: 0.1566420918734942\n",
            "Iteration 5840 - Loss: 0.1565588657853041\n",
            "Iteration 5850 - Loss: 0.15647584191685704\n",
            "Iteration 5860 - Loss: 0.15639301946952458\n",
            "Iteration 5870 - Loss: 0.15631039764897509\n",
            "Iteration 5880 - Loss: 0.15622797566514518\n",
            "Iteration 5890 - Loss: 0.15614575273221118\n",
            "Iteration 5900 - Loss: 0.15606372806856042\n",
            "Iteration 5910 - Loss: 0.15598190089676311\n",
            "Iteration 5920 - Loss: 0.15590027044354485\n",
            "Iteration 5930 - Loss: 0.15581883593975804\n",
            "Iteration 5940 - Loss: 0.1557375966203554\n",
            "Iteration 5950 - Loss: 0.15565655172436194\n",
            "Iteration 5960 - Loss: 0.1555757004948479\n",
            "Iteration 5970 - Loss: 0.15549504217890237\n",
            "Iteration 5980 - Loss: 0.155414576027606\n",
            "Iteration 5990 - Loss: 0.15533430129600528\n",
            "Iteration 6000 - Loss: 0.15525421724308572\n",
            "Iteration 6010 - Loss: 0.15517432313174603\n",
            "Iteration 6020 - Loss: 0.15509461822877227\n",
            "Iteration 6030 - Loss: 0.1550151018048122\n",
            "Iteration 6040 - Loss: 0.15493577313435006\n",
            "Iteration 6050 - Loss: 0.15485663149568107\n",
            "Iteration 6060 - Loss: 0.15477767617088625\n",
            "Iteration 6070 - Loss: 0.15469890644580842\n",
            "Iteration 6080 - Loss: 0.15462032161002662\n",
            "Iteration 6090 - Loss: 0.15454192095683197\n",
            "Iteration 6100 - Loss: 0.15446370378320376\n",
            "Iteration 6110 - Loss: 0.15438566938978482\n",
            "Iteration 6120 - Loss: 0.1543078170808581\n",
            "Iteration 6130 - Loss: 0.15423014616432285\n",
            "Iteration 6140 - Loss: 0.15415265595167063\n",
            "Iteration 6150 - Loss: 0.15407534575796278\n",
            "Iteration 6160 - Loss: 0.15399821490180685\n",
            "Iteration 6170 - Loss: 0.15392126270533346\n",
            "Iteration 6180 - Loss: 0.15384448849417398\n",
            "Iteration 6190 - Loss: 0.15376789159743706\n",
            "Iteration 6200 - Loss: 0.15369147134768749\n",
            "Iteration 6210 - Loss: 0.15361522708092257\n",
            "Iteration 6220 - Loss: 0.153539158136551\n",
            "Iteration 6230 - Loss: 0.15346326385737014\n",
            "Iteration 6240 - Loss: 0.15338754358954526\n",
            "Iteration 6250 - Loss: 0.15331199668258677\n",
            "Iteration 6260 - Loss: 0.15323662248932943\n",
            "Iteration 6270 - Loss: 0.15316142036591135\n",
            "Iteration 6280 - Loss: 0.15308638967175212\n",
            "Iteration 6290 - Loss: 0.15301152976953256\n",
            "Iteration 6300 - Loss: 0.1529368400251731\n",
            "Iteration 6310 - Loss: 0.15286231980781445\n",
            "Iteration 6320 - Loss: 0.15278796848979584\n",
            "Iteration 6330 - Loss: 0.15271378544663503\n",
            "Iteration 6340 - Loss: 0.15263977005700885\n",
            "Iteration 6350 - Loss: 0.15256592170273214\n",
            "Iteration 6360 - Loss: 0.15249223976873863\n",
            "Iteration 6370 - Loss: 0.15241872364306097\n",
            "Iteration 6380 - Loss: 0.15234537271681092\n",
            "Iteration 6390 - Loss: 0.15227218638416046\n",
            "Iteration 6400 - Loss: 0.15219916404232212\n",
            "Iteration 6410 - Loss: 0.15212630509152994\n",
            "Iteration 6420 - Loss: 0.1520536089350206\n",
            "Iteration 6430 - Loss: 0.15198107497901414\n",
            "Iteration 6440 - Loss: 0.15190870263269582\n",
            "Iteration 6450 - Loss: 0.1518364913081975\n",
            "Iteration 6460 - Loss: 0.15176444042057874\n",
            "Iteration 6470 - Loss: 0.15169254938780888\n",
            "Iteration 6480 - Loss: 0.15162081763074875\n",
            "Iteration 6490 - Loss: 0.1515492445731328\n",
            "Iteration 6500 - Loss: 0.15147782964155104\n",
            "Iteration 6510 - Loss: 0.1514065722654314\n",
            "Iteration 6520 - Loss: 0.1513354718770223\n",
            "Iteration 6530 - Loss: 0.15126452791137435\n",
            "Iteration 6540 - Loss: 0.15119373980632433\n",
            "Iteration 6550 - Loss: 0.1511231070024765\n",
            "Iteration 6560 - Loss: 0.15105262894318666\n",
            "Iteration 6570 - Loss: 0.1509823050745444\n",
            "Iteration 6580 - Loss: 0.15091213484535687\n",
            "Iteration 6590 - Loss: 0.15084211770713138\n",
            "Iteration 6600 - Loss: 0.15077225311405942\n",
            "Iteration 6610 - Loss: 0.15070254052299972\n",
            "Iteration 6620 - Loss: 0.15063297939346212\n",
            "Iteration 6630 - Loss: 0.15056356918759112\n",
            "Iteration 6640 - Loss: 0.15049430937015001\n",
            "Iteration 6650 - Loss: 0.15042519940850485\n",
            "Iteration 6660 - Loss: 0.15035623877260826\n",
            "Iteration 6670 - Loss: 0.150287426934984\n",
            "Iteration 6680 - Loss: 0.15021876337071097\n",
            "Iteration 6690 - Loss: 0.1501502475574079\n",
            "Iteration 6700 - Loss: 0.150081878975218\n",
            "Iteration 6710 - Loss: 0.15001365710679349\n",
            "Iteration 6720 - Loss: 0.14994558143728007\n",
            "Iteration 6730 - Loss: 0.14987765145430257\n",
            "Iteration 6740 - Loss: 0.14980986664794907\n",
            "Iteration 6750 - Loss: 0.1497422265107568\n",
            "Iteration 6760 - Loss: 0.14967473053769692\n",
            "Iteration 6770 - Loss: 0.1496073782261601\n",
            "Iteration 6780 - Loss: 0.14954016907594142\n",
            "Iteration 6790 - Loss: 0.14947310258922664\n",
            "Iteration 6800 - Loss: 0.14940617827057748\n",
            "Iteration 6810 - Loss: 0.14933939562691717\n",
            "Iteration 6820 - Loss: 0.14927275416751665\n",
            "Iteration 6830 - Loss: 0.14920625340398033\n",
            "Iteration 6840 - Loss: 0.14913989285023224\n",
            "Iteration 6850 - Loss: 0.14907367202250205\n",
            "Iteration 6860 - Loss: 0.1490075904393115\n",
            "Iteration 6870 - Loss: 0.1489416476214604\n",
            "Iteration 6880 - Loss: 0.14887584309201363\n",
            "Iteration 6890 - Loss: 0.14881017637628702\n",
            "Iteration 6900 - Loss: 0.14874464700183443\n",
            "Iteration 6910 - Loss: 0.14867925449843453\n",
            "Iteration 6920 - Loss: 0.14861399839807737\n",
            "Iteration 6930 - Loss: 0.14854887823495122\n",
            "Iteration 6940 - Loss: 0.1484838935454299\n",
            "Iteration 6950 - Loss: 0.1484190438680596\n",
            "Iteration 6960 - Loss: 0.14835432874354645\n",
            "Iteration 6970 - Loss: 0.14828974771474304\n",
            "Iteration 6980 - Loss: 0.148225300326637\n",
            "Iteration 6990 - Loss: 0.14816098612633707\n",
            "Iteration 7000 - Loss: 0.14809680466306174\n",
            "Iteration 7010 - Loss: 0.14803275548812633\n",
            "Iteration 7020 - Loss: 0.14796883815493087\n",
            "Iteration 7030 - Loss: 0.14790505221894795\n",
            "Iteration 7040 - Loss: 0.14784139723771028\n",
            "Iteration 7050 - Loss: 0.14777787277079948\n",
            "Iteration 7060 - Loss: 0.147714478379833\n",
            "Iteration 7070 - Loss: 0.14765121362845346\n",
            "Iteration 7080 - Loss: 0.14758807808231558\n",
            "Iteration 7090 - Loss: 0.1475250713090759\n",
            "Iteration 7100 - Loss: 0.14746219287838008\n",
            "Iteration 7110 - Loss: 0.14739944236185165\n",
            "Iteration 7120 - Loss: 0.14733681933308096\n",
            "Iteration 7130 - Loss: 0.1472743233676134\n",
            "Iteration 7140 - Loss: 0.1472119540429381\n",
            "Iteration 7150 - Loss: 0.14714971093847698\n",
            "Iteration 7160 - Loss: 0.1470875936355734\n",
            "Iteration 7170 - Loss: 0.1470256017174811\n",
            "Iteration 7180 - Loss: 0.14696373476935357\n",
            "Iteration 7190 - Loss: 0.14690199237823245\n",
            "Iteration 7200 - Loss: 0.14684037413303727\n",
            "Iteration 7210 - Loss: 0.14677887962455471\n",
            "Iteration 7220 - Loss: 0.14671750844542752\n",
            "Iteration 7230 - Loss: 0.14665626019014413\n",
            "Iteration 7240 - Loss: 0.14659513445502811\n",
            "Iteration 7250 - Loss: 0.14653413083822786\n",
            "Iteration 7260 - Loss: 0.14647324893970562\n",
            "Iteration 7270 - Loss: 0.14641248836122814\n",
            "Iteration 7280 - Loss: 0.1463518487063552\n",
            "Iteration 7290 - Loss: 0.14629132958043078\n",
            "Iteration 7300 - Loss: 0.14623093059057174\n",
            "Iteration 7310 - Loss: 0.14617065134565835\n",
            "Iteration 7320 - Loss: 0.14611049145632438\n",
            "Iteration 7330 - Loss: 0.14605045053494695\n",
            "Iteration 7340 - Loss: 0.1459905281956369\n",
            "Iteration 7350 - Loss: 0.14593072405422877\n",
            "Iteration 7360 - Loss: 0.145871037728271\n",
            "Iteration 7370 - Loss: 0.14581146883701693\n",
            "Iteration 7380 - Loss: 0.14575201700141457\n",
            "Iteration 7390 - Loss: 0.14569268184409734\n",
            "Iteration 7400 - Loss: 0.14563346298937446\n",
            "Iteration 7410 - Loss: 0.14557436006322166\n",
            "Iteration 7420 - Loss: 0.14551537269327225\n",
            "Iteration 7430 - Loss: 0.14545650050880696\n",
            "Iteration 7440 - Loss: 0.1453977431407455\n",
            "Iteration 7450 - Loss: 0.1453391002216372\n",
            "Iteration 7460 - Loss: 0.14528057138565179\n",
            "Iteration 7470 - Loss: 0.14522215626857052\n",
            "Iteration 7480 - Loss: 0.14516385450777733\n",
            "Iteration 7490 - Loss: 0.1451056657422496\n",
            "Iteration 7500 - Loss: 0.14504758961254957\n",
            "Iteration 7510 - Loss: 0.14498962576081542\n",
            "Iteration 7520 - Loss: 0.14493177383075292\n",
            "Iteration 7530 - Loss: 0.1448740334676261\n",
            "Iteration 7540 - Loss: 0.1448164043182493\n",
            "Iteration 7550 - Loss: 0.1447588860309779\n",
            "Iteration 7560 - Loss: 0.14470147825570093\n",
            "Iteration 7570 - Loss: 0.1446441806438314\n",
            "Iteration 7580 - Loss: 0.14458699284829854\n",
            "Iteration 7590 - Loss: 0.14452991452353986\n",
            "Iteration 7600 - Loss: 0.1444729453254919\n",
            "Iteration 7610 - Loss: 0.1444160849115827\n",
            "Iteration 7620 - Loss: 0.1443593329407237\n",
            "Iteration 7630 - Loss: 0.14430268907330104\n",
            "Iteration 7640 - Loss: 0.14424615297116813\n",
            "Iteration 7650 - Loss: 0.14418972429763735\n",
            "Iteration 7660 - Loss: 0.1441334027174721\n",
            "Iteration 7670 - Loss: 0.14407718789687887\n",
            "Iteration 7680 - Loss: 0.14402107950349963\n",
            "Iteration 7690 - Loss: 0.14396507720640356\n",
            "Iteration 7700 - Loss: 0.14390918067607997\n",
            "Iteration 7710 - Loss: 0.1438533895844301\n",
            "Iteration 7720 - Loss: 0.14379770360475935\n",
            "Iteration 7730 - Loss: 0.14374212241177048\n",
            "Iteration 7740 - Loss: 0.1436866456815549\n",
            "Iteration 7750 - Loss: 0.14363127309158627\n",
            "Iteration 7760 - Loss: 0.1435760043207125\n",
            "Iteration 7770 - Loss: 0.14352083904914817\n",
            "Iteration 7780 - Loss: 0.1434657769584678\n",
            "Iteration 7790 - Loss: 0.14341081773159792\n",
            "Iteration 7800 - Loss: 0.14335596105281015\n",
            "Iteration 7810 - Loss: 0.14330120660771392\n",
            "Iteration 7820 - Loss: 0.14324655408324938\n",
            "Iteration 7830 - Loss: 0.14319200316768\n",
            "Iteration 7840 - Loss: 0.14313755355058577\n",
            "Iteration 7850 - Loss: 0.14308320492285617\n",
            "Iteration 7860 - Loss: 0.14302895697668308\n",
            "Iteration 7870 - Loss: 0.1429748094055538\n",
            "Iteration 7880 - Loss: 0.14292076190424424\n",
            "Iteration 7890 - Loss: 0.1428668141688121\n",
            "Iteration 7900 - Loss: 0.14281296589659\n",
            "Iteration 7910 - Loss: 0.14275921678617864\n",
            "Iteration 7920 - Loss: 0.14270556653744051\n",
            "Iteration 7930 - Loss: 0.14265201485149231\n",
            "Iteration 7940 - Loss: 0.14259856143069957\n",
            "Iteration 7950 - Loss: 0.1425452059786686\n",
            "Iteration 7960 - Loss: 0.1424919482002414\n",
            "Iteration 7970 - Loss: 0.14243878780148791\n",
            "Iteration 7980 - Loss: 0.14238572448970044\n",
            "Iteration 7990 - Loss: 0.14233275797338696\n",
            "Iteration 8000 - Loss: 0.14227988796226418\n",
            "Iteration 8010 - Loss: 0.1422271141672522\n",
            "Iteration 8020 - Loss: 0.1421744363004672\n",
            "Iteration 8030 - Loss: 0.1421218540752161\n",
            "Iteration 8040 - Loss: 0.14206936720598948\n",
            "Iteration 8050 - Loss: 0.14201697540845604\n",
            "Iteration 8060 - Loss: 0.14196467839945606\n",
            "Iteration 8070 - Loss: 0.1419124758969955\n",
            "Iteration 8080 - Loss: 0.14186036762023982\n",
            "Iteration 8090 - Loss: 0.14180835328950808\n",
            "Iteration 8100 - Loss: 0.14175643262626658\n",
            "Iteration 8110 - Loss: 0.14170460535312335\n",
            "Iteration 8120 - Loss: 0.14165287119382225\n",
            "Iteration 8130 - Loss: 0.14160122987323642\n",
            "Iteration 8140 - Loss: 0.1415496811173632\n",
            "Iteration 8150 - Loss: 0.1414982246533179\n",
            "Iteration 8160 - Loss: 0.1414468602093281\n",
            "Iteration 8170 - Loss: 0.141395587514728\n",
            "Iteration 8180 - Loss: 0.14134440629995285\n",
            "Iteration 8190 - Loss: 0.14129331629653255\n",
            "Iteration 8200 - Loss: 0.14124231723708688\n",
            "Iteration 8210 - Loss: 0.14119140885531983\n",
            "Iteration 8220 - Loss: 0.1411405908860133\n",
            "Iteration 8230 - Loss: 0.14108986306502214\n",
            "Iteration 8240 - Loss: 0.14103922512926878\n",
            "Iteration 8250 - Loss: 0.14098867681673746\n",
            "Iteration 8260 - Loss: 0.14093821786646873\n",
            "Iteration 8270 - Loss: 0.14088784801855436\n",
            "Iteration 8280 - Loss: 0.14083756701413178\n",
            "Iteration 8290 - Loss: 0.1407873745953787\n",
            "Iteration 8300 - Loss: 0.1407372705055083\n",
            "Iteration 8310 - Loss: 0.14068725448876293\n",
            "Iteration 8320 - Loss: 0.14063732629040998\n",
            "Iteration 8330 - Loss: 0.14058748565673626\n",
            "Iteration 8340 - Loss: 0.14053773233504246\n",
            "Iteration 8350 - Loss: 0.14048806607363862\n",
            "Iteration 8360 - Loss: 0.14043848662183867\n",
            "Iteration 8370 - Loss: 0.1403889937299553\n",
            "Iteration 8380 - Loss: 0.14033958714929531\n",
            "Iteration 8390 - Loss: 0.14029026663215421\n",
            "Iteration 8400 - Loss: 0.14024103193181126\n",
            "Iteration 8410 - Loss: 0.14019188280252476\n",
            "Iteration 8420 - Loss: 0.14014281899952688\n",
            "Iteration 8430 - Loss: 0.14009384027901897\n",
            "Iteration 8440 - Loss: 0.14004494639816661\n",
            "Iteration 8450 - Loss: 0.13999613711509462\n",
            "Iteration 8460 - Loss: 0.13994741218888268\n",
            "Iteration 8470 - Loss: 0.13989877137955994\n",
            "Iteration 8480 - Loss: 0.13985021444810086\n",
            "Iteration 8490 - Loss: 0.1398017411564202\n",
            "Iteration 8500 - Loss: 0.1397533512673681\n",
            "Iteration 8510 - Loss: 0.1397050445447262\n",
            "Iteration 8520 - Loss: 0.13965682075320188\n",
            "Iteration 8530 - Loss: 0.1396086796584249\n",
            "Iteration 8540 - Loss: 0.1395606210269418\n",
            "Iteration 8550 - Loss: 0.13951264462621168\n",
            "Iteration 8560 - Loss: 0.13946475022460195\n",
            "Iteration 8570 - Loss: 0.13941693759138363\n",
            "Iteration 8580 - Loss: 0.13936920649672668\n",
            "Iteration 8590 - Loss: 0.13932155671169574\n",
            "Iteration 8600 - Loss: 0.1392739880082459\n",
            "Iteration 8610 - Loss: 0.13922650015921811\n",
            "Iteration 8620 - Loss: 0.1391790929383346\n",
            "Iteration 8630 - Loss: 0.1391317661201949\n",
            "Iteration 8640 - Loss: 0.13908451948027148\n",
            "Iteration 8650 - Loss: 0.1390373527949052\n",
            "Iteration 8660 - Loss: 0.13899026584130134\n",
            "Iteration 8670 - Loss: 0.13894325839752528\n",
            "Iteration 8680 - Loss: 0.13889633024249826\n",
            "Iteration 8690 - Loss: 0.1388494811559929\n",
            "Iteration 8700 - Loss: 0.13880271091862967\n",
            "Iteration 8710 - Loss: 0.1387560193118722\n",
            "Iteration 8720 - Loss: 0.13870940611802346\n",
            "Iteration 8730 - Loss: 0.13866287112022163\n",
            "Iteration 8740 - Loss: 0.1386164141024359\n",
            "Iteration 8750 - Loss: 0.13857003484946245\n",
            "Iteration 8760 - Loss: 0.13852373314692054\n",
            "Iteration 8770 - Loss: 0.13847750878124856\n",
            "Iteration 8780 - Loss: 0.13843136153969984\n",
            "Iteration 8790 - Loss: 0.138385291210339\n",
            "Iteration 8800 - Loss: 0.13833929758203756\n",
            "Iteration 8810 - Loss: 0.13829338044447065\n",
            "Iteration 8820 - Loss: 0.1382475395881126\n",
            "Iteration 8830 - Loss: 0.13820177480423337\n",
            "Iteration 8840 - Loss: 0.13815608588489448\n",
            "Iteration 8850 - Loss: 0.13811047262294573\n",
            "Iteration 8860 - Loss: 0.13806493481202065\n",
            "Iteration 8870 - Loss: 0.13801947224653321\n",
            "Iteration 8880 - Loss: 0.13797408472167422\n",
            "Iteration 8890 - Loss: 0.137928772033407\n",
            "Iteration 8900 - Loss: 0.13788353397846434\n",
            "Iteration 8910 - Loss: 0.13783837035434432\n",
            "Iteration 8920 - Loss: 0.13779328095930687\n",
            "Iteration 8930 - Loss: 0.1377482655923703\n",
            "Iteration 8940 - Loss: 0.13770332405330724\n",
            "Iteration 8950 - Loss: 0.13765845614264147\n",
            "Iteration 8960 - Loss: 0.13761366166164396\n",
            "Iteration 8970 - Loss: 0.1375689404123299\n",
            "Iteration 8980 - Loss: 0.1375242921974542\n",
            "Iteration 8990 - Loss: 0.13747971682050913\n",
            "Iteration 9000 - Loss: 0.13743521408571996\n",
            "Iteration 9010 - Loss: 0.1373907837980417\n",
            "Iteration 9020 - Loss: 0.137346425763156\n",
            "Iteration 9030 - Loss: 0.13730213978746708\n",
            "Iteration 9040 - Loss: 0.13725792567809897\n",
            "Iteration 9050 - Loss: 0.13721378324289166\n",
            "Iteration 9060 - Loss: 0.13716971229039784\n",
            "Iteration 9070 - Loss: 0.13712571262987966\n",
            "Iteration 9080 - Loss: 0.13708178407130514\n",
            "Iteration 9090 - Loss: 0.1370379264253453\n",
            "Iteration 9100 - Loss: 0.13699413950337028\n",
            "Iteration 9110 - Loss: 0.13695042311744637\n",
            "Iteration 9120 - Loss: 0.136906777080333\n",
            "Iteration 9130 - Loss: 0.13686320120547882\n",
            "Iteration 9140 - Loss: 0.13681969530701912\n",
            "Iteration 9150 - Loss: 0.13677625919977235\n",
            "Iteration 9160 - Loss: 0.13673289269923686\n",
            "Iteration 9170 - Loss: 0.1366895956215879\n",
            "Iteration 9180 - Loss: 0.1366463677836741\n",
            "Iteration 9190 - Loss: 0.1366032090030151\n",
            "Iteration 9200 - Loss: 0.13656011909779744\n",
            "Iteration 9210 - Loss: 0.1365170978868723\n",
            "Iteration 9220 - Loss: 0.13647414518975182\n",
            "Iteration 9230 - Loss: 0.13643126082660637\n",
            "Iteration 9240 - Loss: 0.1363884446182617\n",
            "Iteration 9250 - Loss: 0.13634569638619506\n",
            "Iteration 9260 - Loss: 0.13630301595253314\n",
            "Iteration 9270 - Loss: 0.13626040314004845\n",
            "Iteration 9280 - Loss: 0.1362178577721569\n",
            "Iteration 9290 - Loss: 0.13617537967291396\n",
            "Iteration 9300 - Loss: 0.13613296866701266\n",
            "Iteration 9310 - Loss: 0.13609062457978\n",
            "Iteration 9320 - Loss: 0.13604834723717443\n",
            "Iteration 9330 - Loss: 0.13600613646578263\n",
            "Iteration 9340 - Loss: 0.13596399209281695\n",
            "Iteration 9350 - Loss: 0.13592191394611225\n",
            "Iteration 9360 - Loss: 0.13587990185412313\n",
            "Iteration 9370 - Loss: 0.13583795564592113\n",
            "Iteration 9380 - Loss: 0.1357960751511922\n",
            "Iteration 9390 - Loss: 0.13575426020023312\n",
            "Iteration 9400 - Loss: 0.13571251062394943\n",
            "Iteration 9410 - Loss: 0.13567082625385268\n",
            "Iteration 9420 - Loss: 0.13562920692205685\n",
            "Iteration 9430 - Loss: 0.13558765246127644\n",
            "Iteration 9440 - Loss: 0.13554616270482375\n",
            "Iteration 9450 - Loss: 0.13550473748660524\n",
            "Iteration 9460 - Loss: 0.13546337664111996\n",
            "Iteration 9470 - Loss: 0.1354220800034562\n",
            "Iteration 9480 - Loss: 0.13538084740928913\n",
            "Iteration 9490 - Loss: 0.1353396786948775\n",
            "Iteration 9500 - Loss: 0.13529857369706222\n",
            "Iteration 9510 - Loss: 0.13525753225326245\n",
            "Iteration 9520 - Loss: 0.13521655420147388\n",
            "Iteration 9530 - Loss: 0.13517563938026564\n",
            "Iteration 9540 - Loss: 0.13513478762877779\n",
            "Iteration 9550 - Loss: 0.13509399878671915\n",
            "Iteration 9560 - Loss: 0.13505327269436404\n",
            "Iteration 9570 - Loss: 0.13501260919255043\n",
            "Iteration 9580 - Loss: 0.1349720081226771\n",
            "Iteration 9590 - Loss: 0.1349314693267007\n",
            "Iteration 9600 - Loss: 0.13489099264713428\n",
            "Iteration 9610 - Loss: 0.13485057792704402\n",
            "Iteration 9620 - Loss: 0.1348102250100465\n",
            "Iteration 9630 - Loss: 0.13476993374030727\n",
            "Iteration 9640 - Loss: 0.13472970396253758\n",
            "Iteration 9650 - Loss: 0.1346895355219921\n",
            "Iteration 9660 - Loss: 0.13464942826446655\n",
            "Iteration 9670 - Loss: 0.13460938203629563\n",
            "Iteration 9680 - Loss: 0.13456939668434986\n",
            "Iteration 9690 - Loss: 0.134529472056034\n",
            "Iteration 9700 - Loss: 0.13448960799928417\n",
            "Iteration 9710 - Loss: 0.13444980436256565\n",
            "Iteration 9720 - Loss: 0.1344100609948705\n",
            "Iteration 9730 - Loss: 0.13437037774571542\n",
            "Iteration 9740 - Loss: 0.1343307544651393\n",
            "Iteration 9750 - Loss: 0.1342911910037005\n",
            "Iteration 9760 - Loss: 0.13425168721247538\n",
            "Iteration 9770 - Loss: 0.13421224294305553\n",
            "Iteration 9780 - Loss: 0.13417285804754508\n",
            "Iteration 9790 - Loss: 0.13413353237855943\n",
            "Iteration 9800 - Loss: 0.1340942657892225\n",
            "Iteration 9810 - Loss: 0.134055058133164\n",
            "Iteration 9820 - Loss: 0.13401590926451817\n",
            "Iteration 9830 - Loss: 0.13397681903792077\n",
            "Iteration 9840 - Loss: 0.13393778730850747\n",
            "Iteration 9850 - Loss: 0.13389881393191116\n",
            "Iteration 9860 - Loss: 0.13385989876426008\n",
            "Iteration 9870 - Loss: 0.13382104166217548\n",
            "Iteration 9880 - Loss: 0.13378224248277\n",
            "Iteration 9890 - Loss: 0.1337435010836445\n",
            "Iteration 9900 - Loss: 0.1337048173228868\n",
            "Iteration 9910 - Loss: 0.1336661910590694\n",
            "Iteration 9920 - Loss: 0.1336276221512472\n",
            "Iteration 9930 - Loss: 0.13358911045895522\n",
            "Iteration 9940 - Loss: 0.13355065584220718\n",
            "Iteration 9950 - Loss: 0.13351225816149243\n",
            "Iteration 9960 - Loss: 0.13347391727777505\n",
            "Iteration 9970 - Loss: 0.13343563305249084\n",
            "Iteration 9980 - Loss: 0.13339740534754585\n",
            "Iteration 9990 - Loss: 0.13335923402531386\n",
            "Iteration 10000 - Loss: 0.133321118948635\n",
            "Iteration 10010 - Loss: 0.13328305998081302\n",
            "Iteration 10020 - Loss: 0.13324505698561395\n",
            "Iteration 10030 - Loss: 0.13320710982726353\n",
            "Iteration 10040 - Loss: 0.13316921837044582\n",
            "Iteration 10050 - Loss: 0.1331313824803007\n",
            "Iteration 10060 - Loss: 0.1330936020224222\n",
            "Iteration 10070 - Loss: 0.13305587686285636\n",
            "Iteration 10080 - Loss: 0.13301820686809965\n",
            "Iteration 10090 - Loss: 0.13298059190509673\n",
            "Iteration 10100 - Loss: 0.13294303184123832\n",
            "Iteration 10110 - Loss: 0.13290552654436003\n",
            "Iteration 10120 - Loss: 0.13286807588273974\n",
            "Iteration 10130 - Loss: 0.13283067972509593\n",
            "Iteration 10140 - Loss: 0.1327933379405861\n",
            "Iteration 10150 - Loss: 0.13275605039880442\n",
            "Iteration 10160 - Loss: 0.13271881696978033\n",
            "Iteration 10170 - Loss: 0.13268163752397616\n",
            "Iteration 10180 - Loss: 0.13264451193228582\n",
            "Iteration 10190 - Loss: 0.13260744006603262\n",
            "Iteration 10200 - Loss: 0.13257042179696776\n",
            "Iteration 10210 - Loss: 0.1325334569972681\n",
            "Iteration 10220 - Loss: 0.13249654553953474\n",
            "Iteration 10230 - Loss: 0.13245968729679086\n",
            "Iteration 10240 - Loss: 0.13242288214248038\n",
            "Iteration 10250 - Loss: 0.13238612995046592\n",
            "Iteration 10260 - Loss: 0.13234943059502688\n",
            "Iteration 10270 - Loss: 0.13231278395085813\n",
            "Iteration 10280 - Loss: 0.13227618989306766\n",
            "Iteration 10290 - Loss: 0.13223964829717547\n",
            "Iteration 10300 - Loss: 0.13220315903911145\n",
            "Iteration 10310 - Loss: 0.13216672199521362\n",
            "Iteration 10320 - Loss: 0.13213033704222674\n",
            "Iteration 10330 - Loss: 0.1320940040573005\n",
            "Iteration 10340 - Loss: 0.1320577229179873\n",
            "Iteration 10350 - Loss: 0.13202149350224152\n",
            "Iteration 10360 - Loss: 0.13198531568841695\n",
            "Iteration 10370 - Loss: 0.13194918935526576\n",
            "Iteration 10380 - Loss: 0.13191311438193626\n",
            "Iteration 10390 - Loss: 0.13187709064797185\n",
            "Iteration 10400 - Loss: 0.13184111803330886\n",
            "Iteration 10410 - Loss: 0.13180519641827548\n",
            "Iteration 10420 - Loss: 0.13176932568358926\n",
            "Iteration 10430 - Loss: 0.1317335057103565\n",
            "Iteration 10440 - Loss: 0.13169773638006985\n",
            "Iteration 10450 - Loss: 0.1316620175746072\n",
            "Iteration 10460 - Loss: 0.13162634917622965\n",
            "Iteration 10470 - Loss: 0.1315907310675805\n",
            "Iteration 10480 - Loss: 0.13155516313168303\n",
            "Iteration 10490 - Loss: 0.13151964525193952\n",
            "Iteration 10500 - Loss: 0.13148417731212902\n",
            "Iteration 10510 - Loss: 0.13144875919640656\n",
            "Iteration 10520 - Loss: 0.13141339078930098\n",
            "Iteration 10530 - Loss: 0.13137807197571358\n",
            "Iteration 10540 - Loss: 0.131342802640917\n",
            "Iteration 10550 - Loss: 0.1313075826705527\n",
            "Iteration 10560 - Loss: 0.13127241195063066\n",
            "Iteration 10570 - Loss: 0.131237290367527\n",
            "Iteration 10580 - Loss: 0.1312022178079827\n",
            "Iteration 10590 - Loss: 0.1311671941591023\n",
            "Iteration 10600 - Loss: 0.13113221930835212\n",
            "Iteration 10610 - Loss: 0.13109729314355906\n",
            "Iteration 10620 - Loss: 0.13106241555290898\n",
            "Iteration 10630 - Loss: 0.13102758642494516\n",
            "Iteration 10640 - Loss: 0.13099280564856705\n",
            "Iteration 10650 - Loss: 0.13095807311302843\n",
            "Iteration 10660 - Loss: 0.13092338870793657\n",
            "Iteration 10670 - Loss: 0.13088875232325026\n",
            "Iteration 10680 - Loss: 0.1308541638492786\n",
            "Iteration 10690 - Loss: 0.13081962317667956\n",
            "Iteration 10700 - Loss: 0.13078513019645857\n",
            "Iteration 10710 - Loss: 0.13075068479996707\n",
            "Iteration 10720 - Loss: 0.13071628687890116\n",
            "Iteration 10730 - Loss: 0.13068193632530015\n",
            "Iteration 10740 - Loss: 0.1306476330315452\n",
            "Iteration 10750 - Loss: 0.13061337689035785\n",
            "Iteration 10760 - Loss: 0.13057916779479892\n",
            "Iteration 10770 - Loss: 0.13054500563826688\n",
            "Iteration 10780 - Loss: 0.13051089031449645\n",
            "Iteration 10790 - Loss: 0.13047682171755748\n",
            "Iteration 10800 - Loss: 0.13044279974185333\n",
            "Iteration 10810 - Loss: 0.13040882428211995\n",
            "Iteration 10820 - Loss: 0.1303748952334239\n",
            "Iteration 10830 - Loss: 0.1303410124911617\n",
            "Iteration 10840 - Loss: 0.130307175951058\n",
            "Iteration 10850 - Loss: 0.1302733855091647\n",
            "Iteration 10860 - Loss: 0.13023964106185906\n",
            "Iteration 10870 - Loss: 0.13020594250584294\n",
            "Iteration 10880 - Loss: 0.13017228973814135\n",
            "Iteration 10890 - Loss: 0.13013868265610104\n",
            "Iteration 10900 - Loss: 0.13010512115738926\n",
            "Iteration 10910 - Loss: 0.13007160513999252\n",
            "Iteration 10920 - Loss: 0.13003813450221544\n",
            "Iteration 10930 - Loss: 0.13000470914267898\n",
            "Iteration 10940 - Loss: 0.12997132896032004\n",
            "Iteration 10950 - Loss: 0.1299379938543894\n",
            "Iteration 10960 - Loss: 0.12990470372445095\n",
            "Iteration 10970 - Loss: 0.12987145847038004\n",
            "Iteration 10980 - Loss: 0.12983825799236282\n",
            "Iteration 10990 - Loss: 0.12980510219089436\n",
            "Iteration 11000 - Loss: 0.12977199096677805\n",
            "Iteration 11010 - Loss: 0.12973892422112387\n",
            "Iteration 11020 - Loss: 0.12970590185534747\n",
            "Iteration 11030 - Loss: 0.12967292377116887\n",
            "Iteration 11040 - Loss: 0.1296399898706113\n",
            "Iteration 11050 - Loss: 0.12960710005599987\n",
            "Iteration 11060 - Loss: 0.1295742542299607\n",
            "Iteration 11070 - Loss: 0.12954145229541925\n",
            "Iteration 11080 - Loss: 0.12950869415559946\n",
            "Iteration 11090 - Loss: 0.1294759797140229\n",
            "Iteration 11100 - Loss: 0.1294433088745068\n",
            "Iteration 11110 - Loss: 0.1294106815411634\n",
            "Iteration 11120 - Loss: 0.1293780976183989\n",
            "Iteration 11130 - Loss: 0.12934555701091216\n",
            "Iteration 11140 - Loss: 0.12931305962369324\n",
            "Iteration 11150 - Loss: 0.12928060536202274\n",
            "Iteration 11160 - Loss: 0.12924819413147043\n",
            "Iteration 11170 - Loss: 0.12921582583789418\n",
            "Iteration 11180 - Loss: 0.1291835003874388\n",
            "Iteration 11190 - Loss: 0.12915121768653495\n",
            "Iteration 11200 - Loss: 0.1291189776418982\n",
            "Iteration 11210 - Loss: 0.12908678016052727\n",
            "Iteration 11220 - Loss: 0.12905462514970392\n",
            "Iteration 11230 - Loss: 0.12902251251699093\n",
            "Iteration 11240 - Loss: 0.12899044217023173\n",
            "Iteration 11250 - Loss: 0.12895841401754882\n",
            "Iteration 11260 - Loss: 0.12892642796734297\n",
            "Iteration 11270 - Loss: 0.12889448392829184\n",
            "Iteration 11280 - Loss: 0.12886258180934934\n",
            "Iteration 11290 - Loss: 0.12883072151974428\n",
            "Iteration 11300 - Loss: 0.12879890296897917\n",
            "Iteration 11310 - Loss: 0.12876712606682947\n",
            "Iteration 11320 - Loss: 0.12873539072334264\n",
            "Iteration 11330 - Loss: 0.12870369684883637\n",
            "Iteration 11340 - Loss: 0.12867204435389873\n",
            "Iteration 11350 - Loss: 0.1286404331493858\n",
            "Iteration 11360 - Loss: 0.12860886314642156\n",
            "Iteration 11370 - Loss: 0.12857733425639664\n",
            "Iteration 11380 - Loss: 0.12854584639096708\n",
            "Iteration 11390 - Loss: 0.1285143994620538\n",
            "Iteration 11400 - Loss: 0.12848299338184083\n",
            "Iteration 11410 - Loss: 0.1284516280627749\n",
            "Iteration 11420 - Loss: 0.1284203034175645\n",
            "Iteration 11430 - Loss: 0.12838901935917837\n",
            "Iteration 11440 - Loss: 0.12835777580084493\n",
            "Iteration 11450 - Loss: 0.1283265726560511\n",
            "Iteration 11460 - Loss: 0.12829540983854126\n",
            "Iteration 11470 - Loss: 0.12826428726231667\n",
            "Iteration 11480 - Loss: 0.1282332048416339\n",
            "Iteration 11490 - Loss: 0.12820216249100422\n",
            "Iteration 11500 - Loss: 0.12817116012519278\n",
            "Iteration 11510 - Loss: 0.12814019765921716\n",
            "Iteration 11520 - Loss: 0.1281092750083469\n",
            "Iteration 11530 - Loss: 0.12807839208810212\n",
            "Iteration 11540 - Loss: 0.12804754881425306\n",
            "Iteration 11550 - Loss: 0.1280167451028185\n",
            "Iteration 11560 - Loss: 0.1279859808700656\n",
            "Iteration 11570 - Loss: 0.12795525603250824\n",
            "Iteration 11580 - Loss: 0.12792457050690645\n",
            "Iteration 11590 - Loss: 0.12789392421026546\n",
            "Iteration 11600 - Loss: 0.1278633170598348\n",
            "Iteration 11610 - Loss: 0.12783274897310723\n",
            "Iteration 11620 - Loss: 0.12780221986781798\n",
            "Iteration 11630 - Loss: 0.12777172966194367\n",
            "Iteration 11640 - Loss: 0.12774127827370166\n",
            "Iteration 11650 - Loss: 0.12771086562154885\n",
            "Iteration 11660 - Loss: 0.127680491624181\n",
            "Iteration 11670 - Loss: 0.1276501562005318\n",
            "Iteration 11680 - Loss: 0.1276198592697719\n",
            "Iteration 11690 - Loss: 0.12758960075130804\n",
            "Iteration 11700 - Loss: 0.12755938056478217\n",
            "Iteration 11710 - Loss: 0.12752919863007078\n",
            "Iteration 11720 - Loss: 0.12749905486728358\n",
            "Iteration 11730 - Loss: 0.12746894919676297\n",
            "Iteration 11740 - Loss: 0.12743888153908334\n",
            "Iteration 11750 - Loss: 0.12740885181504966\n",
            "Iteration 11760 - Loss: 0.12737885994569698\n",
            "Iteration 11770 - Loss: 0.12734890585228947\n",
            "Iteration 11780 - Loss: 0.12731898945632003\n",
            "Iteration 11790 - Loss: 0.12728911067950838\n",
            "Iteration 11800 - Loss: 0.12725926944380142\n",
            "Iteration 11810 - Loss: 0.1272294656713715\n",
            "Iteration 11820 - Loss: 0.12719969928461605\n",
            "Iteration 11830 - Loss: 0.1271699702061567\n",
            "Iteration 11840 - Loss: 0.12714027835883818\n",
            "Iteration 11850 - Loss: 0.12711062366572773\n",
            "Iteration 11860 - Loss: 0.12708100605011446\n",
            "Iteration 11870 - Loss: 0.12705142543550804\n",
            "Iteration 11880 - Loss: 0.12702188174563828\n",
            "Iteration 11890 - Loss: 0.1269923749044541\n",
            "Iteration 11900 - Loss: 0.12696290483612305\n",
            "Iteration 11910 - Loss: 0.12693347146502995\n",
            "Iteration 11920 - Loss: 0.12690407471577678\n",
            "Iteration 11930 - Loss: 0.1268747145131811\n",
            "Iteration 11940 - Loss: 0.1268453907822761\n",
            "Iteration 11950 - Loss: 0.12681610344830918\n",
            "Iteration 11960 - Loss: 0.12678685243674137\n",
            "Iteration 11970 - Loss: 0.12675763767324655\n",
            "Iteration 11980 - Loss: 0.12672845908371075\n",
            "Iteration 11990 - Loss: 0.12669931659423125\n",
            "Iteration 12000 - Loss: 0.12667021013111587\n",
            "Iteration 12010 - Loss: 0.12664113962088236\n",
            "Iteration 12020 - Loss: 0.12661210499025707\n",
            "Iteration 12030 - Loss: 0.12658310616617496\n",
            "Iteration 12040 - Loss: 0.12655414307577845\n",
            "Iteration 12050 - Loss: 0.12652521564641642\n",
            "Iteration 12060 - Loss: 0.12649632380564393\n",
            "Iteration 12070 - Loss: 0.1264674674812213\n",
            "Iteration 12080 - Loss: 0.12643864660111323\n",
            "Iteration 12090 - Loss: 0.12640986109348815\n",
            "Iteration 12100 - Loss: 0.1263811108867177\n",
            "Iteration 12110 - Loss: 0.12635239590937544\n",
            "Iteration 12120 - Loss: 0.12632371609023693\n",
            "Iteration 12130 - Loss: 0.126295071358278\n",
            "Iteration 12140 - Loss: 0.12626646164267505\n",
            "Iteration 12150 - Loss: 0.12623788687280357\n",
            "Iteration 12160 - Loss: 0.12620934697823788\n",
            "Iteration 12170 - Loss: 0.12618084188875006\n",
            "Iteration 12180 - Loss: 0.12615237153430936\n",
            "Iteration 12190 - Loss: 0.12612393584508189\n",
            "Iteration 12200 - Loss: 0.1260955347514292\n",
            "Iteration 12210 - Loss: 0.12606716818390812\n",
            "Iteration 12220 - Loss: 0.12603883607326982\n",
            "Iteration 12230 - Loss: 0.12601053835045922\n",
            "Iteration 12240 - Loss: 0.12598227494661426\n",
            "Iteration 12250 - Loss: 0.12595404579306516\n",
            "Iteration 12260 - Loss: 0.1259258508213338\n",
            "Iteration 12270 - Loss: 0.12589768996313297\n",
            "Iteration 12280 - Loss: 0.12586956315036574\n",
            "Iteration 12290 - Loss: 0.12584147031512485\n",
            "Iteration 12300 - Loss: 0.12581341138969174\n",
            "Iteration 12310 - Loss: 0.12578538630653632\n",
            "Iteration 12320 - Loss: 0.12575739499831606\n",
            "Iteration 12330 - Loss: 0.1257294373978751\n",
            "Iteration 12340 - Loss: 0.12570151343824412\n",
            "Iteration 12350 - Loss: 0.12567362305263907\n",
            "Iteration 12360 - Loss: 0.1256457661744612\n",
            "Iteration 12370 - Loss: 0.12561794273729557\n",
            "Iteration 12380 - Loss: 0.12559015267491117\n",
            "Iteration 12390 - Loss: 0.12556239592125984\n",
            "Iteration 12400 - Loss: 0.1255346724104758\n",
            "Iteration 12410 - Loss: 0.12550698207687483\n",
            "Iteration 12420 - Loss: 0.12547932485495383\n",
            "Iteration 12430 - Loss: 0.12545170067938996\n",
            "Iteration 12440 - Loss: 0.12542410948504037\n",
            "Iteration 12450 - Loss: 0.12539655120694101\n",
            "Iteration 12460 - Loss: 0.12536902578030656\n",
            "Iteration 12470 - Loss: 0.12534153314052945\n",
            "Iteration 12480 - Loss: 0.12531407322317933\n",
            "Iteration 12490 - Loss: 0.12528664596400263\n",
            "Iteration 12500 - Loss: 0.1252592512989215\n",
            "Iteration 12510 - Loss: 0.12523188916403372\n",
            "Iteration 12520 - Loss: 0.12520455949561163\n",
            "Iteration 12530 - Loss: 0.12517726223010178\n",
            "Iteration 12540 - Loss: 0.12514999730412435\n",
            "Iteration 12550 - Loss: 0.12512276465447217\n",
            "Iteration 12560 - Loss: 0.1250955642181108\n",
            "Iteration 12570 - Loss: 0.12506839593217722\n",
            "Iteration 12580 - Loss: 0.12504125973397967\n",
            "Iteration 12590 - Loss: 0.12501415556099682\n",
            "Iteration 12600 - Loss: 0.12498708335087728\n",
            "Iteration 12610 - Loss: 0.1249600430414392\n",
            "Iteration 12620 - Loss: 0.12493303457066911\n",
            "Iteration 12630 - Loss: 0.12490605787672215\n",
            "Iteration 12640 - Loss: 0.12487911289792064\n",
            "Iteration 12650 - Loss: 0.12485219957275409\n",
            "Iteration 12660 - Loss: 0.12482531783987845\n",
            "Iteration 12670 - Loss: 0.12479846763811538\n",
            "Iteration 12680 - Loss: 0.12477164890645201\n",
            "Iteration 12690 - Loss: 0.12474486158403995\n",
            "Iteration 12700 - Loss: 0.12471810561019507\n",
            "Iteration 12710 - Loss: 0.12469138092439677\n",
            "Iteration 12720 - Loss: 0.12466468746628745\n",
            "Iteration 12730 - Loss: 0.12463802517567182\n",
            "Iteration 12740 - Loss: 0.12461139399251671\n",
            "Iteration 12750 - Loss: 0.12458479385695001\n",
            "Iteration 12760 - Loss: 0.1245582247092606\n",
            "Iteration 12770 - Loss: 0.12453168648989739\n",
            "Iteration 12780 - Loss: 0.1245051791394689\n",
            "Iteration 12790 - Loss: 0.12447870259874293\n",
            "Iteration 12800 - Loss: 0.12445225680864581\n",
            "Iteration 12810 - Loss: 0.12442584171026184\n",
            "Iteration 12820 - Loss: 0.12439945724483262\n",
            "Iteration 12830 - Loss: 0.12437310335375709\n",
            "Iteration 12840 - Loss: 0.12434677997859019\n",
            "Iteration 12850 - Loss: 0.1243204870610429\n",
            "Iteration 12860 - Loss: 0.12429422454298153\n",
            "Iteration 12870 - Loss: 0.12426799236642724\n",
            "Iteration 12880 - Loss: 0.12424179047355532\n",
            "Iteration 12890 - Loss: 0.12421561880669497\n",
            "Iteration 12900 - Loss: 0.1241894773083284\n",
            "Iteration 12910 - Loss: 0.12416336592109072\n",
            "Iteration 12920 - Loss: 0.12413728458776901\n",
            "Iteration 12930 - Loss: 0.1241112332513024\n",
            "Iteration 12940 - Loss: 0.12408521185478058\n",
            "Iteration 12950 - Loss: 0.12405922034144448\n",
            "Iteration 12960 - Loss: 0.12403325865468477\n",
            "Iteration 12970 - Loss: 0.12400732673804168\n",
            "Iteration 12980 - Loss: 0.1239814245352049\n",
            "Iteration 12990 - Loss: 0.12395555199001253\n",
            "Iteration 13000 - Loss: 0.1239297090464506\n",
            "Iteration 13010 - Loss: 0.12390389564865309\n",
            "Iteration 13020 - Loss: 0.12387811174090083\n",
            "Iteration 13030 - Loss: 0.12385235726762124\n",
            "Iteration 13040 - Loss: 0.12382663217338798\n",
            "Iteration 13050 - Loss: 0.12380093640292025\n",
            "Iteration 13060 - Loss: 0.12377526990108244\n",
            "Iteration 13070 - Loss: 0.12374963261288345\n",
            "Iteration 13080 - Loss: 0.12372402448347654\n",
            "Iteration 13090 - Loss: 0.12369844545815847\n",
            "Iteration 13100 - Loss: 0.12367289548236919\n",
            "Iteration 13110 - Loss: 0.12364737450169147\n",
            "Iteration 13120 - Loss: 0.12362188246185013\n",
            "Iteration 13130 - Loss: 0.12359641930871187\n",
            "Iteration 13140 - Loss: 0.12357098498828473\n",
            "Iteration 13150 - Loss: 0.12354557944671729\n",
            "Iteration 13160 - Loss: 0.12352020263029874\n",
            "Iteration 13170 - Loss: 0.123494854485458\n",
            "Iteration 13180 - Loss: 0.1234695349587634\n",
            "Iteration 13190 - Loss: 0.12344424399692211\n",
            "Iteration 13200 - Loss: 0.12341898154678002\n",
            "Iteration 13210 - Loss: 0.12339374755532082\n",
            "Iteration 13220 - Loss: 0.12336854196966573\n",
            "Iteration 13230 - Loss: 0.12334336473707319\n",
            "Iteration 13240 - Loss: 0.12331821580493815\n",
            "Iteration 13250 - Loss: 0.12329309512079184\n",
            "Iteration 13260 - Loss: 0.12326800263230112\n",
            "Iteration 13270 - Loss: 0.12324293828726823\n",
            "Iteration 13280 - Loss: 0.12321790203363021\n",
            "Iteration 13290 - Loss: 0.12319289381945836\n",
            "Iteration 13300 - Loss: 0.1231679135929581\n",
            "Iteration 13310 - Loss: 0.12314296130246823\n",
            "Iteration 13320 - Loss: 0.12311803689646057\n",
            "Iteration 13330 - Loss: 0.12309314032353982\n",
            "Iteration 13340 - Loss: 0.12306827153244246\n",
            "Iteration 13350 - Loss: 0.12304343047203702\n",
            "Iteration 13360 - Loss: 0.12301861709132303\n",
            "Iteration 13370 - Loss: 0.12299383133943133\n",
            "Iteration 13380 - Loss: 0.12296907316562285\n",
            "Iteration 13390 - Loss: 0.12294434251928858\n",
            "Iteration 13400 - Loss: 0.12291963934994929\n",
            "Iteration 13410 - Loss: 0.12289496360725471\n",
            "Iteration 13420 - Loss: 0.1228703152409833\n",
            "Iteration 13430 - Loss: 0.12284569420104202\n",
            "Iteration 13440 - Loss: 0.1228211004374656\n",
            "Iteration 13450 - Loss: 0.12279653390041617\n",
            "Iteration 13460 - Loss: 0.1227719945401832\n",
            "Iteration 13470 - Loss: 0.12274748230718248\n",
            "Iteration 13480 - Loss: 0.12272299715195613\n",
            "Iteration 13490 - Loss: 0.12269853902517222\n",
            "Iteration 13500 - Loss: 0.12267410787762405\n",
            "Iteration 13510 - Loss: 0.12264970366023015\n",
            "Iteration 13520 - Loss: 0.12262532632403318\n",
            "Iteration 13530 - Loss: 0.12260097582020059\n",
            "Iteration 13540 - Loss: 0.1225766521000232\n",
            "Iteration 13550 - Loss: 0.12255235511491527\n",
            "Iteration 13560 - Loss: 0.12252808481641426\n",
            "Iteration 13570 - Loss: 0.1225038411561799\n",
            "Iteration 13580 - Loss: 0.12247962408599425\n",
            "Iteration 13590 - Loss: 0.12245543355776116\n",
            "Iteration 13600 - Loss: 0.12243126952350582\n",
            "Iteration 13610 - Loss: 0.12240713193537456\n",
            "Iteration 13620 - Loss: 0.12238302074563408\n",
            "Iteration 13630 - Loss: 0.12235893590667159\n",
            "Iteration 13640 - Loss: 0.12233487737099379\n",
            "Iteration 13650 - Loss: 0.12231084509122707\n",
            "Iteration 13660 - Loss: 0.12228683902011676\n",
            "Iteration 13670 - Loss: 0.12226285911052688\n",
            "Iteration 13680 - Loss: 0.12223890531543992\n",
            "Iteration 13690 - Loss: 0.12221497758795594\n",
            "Iteration 13700 - Loss: 0.1221910758812927\n",
            "Iteration 13710 - Loss: 0.1221672001487853\n",
            "Iteration 13720 - Loss: 0.12214335034388528\n",
            "Iteration 13730 - Loss: 0.12211952642016079\n",
            "Iteration 13740 - Loss: 0.12209572833129587\n",
            "Iteration 13750 - Loss: 0.12207195603109035\n",
            "Iteration 13760 - Loss: 0.1220482094734593\n",
            "Iteration 13770 - Loss: 0.12202448861243266\n",
            "Iteration 13780 - Loss: 0.12200079340215497\n",
            "Iteration 13790 - Loss: 0.12197712379688488\n",
            "Iteration 13800 - Loss: 0.12195347975099498\n",
            "Iteration 13810 - Loss: 0.12192986121897129\n",
            "Iteration 13820 - Loss: 0.12190626815541271\n",
            "Iteration 13830 - Loss: 0.12188270051503111\n",
            "Iteration 13840 - Loss: 0.1218591582526507\n",
            "Iteration 13850 - Loss: 0.12183564132320762\n",
            "Iteration 13860 - Loss: 0.12181214968174965\n",
            "Iteration 13870 - Loss: 0.12178868328343602\n",
            "Iteration 13880 - Loss: 0.12176524208353678\n",
            "Iteration 13890 - Loss: 0.12174182603743253\n",
            "Iteration 13900 - Loss: 0.12171843510061438\n",
            "Iteration 13910 - Loss: 0.12169506922868298\n",
            "Iteration 13920 - Loss: 0.12167172837734887\n",
            "Iteration 13930 - Loss: 0.12164841250243136\n",
            "Iteration 13940 - Loss: 0.1216251215598591\n",
            "Iteration 13950 - Loss: 0.12160185550566904\n",
            "Iteration 13960 - Loss: 0.12157861429600618\n",
            "Iteration 13970 - Loss: 0.12155539788712348\n",
            "Iteration 13980 - Loss: 0.12153220623538145\n",
            "Iteration 13990 - Loss: 0.12150903929724756\n",
            "Iteration 14000 - Loss: 0.12148589702929621\n",
            "Iteration 14010 - Loss: 0.1214627793882083\n",
            "Iteration 14020 - Loss: 0.12143968633077083\n",
            "Iteration 14030 - Loss: 0.12141661781387664\n",
            "Iteration 14040 - Loss: 0.12139357379452385\n",
            "Iteration 14050 - Loss: 0.12137055422981607\n",
            "Iteration 14060 - Loss: 0.12134755907696122\n",
            "Iteration 14070 - Loss: 0.12132458829327246\n",
            "Iteration 14080 - Loss: 0.12130164183616624\n",
            "Iteration 14090 - Loss: 0.12127871966316355\n",
            "Iteration 14100 - Loss: 0.12125582173188847\n",
            "Iteration 14110 - Loss: 0.12123294800006841\n",
            "Iteration 14120 - Loss: 0.12121009842553353\n",
            "Iteration 14130 - Loss: 0.12118727296621677\n",
            "Iteration 14140 - Loss: 0.12116447158015299\n",
            "Iteration 14150 - Loss: 0.12114169422547917\n",
            "Iteration 14160 - Loss: 0.12111894086043373\n",
            "Iteration 14170 - Loss: 0.12109621144335646\n",
            "Iteration 14180 - Loss: 0.12107350593268797\n",
            "Iteration 14190 - Loss: 0.1210508242869698\n",
            "Iteration 14200 - Loss: 0.12102816646484336\n",
            "Iteration 14210 - Loss: 0.12100553242505045\n",
            "Iteration 14220 - Loss: 0.12098292212643237\n",
            "Iteration 14230 - Loss: 0.12096033552792994\n",
            "Iteration 14240 - Loss: 0.12093777258858293\n",
            "Iteration 14250 - Loss: 0.12091523326753008\n",
            "Iteration 14260 - Loss: 0.12089271752400839\n",
            "Iteration 14270 - Loss: 0.12087022531735313\n",
            "Iteration 14280 - Loss: 0.1208477566069974\n",
            "Iteration 14290 - Loss: 0.12082531135247189\n",
            "Iteration 14300 - Loss: 0.12080288951340452\n",
            "Iteration 14310 - Loss: 0.12078049104952028\n",
            "Iteration 14320 - Loss: 0.12075811592064059\n",
            "Iteration 14330 - Loss: 0.12073576408668356\n",
            "Iteration 14340 - Loss: 0.12071343550766307\n",
            "Iteration 14350 - Loss: 0.12069113014368879\n",
            "Iteration 14360 - Loss: 0.12066884795496613\n",
            "Iteration 14370 - Loss: 0.12064658890179547\n",
            "Iteration 14380 - Loss: 0.12062435294457206\n",
            "Iteration 14390 - Loss: 0.12060214004378579\n",
            "Iteration 14400 - Loss: 0.12057995016002106\n",
            "Iteration 14410 - Loss: 0.12055778325395591\n",
            "Iteration 14420 - Loss: 0.12053563928636234\n",
            "Iteration 14430 - Loss: 0.12051351821810595\n",
            "Iteration 14440 - Loss: 0.12049142001014512\n",
            "Iteration 14450 - Loss: 0.1204693446235314\n",
            "Iteration 14460 - Loss: 0.12044729201940875\n",
            "Iteration 14470 - Loss: 0.12042526215901374\n",
            "Iteration 14480 - Loss: 0.12040325500367455\n",
            "Iteration 14490 - Loss: 0.12038127051481133\n",
            "Iteration 14500 - Loss: 0.12035930865393576\n",
            "Iteration 14510 - Loss: 0.12033736938265066\n",
            "Iteration 14520 - Loss: 0.12031545266264965\n",
            "Iteration 14530 - Loss: 0.12029355845571703\n",
            "Iteration 14540 - Loss: 0.12027168672372764\n",
            "Iteration 14550 - Loss: 0.12024983742864616\n",
            "Iteration 14560 - Loss: 0.12022801053252721\n",
            "Iteration 14570 - Loss: 0.12020620599751497\n",
            "Iteration 14580 - Loss: 0.12018442378584274\n",
            "Iteration 14590 - Loss: 0.12016266385983305\n",
            "Iteration 14600 - Loss: 0.12014092618189709\n",
            "Iteration 14610 - Loss: 0.12011921071453427\n",
            "Iteration 14620 - Loss: 0.12009751742033242\n",
            "Iteration 14630 - Loss: 0.1200758462619674\n",
            "Iteration 14640 - Loss: 0.12005419720220249\n",
            "Iteration 14650 - Loss: 0.12003257020388845\n",
            "Iteration 14660 - Loss: 0.12001096522996327\n",
            "Iteration 14670 - Loss: 0.1199893822434517\n",
            "Iteration 14680 - Loss: 0.11996782120746519\n",
            "Iteration 14690 - Loss: 0.11994628208520143\n",
            "Iteration 14700 - Loss: 0.11992476483994427\n",
            "Iteration 14710 - Loss: 0.11990326943506346\n",
            "Iteration 14720 - Loss: 0.11988179583401419\n",
            "Iteration 14730 - Loss: 0.11986034400033695\n",
            "Iteration 14740 - Loss: 0.11983891389765748\n",
            "Iteration 14750 - Loss: 0.11981750548968625\n",
            "Iteration 14760 - Loss: 0.11979611874021819\n",
            "Iteration 14770 - Loss: 0.11977475361313253\n",
            "Iteration 14780 - Loss: 0.11975341007239276\n",
            "Iteration 14790 - Loss: 0.1197320880820459\n",
            "Iteration 14800 - Loss: 0.11971078760622271\n",
            "Iteration 14810 - Loss: 0.11968950860913707\n",
            "Iteration 14820 - Loss: 0.11966825105508624\n",
            "Iteration 14830 - Loss: 0.11964701490844981\n",
            "Iteration 14840 - Loss: 0.11962580013369029\n",
            "Iteration 14850 - Loss: 0.11960460669535238\n",
            "Iteration 14860 - Loss: 0.11958343455806285\n",
            "Iteration 14870 - Loss: 0.11956228368653027\n",
            "Iteration 14880 - Loss: 0.11954115404554481\n",
            "Iteration 14890 - Loss: 0.11952004559997798\n",
            "Iteration 14900 - Loss: 0.11949895831478238\n",
            "Iteration 14910 - Loss: 0.11947789215499148\n",
            "Iteration 14920 - Loss: 0.11945684708571934\n",
            "Iteration 14930 - Loss: 0.1194358230721604\n",
            "Iteration 14940 - Loss: 0.11941482007958923\n",
            "Iteration 14950 - Loss: 0.11939383807336035\n",
            "Iteration 14960 - Loss: 0.11937287701890788\n",
            "Iteration 14970 - Loss: 0.11935193688174539\n",
            "Iteration 14980 - Loss: 0.11933101762746572\n",
            "Iteration 14990 - Loss: 0.11931011922174053\n",
            "Iteration 15000 - Loss: 0.11928924163032031\n",
            "Iteration 15010 - Loss: 0.11926838481903411\n",
            "Iteration 15020 - Loss: 0.11924754875378905\n",
            "Iteration 15030 - Loss: 0.11922673340057047\n",
            "Iteration 15040 - Loss: 0.11920593872544141\n",
            "Iteration 15050 - Loss: 0.1191851646945425\n",
            "Iteration 15060 - Loss: 0.11916441127409186\n",
            "Iteration 15070 - Loss: 0.11914367843038451\n",
            "Iteration 15080 - Loss: 0.11912296612979249\n",
            "Iteration 15090 - Loss: 0.11910227433876455\n",
            "Iteration 15100 - Loss: 0.11908160302382577\n",
            "Iteration 15110 - Loss: 0.1190609521515776\n",
            "Iteration 15120 - Loss: 0.11904032168869731\n",
            "Iteration 15130 - Loss: 0.11901971160193818\n",
            "Iteration 15140 - Loss: 0.11899912185812873\n",
            "Iteration 15150 - Loss: 0.11897855242417324\n",
            "Iteration 15160 - Loss: 0.11895800326705076\n",
            "Iteration 15170 - Loss: 0.1189374743538153\n",
            "Iteration 15180 - Loss: 0.11891696565159568\n",
            "Iteration 15190 - Loss: 0.11889647712759507\n",
            "Iteration 15200 - Loss: 0.11887600874909088\n",
            "Iteration 15210 - Loss: 0.11885556048343476\n",
            "Iteration 15220 - Loss: 0.11883513229805182\n",
            "Iteration 15230 - Loss: 0.11881472416044102\n",
            "Iteration 15240 - Loss: 0.11879433603817471\n",
            "Iteration 15250 - Loss: 0.1187739678988983\n",
            "Iteration 15260 - Loss: 0.11875361971033017\n",
            "Iteration 15270 - Loss: 0.11873329144026154\n",
            "Iteration 15280 - Loss: 0.11871298305655612\n",
            "Iteration 15290 - Loss: 0.11869269452714994\n",
            "Iteration 15300 - Loss: 0.11867242582005122\n",
            "Iteration 15310 - Loss: 0.11865217690334003\n",
            "Iteration 15320 - Loss: 0.11863194774516808\n",
            "Iteration 15330 - Loss: 0.11861173831375875\n",
            "Iteration 15340 - Loss: 0.1185915485774065\n",
            "Iteration 15350 - Loss: 0.11857137850447712\n",
            "Iteration 15360 - Loss: 0.1185512280634071\n",
            "Iteration 15370 - Loss: 0.11853109722270362\n",
            "Iteration 15380 - Loss: 0.11851098595094446\n",
            "Iteration 15390 - Loss: 0.11849089421677758\n",
            "Iteration 15400 - Loss: 0.11847082198892096\n",
            "Iteration 15410 - Loss: 0.11845076923616259\n",
            "Iteration 15420 - Loss: 0.11843073592736003\n",
            "Iteration 15430 - Loss: 0.11841072203144032\n",
            "Iteration 15440 - Loss: 0.11839072751739971\n",
            "Iteration 15450 - Loss: 0.11837075235430365\n",
            "Iteration 15460 - Loss: 0.11835079651128654\n",
            "Iteration 15470 - Loss: 0.11833085995755117\n",
            "Iteration 15480 - Loss: 0.11831094266236897\n",
            "Iteration 15490 - Loss: 0.11829104459507972\n",
            "Iteration 15500 - Loss: 0.11827116572509114\n",
            "Iteration 15510 - Loss: 0.11825130602187901\n",
            "Iteration 15520 - Loss: 0.11823146545498664\n",
            "Iteration 15530 - Loss: 0.11821164399402502\n",
            "Iteration 15540 - Loss: 0.11819184160867238\n",
            "Iteration 15550 - Loss: 0.118172058268674\n",
            "Iteration 15560 - Loss: 0.1181522939438422\n",
            "Iteration 15570 - Loss: 0.11813254860405609\n",
            "Iteration 15580 - Loss: 0.11811282221926113\n",
            "Iteration 15590 - Loss: 0.11809311475946942\n",
            "Iteration 15600 - Loss: 0.11807342619475897\n",
            "Iteration 15610 - Loss: 0.11805375649527394\n",
            "Iteration 15620 - Loss: 0.11803410563122421\n",
            "Iteration 15630 - Loss: 0.11801447357288548\n",
            "Iteration 15640 - Loss: 0.11799486029059852\n",
            "Iteration 15650 - Loss: 0.1179752657547697\n",
            "Iteration 15660 - Loss: 0.11795568993587031\n",
            "Iteration 15670 - Loss: 0.11793613280443643\n",
            "Iteration 15680 - Loss: 0.11791659433106892\n",
            "Iteration 15690 - Loss: 0.11789707448643329\n",
            "Iteration 15700 - Loss: 0.11787757324125904\n",
            "Iteration 15710 - Loss: 0.11785809056634018\n",
            "Iteration 15720 - Loss: 0.11783862643253444\n",
            "Iteration 15730 - Loss: 0.11781918081076342\n",
            "Iteration 15740 - Loss: 0.11779975367201248\n",
            "Iteration 15750 - Loss: 0.11778034498733006\n",
            "Iteration 15760 - Loss: 0.11776095472782822\n",
            "Iteration 15770 - Loss: 0.11774158286468192\n",
            "Iteration 15780 - Loss: 0.11772222936912899\n",
            "Iteration 15790 - Loss: 0.11770289421247003\n",
            "Iteration 15800 - Loss: 0.11768357736606821\n",
            "Iteration 15810 - Loss: 0.1176642788013491\n",
            "Iteration 15820 - Loss: 0.11764499848980027\n",
            "Iteration 15830 - Loss: 0.11762573640297161\n",
            "Iteration 15840 - Loss: 0.11760649251247458\n",
            "Iteration 15850 - Loss: 0.11758726678998253\n",
            "Iteration 15860 - Loss: 0.11756805920723022\n",
            "Iteration 15870 - Loss: 0.11754886973601365\n",
            "Iteration 15880 - Loss: 0.11752969834819023\n",
            "Iteration 15890 - Loss: 0.11751054501567806\n",
            "Iteration 15900 - Loss: 0.11749140971045641\n",
            "Iteration 15910 - Loss: 0.11747229240456478\n",
            "Iteration 15920 - Loss: 0.11745319307010359\n",
            "Iteration 15930 - Loss: 0.11743411167923323\n",
            "Iteration 15940 - Loss: 0.11741504820417431\n",
            "Iteration 15950 - Loss: 0.11739600261720777\n",
            "Iteration 15960 - Loss: 0.11737697489067382\n",
            "Iteration 15970 - Loss: 0.11735796499697267\n",
            "Iteration 15980 - Loss: 0.1173389729085639\n",
            "Iteration 15990 - Loss: 0.11731999859796653\n",
            "Iteration 16000 - Loss: 0.11730104203775851\n",
            "Iteration 16010 - Loss: 0.1172821032005769\n",
            "Iteration 16020 - Loss: 0.11726318205911772\n",
            "Iteration 16030 - Loss: 0.11724427858613544\n",
            "Iteration 16040 - Loss: 0.1172253927544432\n",
            "Iteration 16050 - Loss: 0.11720652453691233\n",
            "Iteration 16060 - Loss: 0.11718767390647243\n",
            "Iteration 16070 - Loss: 0.1171688408361112\n",
            "Iteration 16080 - Loss: 0.11715002529887401\n",
            "Iteration 16090 - Loss: 0.11713122726786414\n",
            "Iteration 16100 - Loss: 0.11711244671624219\n",
            "Iteration 16110 - Loss: 0.11709368361722637\n",
            "Iteration 16120 - Loss: 0.11707493794409195\n",
            "Iteration 16130 - Loss: 0.11705620967017132\n",
            "Iteration 16140 - Loss: 0.11703749876885387\n",
            "Iteration 16150 - Loss: 0.11701880521358562\n",
            "Iteration 16160 - Loss: 0.1170001289778693\n",
            "Iteration 16170 - Loss: 0.11698147003526388\n",
            "Iteration 16180 - Loss: 0.11696282835938504\n",
            "Iteration 16190 - Loss: 0.1169442039239041\n",
            "Iteration 16200 - Loss: 0.11692559670254875\n",
            "Iteration 16210 - Loss: 0.11690700666910227\n",
            "Iteration 16220 - Loss: 0.11688843379740375\n",
            "Iteration 16230 - Loss: 0.1168698780613477\n",
            "Iteration 16240 - Loss: 0.1168513394348843\n",
            "Iteration 16250 - Loss: 0.11683281789201852\n",
            "Iteration 16260 - Loss: 0.11681431340681069\n",
            "Iteration 16270 - Loss: 0.11679582595337593\n",
            "Iteration 16280 - Loss: 0.11677735550588435\n",
            "Iteration 16290 - Loss: 0.11675890203856044\n",
            "Iteration 16300 - Loss: 0.11674046552568326\n",
            "Iteration 16310 - Loss: 0.11672204594158622\n",
            "Iteration 16320 - Loss: 0.11670364326065695\n",
            "Iteration 16330 - Loss: 0.11668525745733695\n",
            "Iteration 16340 - Loss: 0.11666688850612177\n",
            "Iteration 16350 - Loss: 0.11664853638156077\n",
            "Iteration 16360 - Loss: 0.11663020105825662\n",
            "Iteration 16370 - Loss: 0.11661188251086582\n",
            "Iteration 16380 - Loss: 0.11659358071409784\n",
            "Iteration 16390 - Loss: 0.11657529564271539\n",
            "Iteration 16400 - Loss: 0.11655702727153444\n",
            "Iteration 16410 - Loss: 0.11653877557542362\n",
            "Iteration 16420 - Loss: 0.11652054052930427\n",
            "Iteration 16430 - Loss: 0.11650232210815037\n",
            "Iteration 16440 - Loss: 0.11648412028698846\n",
            "Iteration 16450 - Loss: 0.11646593504089724\n",
            "Iteration 16460 - Loss: 0.1164477663450077\n",
            "Iteration 16470 - Loss: 0.11642961417450266\n",
            "Iteration 16480 - Loss: 0.11641147850461717\n",
            "Iteration 16490 - Loss: 0.11639335931063767\n",
            "Iteration 16500 - Loss: 0.11637525656790242\n",
            "Iteration 16510 - Loss: 0.11635717025180113\n",
            "Iteration 16520 - Loss: 0.11633910033777484\n",
            "Iteration 16530 - Loss: 0.1163210468013156\n",
            "Iteration 16540 - Loss: 0.1163030096179668\n",
            "Iteration 16550 - Loss: 0.11628498876332267\n",
            "Iteration 16560 - Loss: 0.1162669842130281\n",
            "Iteration 16570 - Loss: 0.11624899594277872\n",
            "Iteration 16580 - Loss: 0.11623102392832076\n",
            "Iteration 16590 - Loss: 0.11621306814545063\n",
            "Iteration 16600 - Loss: 0.11619512857001518\n",
            "Iteration 16610 - Loss: 0.11617720517791126\n",
            "Iteration 16620 - Loss: 0.11615929794508577\n",
            "Iteration 16630 - Loss: 0.11614140684753535\n",
            "Iteration 16640 - Loss: 0.11612353186130643\n",
            "Iteration 16650 - Loss: 0.11610567296249509\n",
            "Iteration 16660 - Loss: 0.11608783012724676\n",
            "Iteration 16670 - Loss: 0.11607000333175603\n",
            "Iteration 16680 - Loss: 0.11605219255226702\n",
            "Iteration 16690 - Loss: 0.11603439776507268\n",
            "Iteration 16700 - Loss: 0.11601661894651494\n",
            "Iteration 16710 - Loss: 0.11599885607298459\n",
            "Iteration 16720 - Loss: 0.11598110912092086\n",
            "Iteration 16730 - Loss: 0.11596337806681181\n",
            "Iteration 16740 - Loss: 0.11594566288719371\n",
            "Iteration 16750 - Loss: 0.1159279635586512\n",
            "Iteration 16760 - Loss: 0.11591028005781707\n",
            "Iteration 16770 - Loss: 0.11589261236137205\n",
            "Iteration 16780 - Loss: 0.11587496044604488\n",
            "Iteration 16790 - Loss: 0.11585732428861198\n",
            "Iteration 16800 - Loss: 0.11583970386589745\n",
            "Iteration 16810 - Loss: 0.11582209915477304\n",
            "Iteration 16820 - Loss: 0.1158045101321577\n",
            "Iteration 16830 - Loss: 0.1157869367750176\n",
            "Iteration 16840 - Loss: 0.1157693790603663\n",
            "Iteration 16850 - Loss: 0.11575183696526439\n",
            "Iteration 16860 - Loss: 0.11573431046681898\n",
            "Iteration 16870 - Loss: 0.11571679954218436\n",
            "Iteration 16880 - Loss: 0.1156993041685613\n",
            "Iteration 16890 - Loss: 0.11568182432319714\n",
            "Iteration 16900 - Loss: 0.11566435998338546\n",
            "Iteration 16910 - Loss: 0.11564691112646643\n",
            "Iteration 16920 - Loss: 0.11562947772982624\n",
            "Iteration 16930 - Loss: 0.11561205977089699\n",
            "Iteration 16940 - Loss: 0.11559465722715699\n",
            "Iteration 16950 - Loss: 0.11557727007612992\n",
            "Iteration 16960 - Loss: 0.1155598982953857\n",
            "Iteration 16970 - Loss: 0.11554254186253946\n",
            "Iteration 16980 - Loss: 0.1155252007552517\n",
            "Iteration 16990 - Loss: 0.11550787495122855\n",
            "Iteration 17000 - Loss: 0.1154905644282211\n",
            "Iteration 17010 - Loss: 0.11547326916402577\n",
            "Iteration 17020 - Loss: 0.11545598913648357\n",
            "Iteration 17030 - Loss: 0.11543872432348078\n",
            "Iteration 17040 - Loss: 0.11542147470294817\n",
            "Iteration 17050 - Loss: 0.11540424025286129\n",
            "Iteration 17060 - Loss: 0.11538702095123997\n",
            "Iteration 17070 - Loss: 0.11536981677614869\n",
            "Iteration 17080 - Loss: 0.11535262770569601\n",
            "Iteration 17090 - Loss: 0.1153354537180349\n",
            "Iteration 17100 - Loss: 0.11531829479136196\n",
            "Iteration 17110 - Loss: 0.11530115090391818\n",
            "Iteration 17120 - Loss: 0.11528402203398812\n",
            "Iteration 17130 - Loss: 0.11526690815990001\n",
            "Iteration 17140 - Loss: 0.11524980926002586\n",
            "Iteration 17150 - Loss: 0.11523272531278096\n",
            "Iteration 17160 - Loss: 0.11521565629662421\n",
            "Iteration 17170 - Loss: 0.11519860219005744\n",
            "Iteration 17180 - Loss: 0.11518156297162585\n",
            "Iteration 17190 - Loss: 0.11516453861991782\n",
            "Iteration 17200 - Loss: 0.1151475291135642\n",
            "Iteration 17210 - Loss: 0.115130534431239\n",
            "Iteration 17220 - Loss: 0.11511355455165893\n",
            "Iteration 17230 - Loss: 0.11509658945358313\n",
            "Iteration 17240 - Loss: 0.11507963911581323\n",
            "Iteration 17250 - Loss: 0.11506270351719346\n",
            "Iteration 17260 - Loss: 0.11504578263661007\n",
            "Iteration 17270 - Loss: 0.11502887645299152\n",
            "Iteration 17280 - Loss: 0.1150119849453084\n",
            "Iteration 17290 - Loss: 0.11499510809257309\n",
            "Iteration 17300 - Loss: 0.11497824587384013\n",
            "Iteration 17310 - Loss: 0.1149613982682053\n",
            "Iteration 17320 - Loss: 0.1149445652548064\n",
            "Iteration 17330 - Loss: 0.11492774681282245\n",
            "Iteration 17340 - Loss: 0.11491094292147422\n",
            "Iteration 17350 - Loss: 0.11489415356002351\n",
            "Iteration 17360 - Loss: 0.11487737870777323\n",
            "Iteration 17370 - Loss: 0.1148606183440678\n",
            "Iteration 17380 - Loss: 0.1148438724482921\n",
            "Iteration 17390 - Loss: 0.1148271409998723\n",
            "Iteration 17400 - Loss: 0.11481042397827523\n",
            "Iteration 17410 - Loss: 0.11479372136300829\n",
            "Iteration 17420 - Loss: 0.11477703313361942\n",
            "Iteration 17430 - Loss: 0.11476035926969722\n",
            "Iteration 17440 - Loss: 0.11474369975087059\n",
            "Iteration 17450 - Loss: 0.1147270545568086\n",
            "Iteration 17460 - Loss: 0.1147104236672205\n",
            "Iteration 17470 - Loss: 0.11469380706185561\n",
            "Iteration 17480 - Loss: 0.11467720472050327\n",
            "Iteration 17490 - Loss: 0.1146606166229927\n",
            "Iteration 17500 - Loss: 0.1146440427491926\n",
            "Iteration 17510 - Loss: 0.11462748307901162\n",
            "Iteration 17520 - Loss: 0.11461093759239781\n",
            "Iteration 17530 - Loss: 0.11459440626933881\n",
            "Iteration 17540 - Loss: 0.11457788908986143\n",
            "Iteration 17550 - Loss: 0.11456138603403186\n",
            "Iteration 17560 - Loss: 0.11454489708195531\n",
            "Iteration 17570 - Loss: 0.11452842221377618\n",
            "Iteration 17580 - Loss: 0.11451196140967781\n",
            "Iteration 17590 - Loss: 0.11449551464988238\n",
            "Iteration 17600 - Loss: 0.11447908191465068\n",
            "Iteration 17610 - Loss: 0.11446266318428241\n",
            "Iteration 17620 - Loss: 0.11444625843911564\n",
            "Iteration 17630 - Loss: 0.11442986765952708\n",
            "Iteration 17640 - Loss: 0.1144134908259316\n",
            "Iteration 17650 - Loss: 0.11439712791878251\n",
            "Iteration 17660 - Loss: 0.11438077891857121\n",
            "Iteration 17670 - Loss: 0.11436444380582729\n",
            "Iteration 17680 - Loss: 0.11434812256111816\n",
            "Iteration 17690 - Loss: 0.11433181516504928\n",
            "Iteration 17700 - Loss: 0.1143155215982638\n",
            "Iteration 17710 - Loss: 0.11429924184144256\n",
            "Iteration 17720 - Loss: 0.11428297587530416\n",
            "Iteration 17730 - Loss: 0.1142667236806045\n",
            "Iteration 17740 - Loss: 0.11425048523813713\n",
            "Iteration 17750 - Loss: 0.11423426052873276\n",
            "Iteration 17760 - Loss: 0.11421804953325926\n",
            "Iteration 17770 - Loss: 0.11420185223262191\n",
            "Iteration 17780 - Loss: 0.11418566860776275\n",
            "Iteration 17790 - Loss: 0.11416949863966108\n",
            "Iteration 17800 - Loss: 0.11415334230933268\n",
            "Iteration 17810 - Loss: 0.11413719959783039\n",
            "Iteration 17820 - Loss: 0.11412107048624368\n",
            "Iteration 17830 - Loss: 0.1141049549556986\n",
            "Iteration 17840 - Loss: 0.11408885298735758\n",
            "Iteration 17850 - Loss: 0.11407276456241962\n",
            "Iteration 17860 - Loss: 0.1140566896621199\n",
            "Iteration 17870 - Loss: 0.11404062826772997\n",
            "Iteration 17880 - Loss: 0.11402458036055736\n",
            "Iteration 17890 - Loss: 0.11400854592194566\n",
            "Iteration 17900 - Loss: 0.11399252493327475\n",
            "Iteration 17910 - Loss: 0.11397651737595973\n",
            "Iteration 17920 - Loss: 0.11396052323145218\n",
            "Iteration 17930 - Loss: 0.11394454248123884\n",
            "Iteration 17940 - Loss: 0.11392857510684234\n",
            "Iteration 17950 - Loss: 0.11391262108982074\n",
            "Iteration 17960 - Loss: 0.11389668041176748\n",
            "Iteration 17970 - Loss: 0.11388075305431142\n",
            "Iteration 17980 - Loss: 0.11386483899911658\n",
            "Iteration 17990 - Loss: 0.11384893822788235\n",
            "Iteration 18000 - Loss: 0.11383305072234284\n",
            "Iteration 18010 - Loss: 0.11381717646426753\n",
            "Iteration 18020 - Loss: 0.11380131543546051\n",
            "Iteration 18030 - Loss: 0.11378546761776091\n",
            "Iteration 18040 - Loss: 0.11376963299304252\n",
            "Iteration 18050 - Loss: 0.11375381154321362\n",
            "Iteration 18060 - Loss: 0.11373800325021734\n",
            "Iteration 18070 - Loss: 0.11372220809603105\n",
            "Iteration 18080 - Loss: 0.11370642606266665\n",
            "Iteration 18090 - Loss: 0.11369065713217041\n",
            "Iteration 18100 - Loss: 0.11367490128662243\n",
            "Iteration 18110 - Loss: 0.11365915850813751\n",
            "Iteration 18120 - Loss: 0.11364342877886413\n",
            "Iteration 18130 - Loss: 0.11362771208098488\n",
            "Iteration 18140 - Loss: 0.11361200839671624\n",
            "Iteration 18150 - Loss: 0.11359631770830837\n",
            "Iteration 18160 - Loss: 0.11358063999804544\n",
            "Iteration 18170 - Loss: 0.11356497524824499\n",
            "Iteration 18180 - Loss: 0.11354932344125808\n",
            "Iteration 18190 - Loss: 0.11353368455946973\n",
            "Iteration 18200 - Loss: 0.1135180585852977\n",
            "Iteration 18210 - Loss: 0.11350244550119351\n",
            "Iteration 18220 - Loss: 0.11348684528964183\n",
            "Iteration 18230 - Loss: 0.11347125793316033\n",
            "Iteration 18240 - Loss: 0.1134556834143\n",
            "Iteration 18250 - Loss: 0.11344012171564459\n",
            "Iteration 18260 - Loss: 0.11342457281981094\n",
            "Iteration 18270 - Loss: 0.1134090367094486\n",
            "Iteration 18280 - Loss: 0.11339351336724003\n",
            "Iteration 18290 - Loss: 0.11337800277590004\n",
            "Iteration 18300 - Loss: 0.11336250491817647\n",
            "Iteration 18310 - Loss: 0.11334701977684945\n",
            "Iteration 18320 - Loss: 0.1133315473347314\n",
            "Iteration 18330 - Loss: 0.11331608757466738\n",
            "Iteration 18340 - Loss: 0.11330064047953466\n",
            "Iteration 18350 - Loss: 0.11328520603224257\n",
            "Iteration 18360 - Loss: 0.11326978421573262\n",
            "Iteration 18370 - Loss: 0.11325437501297853\n",
            "Iteration 18380 - Loss: 0.11323897840698575\n",
            "Iteration 18390 - Loss: 0.11322359438079185\n",
            "Iteration 18400 - Loss: 0.11320822291746603\n",
            "Iteration 18410 - Loss: 0.1131928640001093\n",
            "Iteration 18420 - Loss: 0.1131775176118544\n",
            "Iteration 18430 - Loss: 0.11316218373586555\n",
            "Iteration 18440 - Loss: 0.11314686235533854\n",
            "Iteration 18450 - Loss: 0.11313155345350055\n",
            "Iteration 18460 - Loss: 0.11311625701361017\n",
            "Iteration 18470 - Loss: 0.11310097301895733\n",
            "Iteration 18480 - Loss: 0.11308570145286292\n",
            "Iteration 18490 - Loss: 0.1130704422986793\n",
            "Iteration 18500 - Loss: 0.11305519553978964\n",
            "Iteration 18510 - Loss: 0.11303996115960824\n",
            "Iteration 18520 - Loss: 0.11302473914158015\n",
            "Iteration 18530 - Loss: 0.11300952946918134\n",
            "Iteration 18540 - Loss: 0.11299433212591878\n",
            "Iteration 18550 - Loss: 0.11297914709532972\n",
            "Iteration 18560 - Loss: 0.11296397436098225\n",
            "Iteration 18570 - Loss: 0.11294881390647499\n",
            "Iteration 18580 - Loss: 0.11293366571543698\n",
            "Iteration 18590 - Loss: 0.11291852977152778\n",
            "Iteration 18600 - Loss: 0.11290340605843703\n",
            "Iteration 18610 - Loss: 0.11288829455988482\n",
            "Iteration 18620 - Loss: 0.11287319525962143\n",
            "Iteration 18630 - Loss: 0.11285810814142713\n",
            "Iteration 18640 - Loss: 0.11284303318911229\n",
            "Iteration 18650 - Loss: 0.1128279703865173\n",
            "Iteration 18660 - Loss: 0.11281291971751242\n",
            "Iteration 18670 - Loss: 0.1127978811659974\n",
            "Iteration 18680 - Loss: 0.11278285471590241\n",
            "Iteration 18690 - Loss: 0.11276784035118681\n",
            "Iteration 18700 - Loss: 0.11275283805583959\n",
            "Iteration 18710 - Loss: 0.11273784781387937\n",
            "Iteration 18720 - Loss: 0.11272286960935449\n",
            "Iteration 18730 - Loss: 0.11270790342634224\n",
            "Iteration 18740 - Loss: 0.1126929492489494\n",
            "Iteration 18750 - Loss: 0.11267800706131222\n",
            "Iteration 18760 - Loss: 0.112663076847596\n",
            "Iteration 18770 - Loss: 0.11264815859199498\n",
            "Iteration 18780 - Loss: 0.11263325227873278\n",
            "Iteration 18790 - Loss: 0.1126183578920618\n",
            "Iteration 18800 - Loss: 0.1126034754162634\n",
            "Iteration 18810 - Loss: 0.11258860483564782\n",
            "Iteration 18820 - Loss: 0.11257374613455398\n",
            "Iteration 18830 - Loss: 0.11255889929734963\n",
            "Iteration 18840 - Loss: 0.11254406430843122\n",
            "Iteration 18850 - Loss: 0.11252924115222356\n",
            "Iteration 18860 - Loss: 0.11251442981318019\n",
            "Iteration 18870 - Loss: 0.1124996302757829\n",
            "Iteration 18880 - Loss: 0.11248484252454209\n",
            "Iteration 18890 - Loss: 0.11247006654399619\n",
            "Iteration 18900 - Loss: 0.11245530231871213\n",
            "Iteration 18910 - Loss: 0.1124405498332848\n",
            "Iteration 18920 - Loss: 0.11242580907233744\n",
            "Iteration 18930 - Loss: 0.11241108002052116\n",
            "Iteration 18940 - Loss: 0.112396362662515\n",
            "Iteration 18950 - Loss: 0.11238165698302618\n",
            "Iteration 18960 - Loss: 0.11236696296678948\n",
            "Iteration 18970 - Loss: 0.11235228059856756\n",
            "Iteration 18980 - Loss: 0.11233760986315089\n",
            "Iteration 18990 - Loss: 0.11232295074535753\n",
            "Iteration 19000 - Loss: 0.11230830323003309\n",
            "Iteration 19010 - Loss: 0.11229366730205076\n",
            "Iteration 19020 - Loss: 0.11227904294631114\n",
            "Iteration 19030 - Loss: 0.1122644301477423\n",
            "Iteration 19040 - Loss: 0.1122498288912996\n",
            "Iteration 19050 - Loss: 0.11223523916196554\n",
            "Iteration 19060 - Loss: 0.11222066094475018\n",
            "Iteration 19070 - Loss: 0.1122060942246903\n",
            "Iteration 19080 - Loss: 0.11219153898685004\n",
            "Iteration 19090 - Loss: 0.11217699521632052\n",
            "Iteration 19100 - Loss: 0.11216246289821968\n",
            "Iteration 19110 - Loss: 0.11214794201769243\n",
            "Iteration 19120 - Loss: 0.11213343255991061\n",
            "Iteration 19130 - Loss: 0.11211893451007263\n",
            "Iteration 19140 - Loss: 0.11210444785340379\n",
            "Iteration 19150 - Loss: 0.11208997257515588\n",
            "Iteration 19160 - Loss: 0.11207550866060736\n",
            "Iteration 19170 - Loss: 0.1120610560950632\n",
            "Iteration 19180 - Loss: 0.11204661486385482\n",
            "Iteration 19190 - Loss: 0.11203218495234003\n",
            "Iteration 19200 - Loss: 0.11201776634590288\n",
            "Iteration 19210 - Loss: 0.11200335902995402\n",
            "Iteration 19220 - Loss: 0.1119889629899298\n",
            "Iteration 19230 - Loss: 0.11197457821129313\n",
            "Iteration 19240 - Loss: 0.111960204679533\n",
            "Iteration 19250 - Loss: 0.11194584238016418\n",
            "Iteration 19260 - Loss: 0.11193149129872759\n",
            "Iteration 19270 - Loss: 0.1119171514207901\n",
            "Iteration 19280 - Loss: 0.11190282273194427\n",
            "Iteration 19290 - Loss: 0.11188850521780852\n",
            "Iteration 19300 - Loss: 0.11187419886402711\n",
            "Iteration 19310 - Loss: 0.11185990365626991\n",
            "Iteration 19320 - Loss: 0.11184561958023227\n",
            "Iteration 19330 - Loss: 0.11183134662163534\n",
            "Iteration 19340 - Loss: 0.11181708476622565\n",
            "Iteration 19350 - Loss: 0.11180283399977509\n",
            "Iteration 19360 - Loss: 0.11178859430808102\n",
            "Iteration 19370 - Loss: 0.11177436567696619\n",
            "Iteration 19380 - Loss: 0.11176014809227845\n",
            "Iteration 19390 - Loss: 0.11174594153989104\n",
            "Iteration 19400 - Loss: 0.11173174600570206\n",
            "Iteration 19410 - Loss: 0.11171756147563512\n",
            "Iteration 19420 - Loss: 0.11170338793563858\n",
            "Iteration 19430 - Loss: 0.11168922537168582\n",
            "Iteration 19440 - Loss: 0.11167507376977512\n",
            "Iteration 19450 - Loss: 0.11166093311592971\n",
            "Iteration 19460 - Loss: 0.11164680339619759\n",
            "Iteration 19470 - Loss: 0.11163268459665149\n",
            "Iteration 19480 - Loss: 0.11161857670338898\n",
            "Iteration 19490 - Loss: 0.11160447970253193\n",
            "Iteration 19500 - Loss: 0.11159039358022707\n",
            "Iteration 19510 - Loss: 0.11157631832264565\n",
            "Iteration 19520 - Loss: 0.11156225391598334\n",
            "Iteration 19530 - Loss: 0.1115482003464601\n",
            "Iteration 19540 - Loss: 0.11153415760032037\n",
            "Iteration 19550 - Loss: 0.111520125663833\n",
            "Iteration 19560 - Loss: 0.11150610452329092\n",
            "Iteration 19570 - Loss: 0.1114920941650113\n",
            "Iteration 19580 - Loss: 0.11147809457533542\n",
            "Iteration 19590 - Loss: 0.11146410574062873\n",
            "Iteration 19600 - Loss: 0.1114501276472807\n",
            "Iteration 19610 - Loss: 0.1114361602817046\n",
            "Iteration 19620 - Loss: 0.11142220363033789\n",
            "Iteration 19630 - Loss: 0.11140825767964169\n",
            "Iteration 19640 - Loss: 0.11139432241610105\n",
            "Iteration 19650 - Loss: 0.11138039782622466\n",
            "Iteration 19660 - Loss: 0.11136648389654509\n",
            "Iteration 19670 - Loss: 0.11135258061361841\n",
            "Iteration 19680 - Loss: 0.11133868796402442\n",
            "Iteration 19690 - Loss: 0.11132480593436625\n",
            "Iteration 19700 - Loss: 0.11131093451127069\n",
            "Iteration 19710 - Loss: 0.11129707368138803\n",
            "Iteration 19720 - Loss: 0.11128322343139174\n",
            "Iteration 19730 - Loss: 0.1112693837479789\n",
            "Iteration 19740 - Loss: 0.11125555461786954\n",
            "Iteration 19750 - Loss: 0.1112417360278072\n",
            "Iteration 19760 - Loss: 0.11122792796455844\n",
            "Iteration 19770 - Loss: 0.111214130414913\n",
            "Iteration 19780 - Loss: 0.11120034336568367\n",
            "Iteration 19790 - Loss: 0.11118656680370628\n",
            "Iteration 19800 - Loss: 0.11117280071583965\n",
            "Iteration 19810 - Loss: 0.11115904508896549\n",
            "Iteration 19820 - Loss: 0.11114529990998831\n",
            "Iteration 19830 - Loss: 0.11113156516583568\n",
            "Iteration 19840 - Loss: 0.11111784084345756\n",
            "Iteration 19850 - Loss: 0.11110412692982696\n",
            "Iteration 19860 - Loss: 0.11109042341193934\n",
            "Iteration 19870 - Loss: 0.11107673027681286\n",
            "Iteration 19880 - Loss: 0.11106304751148828\n",
            "Iteration 19890 - Loss: 0.11104937510302883\n",
            "Iteration 19900 - Loss: 0.11103571303852007\n",
            "Iteration 19910 - Loss: 0.1110220613050703\n",
            "Iteration 19920 - Loss: 0.1110084198898097\n",
            "Iteration 19930 - Loss: 0.11099478877989127\n",
            "Iteration 19940 - Loss: 0.11098116796248995\n",
            "Iteration 19950 - Loss: 0.11096755742480295\n",
            "Iteration 19960 - Loss: 0.11095395715404985\n",
            "Iteration 19970 - Loss: 0.11094036713747187\n",
            "Iteration 19980 - Loss: 0.11092678736233286\n",
            "Iteration 19990 - Loss: 0.11091321781591823\n",
            "Iteration 20000 - Loss: 0.11089965848553561\n",
            "Iteration 20010 - Loss: 0.11088610935851453\n",
            "Iteration 20020 - Loss: 0.11087257042220629\n",
            "Iteration 20030 - Loss: 0.11085904166398393\n",
            "Iteration 20040 - Loss: 0.11084552307124262\n",
            "Iteration 20050 - Loss: 0.1108320146313989\n",
            "Iteration 20060 - Loss: 0.11081851633189117\n",
            "Iteration 20070 - Loss: 0.1108050281601793\n",
            "Iteration 20080 - Loss: 0.11079155010374489\n",
            "Iteration 20090 - Loss: 0.11077808215009104\n",
            "Iteration 20100 - Loss: 0.1107646242867423\n",
            "Iteration 20110 - Loss: 0.11075117650124461\n",
            "Iteration 20120 - Loss: 0.11073773878116545\n",
            "Iteration 20130 - Loss: 0.11072431111409352\n",
            "Iteration 20140 - Loss: 0.11071089348763885\n",
            "Iteration 20150 - Loss: 0.1106974858894327\n",
            "Iteration 20160 - Loss: 0.11068408830712755\n",
            "Iteration 20170 - Loss: 0.11067070072839709\n",
            "Iteration 20180 - Loss: 0.110657323140936\n",
            "Iteration 20190 - Loss: 0.11064395553246013\n",
            "Iteration 20200 - Loss: 0.11063059789070624\n",
            "Iteration 20210 - Loss: 0.1106172502034322\n",
            "Iteration 20220 - Loss: 0.11060391245841666\n",
            "Iteration 20230 - Loss: 0.11059058464345925\n",
            "Iteration 20240 - Loss: 0.1105772667463804\n",
            "Iteration 20250 - Loss: 0.11056395875502124\n",
            "Iteration 20260 - Loss: 0.11055066065724382\n",
            "Iteration 20270 - Loss: 0.1105373724409308\n",
            "Iteration 20280 - Loss: 0.11052409409398536\n",
            "Iteration 20290 - Loss: 0.11051082560433158\n",
            "Iteration 20300 - Loss: 0.11049756695991372\n",
            "Iteration 20310 - Loss: 0.11048431814869687\n",
            "Iteration 20320 - Loss: 0.11047107915866644\n",
            "Iteration 20330 - Loss: 0.11045784997782832\n",
            "Iteration 20340 - Loss: 0.11044463059420878\n",
            "Iteration 20350 - Loss: 0.11043142099585436\n",
            "Iteration 20360 - Loss: 0.11041822117083196\n",
            "Iteration 20370 - Loss: 0.11040503110722874\n",
            "Iteration 20380 - Loss: 0.11039185079315203\n",
            "Iteration 20390 - Loss: 0.11037868021672925\n",
            "Iteration 20400 - Loss: 0.11036551936610807\n",
            "Iteration 20410 - Loss: 0.1103523682294561\n",
            "Iteration 20420 - Loss: 0.11033922679496115\n",
            "Iteration 20430 - Loss: 0.11032609505083077\n",
            "Iteration 20440 - Loss: 0.11031297298529273\n",
            "Iteration 20450 - Loss: 0.11029986058659445\n",
            "Iteration 20460 - Loss: 0.11028675784300336\n",
            "Iteration 20470 - Loss: 0.11027366474280668\n",
            "Iteration 20480 - Loss: 0.11026058127431136\n",
            "Iteration 20490 - Loss: 0.11024750742584406\n",
            "Iteration 20500 - Loss: 0.11023444318575117\n",
            "Iteration 20510 - Loss: 0.11022138854239867\n",
            "Iteration 20520 - Loss: 0.11020834348417219\n",
            "Iteration 20530 - Loss: 0.11019530799947698\n",
            "Iteration 20540 - Loss: 0.11018228207673754\n",
            "Iteration 20550 - Loss: 0.11016926570439813\n",
            "Iteration 20560 - Loss: 0.1101562588709224\n",
            "Iteration 20570 - Loss: 0.11014326156479305\n",
            "Iteration 20580 - Loss: 0.11013027377451269\n",
            "Iteration 20590 - Loss: 0.11011729548860288\n",
            "Iteration 20600 - Loss: 0.11010432669560433\n",
            "Iteration 20610 - Loss: 0.11009136738407722\n",
            "Iteration 20620 - Loss: 0.11007841754260089\n",
            "Iteration 20630 - Loss: 0.11006547715977366\n",
            "Iteration 20640 - Loss: 0.11005254622421311\n",
            "Iteration 20650 - Loss: 0.11003962472455572\n",
            "Iteration 20660 - Loss: 0.11002671264945717\n",
            "Iteration 20670 - Loss: 0.11001380998759194\n",
            "Iteration 20680 - Loss: 0.1100009167276535\n",
            "Iteration 20690 - Loss: 0.10998803285835429\n",
            "Iteration 20700 - Loss: 0.10997515836842552\n",
            "Iteration 20710 - Loss: 0.10996229324661717\n",
            "Iteration 20720 - Loss: 0.1099494374816981\n",
            "Iteration 20730 - Loss: 0.10993659106245586\n",
            "Iteration 20740 - Loss: 0.10992375397769663\n",
            "Iteration 20750 - Loss: 0.10991092621624533\n",
            "Iteration 20760 - Loss: 0.10989810776694545\n",
            "Iteration 20770 - Loss: 0.10988529861865903\n",
            "Iteration 20780 - Loss: 0.10987249876026664\n",
            "Iteration 20790 - Loss: 0.10985970818066747\n",
            "Iteration 20800 - Loss: 0.10984692686877899\n",
            "Iteration 20810 - Loss: 0.10983415481353709\n",
            "Iteration 20820 - Loss: 0.10982139200389612\n",
            "Iteration 20830 - Loss: 0.10980863842882892\n",
            "Iteration 20840 - Loss: 0.10979589407732616\n",
            "Iteration 20850 - Loss: 0.1097831589383972\n",
            "Iteration 20860 - Loss: 0.10977043300106948\n",
            "Iteration 20870 - Loss: 0.10975771625438846\n",
            "Iteration 20880 - Loss: 0.10974500868741811\n",
            "Iteration 20890 - Loss: 0.10973231028924006\n",
            "Iteration 20900 - Loss: 0.10971962104895426\n",
            "Iteration 20910 - Loss: 0.10970694095567886\n",
            "Iteration 20920 - Loss: 0.10969426999854943\n",
            "Iteration 20930 - Loss: 0.10968160816672004\n",
            "Iteration 20940 - Loss: 0.10966895544936243\n",
            "Iteration 20950 - Loss: 0.10965631183566628\n",
            "Iteration 20960 - Loss: 0.10964367731483901\n",
            "Iteration 20970 - Loss: 0.1096310518761059\n",
            "Iteration 20980 - Loss: 0.10961843550871005\n",
            "Iteration 20990 - Loss: 0.10960582820191217\n",
            "Iteration 21000 - Loss: 0.10959322994499072\n",
            "Iteration 21010 - Loss: 0.10958064072724183\n",
            "Iteration 21020 - Loss: 0.10956806053797898\n",
            "Iteration 21030 - Loss: 0.10955548936653359\n",
            "Iteration 21040 - Loss: 0.10954292720225457\n",
            "Iteration 21050 - Loss: 0.10953037403450809\n",
            "Iteration 21060 - Loss: 0.10951782985267787\n",
            "Iteration 21070 - Loss: 0.10950529464616517\n",
            "Iteration 21080 - Loss: 0.10949276840438849\n",
            "Iteration 21090 - Loss: 0.10948025111678385\n",
            "Iteration 21100 - Loss: 0.10946774277280438\n",
            "Iteration 21110 - Loss: 0.10945524336192072\n",
            "Iteration 21120 - Loss: 0.10944275287362049\n",
            "Iteration 21130 - Loss: 0.1094302712974087\n",
            "Iteration 21140 - Loss: 0.10941779862280741\n",
            "Iteration 21150 - Loss: 0.109405334839356\n",
            "Iteration 21160 - Loss: 0.10939287993661076\n",
            "Iteration 21170 - Loss: 0.1093804339041451\n",
            "Iteration 21180 - Loss: 0.10936799673154943\n",
            "Iteration 21190 - Loss: 0.10935556840843133\n",
            "Iteration 21200 - Loss: 0.10934314892441496\n",
            "Iteration 21210 - Loss: 0.10933073826914178\n",
            "Iteration 21220 - Loss: 0.10931833643226993\n",
            "Iteration 21230 - Loss: 0.10930594340347442\n",
            "Iteration 21240 - Loss: 0.10929355917244708\n",
            "Iteration 21250 - Loss: 0.10928118372889666\n",
            "Iteration 21260 - Loss: 0.1092688170625483\n",
            "Iteration 21270 - Loss: 0.1092564591631443\n",
            "Iteration 21280 - Loss: 0.10924411002044325\n",
            "Iteration 21290 - Loss: 0.10923176962422061\n",
            "Iteration 21300 - Loss: 0.10921943796426829\n",
            "Iteration 21310 - Loss: 0.10920711503039489\n",
            "Iteration 21320 - Loss: 0.10919480081242547\n",
            "Iteration 21330 - Loss: 0.10918249530020165\n",
            "Iteration 21340 - Loss: 0.1091701984835815\n",
            "Iteration 21350 - Loss: 0.10915791035243946\n",
            "Iteration 21360 - Loss: 0.1091456308966663\n",
            "Iteration 21370 - Loss: 0.1091333601061694\n",
            "Iteration 21380 - Loss: 0.10912109797087223\n",
            "Iteration 21390 - Loss: 0.10910884448071481\n",
            "Iteration 21400 - Loss: 0.1090965996256531\n",
            "Iteration 21410 - Loss: 0.10908436339565951\n",
            "Iteration 21420 - Loss: 0.10907213578072247\n",
            "Iteration 21430 - Loss: 0.1090599167708469\n",
            "Iteration 21440 - Loss: 0.1090477063560534\n",
            "Iteration 21450 - Loss: 0.10903550452637904\n",
            "Iteration 21460 - Loss: 0.10902331127187678\n",
            "Iteration 21470 - Loss: 0.10901112658261551\n",
            "Iteration 21480 - Loss: 0.10899895044868037\n",
            "Iteration 21490 - Loss: 0.10898678286017222\n",
            "Iteration 21500 - Loss: 0.10897462380720802\n",
            "Iteration 21510 - Loss: 0.10896247327992044\n",
            "Iteration 21520 - Loss: 0.10895033126845816\n",
            "Iteration 21530 - Loss: 0.10893819776298576\n",
            "Iteration 21540 - Loss: 0.10892607275368331\n",
            "Iteration 21550 - Loss: 0.10891395623074696\n",
            "Iteration 21560 - Loss: 0.10890184818438851\n",
            "Iteration 21570 - Loss: 0.10888974860483532\n",
            "Iteration 21580 - Loss: 0.10887765748233046\n",
            "Iteration 21590 - Loss: 0.1088655748071329\n",
            "Iteration 21600 - Loss: 0.10885350056951669\n",
            "Iteration 21610 - Loss: 0.10884143475977205\n",
            "Iteration 21620 - Loss: 0.1088293773682043\n",
            "Iteration 21630 - Loss: 0.10881732838513444\n",
            "Iteration 21640 - Loss: 0.10880528780089888\n",
            "Iteration 21650 - Loss: 0.10879325560584946\n",
            "Iteration 21660 - Loss: 0.10878123179035366\n",
            "Iteration 21670 - Loss: 0.10876921634479396\n",
            "Iteration 21680 - Loss: 0.1087572092595685\n",
            "Iteration 21690 - Loss: 0.10874521052509056\n",
            "Iteration 21700 - Loss: 0.10873322013178875\n",
            "Iteration 21710 - Loss: 0.10872123807010697\n",
            "Iteration 21720 - Loss: 0.10870926433050439\n",
            "Iteration 21730 - Loss: 0.10869729890345514\n",
            "Iteration 21740 - Loss: 0.10868534177944882\n",
            "Iteration 21750 - Loss: 0.10867339294898996\n",
            "Iteration 21760 - Loss: 0.10866145240259817\n",
            "Iteration 21770 - Loss: 0.10864952013080824\n",
            "Iteration 21780 - Loss: 0.10863759612417004\n",
            "Iteration 21790 - Loss: 0.10862568037324821\n",
            "Iteration 21800 - Loss: 0.10861377286862248\n",
            "Iteration 21810 - Loss: 0.10860187360088777\n",
            "Iteration 21820 - Loss: 0.1085899825606535\n",
            "Iteration 21830 - Loss: 0.10857809973854428\n",
            "Iteration 21840 - Loss: 0.10856622512519933\n",
            "Iteration 21850 - Loss: 0.10855435871127296\n",
            "Iteration 21860 - Loss: 0.10854250048743416\n",
            "Iteration 21870 - Loss: 0.10853065044436656\n",
            "Iteration 21880 - Loss: 0.10851880857276872\n",
            "Iteration 21890 - Loss: 0.10850697486335376\n",
            "Iteration 21900 - Loss: 0.10849514930684952\n",
            "Iteration 21910 - Loss: 0.10848333189399852\n",
            "Iteration 21920 - Loss: 0.10847152261555788\n",
            "Iteration 21930 - Loss: 0.10845972146229921\n",
            "Iteration 21940 - Loss: 0.10844792842500886\n",
            "Iteration 21950 - Loss: 0.10843614349448749\n",
            "Iteration 21960 - Loss: 0.10842436666155046\n",
            "Iteration 21970 - Loss: 0.10841259791702737\n",
            "Iteration 21980 - Loss: 0.10840083725176258\n",
            "Iteration 21990 - Loss: 0.10838908465661451\n",
            "Iteration 22000 - Loss: 0.10837734012245623\n",
            "Iteration 22010 - Loss: 0.1083656036401749\n",
            "Iteration 22020 - Loss: 0.1083538752006723\n",
            "Iteration 22030 - Loss: 0.10834215479486425\n",
            "Iteration 22040 - Loss: 0.108330442413681\n",
            "Iteration 22050 - Loss: 0.10831873804806688\n",
            "Iteration 22060 - Loss: 0.10830704168898057\n",
            "Iteration 22070 - Loss: 0.10829535332739493\n",
            "Iteration 22080 - Loss: 0.10828367295429683\n",
            "Iteration 22090 - Loss: 0.10827200056068738\n",
            "Iteration 22100 - Loss: 0.10826033613758172\n",
            "Iteration 22110 - Loss: 0.10824867967600904\n",
            "Iteration 22120 - Loss: 0.10823703116701272\n",
            "Iteration 22130 - Loss: 0.10822539060164989\n",
            "Iteration 22140 - Loss: 0.10821375797099188\n",
            "Iteration 22150 - Loss: 0.10820213326612393\n",
            "Iteration 22160 - Loss: 0.10819051647814508\n",
            "Iteration 22170 - Loss: 0.10817890759816838\n",
            "Iteration 22180 - Loss: 0.10816730661732069\n",
            "Iteration 22190 - Loss: 0.10815571352674283\n",
            "Iteration 22200 - Loss: 0.1081441283175892\n",
            "Iteration 22210 - Loss: 0.10813255098102828\n",
            "Iteration 22220 - Loss: 0.10812098150824202\n",
            "Iteration 22230 - Loss: 0.1081094198904263\n",
            "Iteration 22240 - Loss: 0.10809786611879052\n",
            "Iteration 22250 - Loss: 0.10808632018455806\n",
            "Iteration 22260 - Loss: 0.10807478207896558\n",
            "Iteration 22270 - Loss: 0.10806325179326368\n",
            "Iteration 22280 - Loss: 0.10805172931871622\n",
            "Iteration 22290 - Loss: 0.10804021464660096\n",
            "Iteration 22300 - Loss: 0.10802870776820901\n",
            "Iteration 22310 - Loss: 0.10801720867484498\n",
            "Iteration 22320 - Loss: 0.10800571735782713\n",
            "Iteration 22330 - Loss: 0.10799423380848694\n",
            "Iteration 22340 - Loss: 0.10798275801816955\n",
            "Iteration 22350 - Loss: 0.10797128997823334\n",
            "Iteration 22360 - Loss: 0.10795982968005005\n",
            "Iteration 22370 - Loss: 0.10794837711500505\n",
            "Iteration 22380 - Loss: 0.10793693227449666\n",
            "Iteration 22390 - Loss: 0.1079254951499368\n",
            "Iteration 22400 - Loss: 0.1079140657327504\n",
            "Iteration 22410 - Loss: 0.10790264401437588\n",
            "Iteration 22420 - Loss: 0.10789122998626477\n",
            "Iteration 22430 - Loss: 0.10787982363988166\n",
            "Iteration 22440 - Loss: 0.10786842496670457\n",
            "Iteration 22450 - Loss: 0.10785703395822452\n",
            "Iteration 22460 - Loss: 0.10784565060594559\n",
            "Iteration 22470 - Loss: 0.10783427490138509\n",
            "Iteration 22480 - Loss: 0.10782290683607325\n",
            "Iteration 22490 - Loss: 0.10781154640155341\n",
            "Iteration 22500 - Loss: 0.10780019358938199\n",
            "Iteration 22510 - Loss: 0.1077888483911282\n",
            "Iteration 22520 - Loss: 0.10777751079837455\n",
            "Iteration 22530 - Loss: 0.10776618080271601\n",
            "Iteration 22540 - Loss: 0.10775485839576081\n",
            "Iteration 22550 - Loss: 0.1077435435691301\n",
            "Iteration 22560 - Loss: 0.10773223631445751\n",
            "Iteration 22570 - Loss: 0.10772093662339\n",
            "Iteration 22580 - Loss: 0.10770964448758705\n",
            "Iteration 22590 - Loss: 0.10769835989872073\n",
            "Iteration 22600 - Loss: 0.10768708284847636\n",
            "Iteration 22610 - Loss: 0.10767581332855167\n",
            "Iteration 22620 - Loss: 0.10766455133065705\n",
            "Iteration 22630 - Loss: 0.1076532968465158\n",
            "Iteration 22640 - Loss: 0.1076420498678637\n",
            "Iteration 22650 - Loss: 0.10763081038644928\n",
            "Iteration 22660 - Loss: 0.10761957839403359\n",
            "Iteration 22670 - Loss: 0.10760835388239028\n",
            "Iteration 22680 - Loss: 0.10759713684330555\n",
            "Iteration 22690 - Loss: 0.10758592726857819\n",
            "Iteration 22700 - Loss: 0.1075747251500195\n",
            "Iteration 22710 - Loss: 0.10756353047945318\n",
            "Iteration 22720 - Loss: 0.10755234324871545\n",
            "Iteration 22730 - Loss: 0.10754116344965499\n",
            "Iteration 22740 - Loss: 0.10752999107413282\n",
            "Iteration 22750 - Loss: 0.10751882611402243\n",
            "Iteration 22760 - Loss: 0.10750766856120952\n",
            "Iteration 22770 - Loss: 0.10749651840759239\n",
            "Iteration 22780 - Loss: 0.1074853756450814\n",
            "Iteration 22790 - Loss: 0.10747424026559935\n",
            "Iteration 22800 - Loss: 0.10746311226108128\n",
            "Iteration 22810 - Loss: 0.10745199162347441\n",
            "Iteration 22820 - Loss: 0.10744087834473826\n",
            "Iteration 22830 - Loss: 0.10742977241684445\n",
            "Iteration 22840 - Loss: 0.10741867383177695\n",
            "Iteration 22850 - Loss: 0.10740758258153169\n",
            "Iteration 22860 - Loss: 0.10739649865811678\n",
            "Iteration 22870 - Loss: 0.10738542205355245\n",
            "Iteration 22880 - Loss: 0.1073743527598711\n",
            "Iteration 22890 - Loss: 0.10736329076911692\n",
            "Iteration 22900 - Loss: 0.10735223607334646\n",
            "Iteration 22910 - Loss: 0.10734118866462802\n",
            "Iteration 22920 - Loss: 0.10733014853504202\n",
            "Iteration 22930 - Loss: 0.10731911567668087\n",
            "Iteration 22940 - Loss: 0.10730809008164864\n",
            "Iteration 22950 - Loss: 0.10729707174206174\n",
            "Iteration 22960 - Loss: 0.10728606065004818\n",
            "Iteration 22970 - Loss: 0.1072750567977479\n",
            "Iteration 22980 - Loss: 0.10726406017731273\n",
            "Iteration 22990 - Loss: 0.10725307078090635\n",
            "Iteration 23000 - Loss: 0.10724208860070403\n",
            "Iteration 23010 - Loss: 0.10723111362889302\n",
            "Iteration 23020 - Loss: 0.10722014585767235\n",
            "Iteration 23030 - Loss: 0.10720918527925272\n",
            "Iteration 23040 - Loss: 0.1071982318858564\n",
            "Iteration 23050 - Loss: 0.10718728566971752\n",
            "Iteration 23060 - Loss: 0.10717634662308194\n",
            "Iteration 23070 - Loss: 0.10716541473820691\n",
            "Iteration 23080 - Loss: 0.10715449000736152\n",
            "Iteration 23090 - Loss: 0.10714357242282621\n",
            "Iteration 23100 - Loss: 0.10713266197689338\n",
            "Iteration 23110 - Loss: 0.10712175866186664\n",
            "Iteration 23120 - Loss: 0.10711086247006125\n",
            "Iteration 23130 - Loss: 0.10709997339380405\n",
            "Iteration 23140 - Loss: 0.10708909142543313\n",
            "Iteration 23150 - Loss: 0.10707821655729845\n",
            "Iteration 23160 - Loss: 0.10706734878176093\n",
            "Iteration 23170 - Loss: 0.10705648809119325\n",
            "Iteration 23180 - Loss: 0.10704563447797938\n",
            "Iteration 23190 - Loss: 0.1070347879345147\n",
            "Iteration 23200 - Loss: 0.10702394845320581\n",
            "Iteration 23210 - Loss: 0.10701311602647089\n",
            "Iteration 23220 - Loss: 0.1070022906467392\n",
            "Iteration 23230 - Loss: 0.10699147230645138\n",
            "Iteration 23240 - Loss: 0.10698066099805927\n",
            "Iteration 23250 - Loss: 0.10696985671402624\n",
            "Iteration 23260 - Loss: 0.10695905944682642\n",
            "Iteration 23270 - Loss: 0.10694826918894554\n",
            "Iteration 23280 - Loss: 0.10693748593288026\n",
            "Iteration 23290 - Loss: 0.10692670967113856\n",
            "Iteration 23300 - Loss: 0.10691594039623945\n",
            "Iteration 23310 - Loss: 0.1069051781007131\n",
            "Iteration 23320 - Loss: 0.10689442277710083\n",
            "Iteration 23330 - Loss: 0.10688367441795503\n",
            "Iteration 23340 - Loss: 0.10687293301583899\n",
            "Iteration 23350 - Loss: 0.10686219856332718\n",
            "Iteration 23360 - Loss: 0.10685147105300502\n",
            "Iteration 23370 - Loss: 0.10684075047746903\n",
            "Iteration 23380 - Loss: 0.10683003682932656\n",
            "Iteration 23390 - Loss: 0.10681933010119597\n",
            "Iteration 23400 - Loss: 0.1068086302857065\n",
            "Iteration 23410 - Loss: 0.10679793737549849\n",
            "Iteration 23420 - Loss: 0.10678725136322284\n",
            "Iteration 23430 - Loss: 0.10677657224154166\n",
            "Iteration 23440 - Loss: 0.1067659000031276\n",
            "Iteration 23450 - Loss: 0.10675523464066436\n",
            "Iteration 23460 - Loss: 0.10674457614684628\n",
            "Iteration 23470 - Loss: 0.1067339245143787\n",
            "Iteration 23480 - Loss: 0.10672327973597745\n",
            "Iteration 23490 - Loss: 0.10671264180436936\n",
            "Iteration 23500 - Loss: 0.10670201071229181\n",
            "Iteration 23510 - Loss: 0.10669138645249296\n",
            "Iteration 23520 - Loss: 0.1066807690177316\n",
            "Iteration 23530 - Loss: 0.10667015840077734\n",
            "Iteration 23540 - Loss: 0.10665955459441015\n",
            "Iteration 23550 - Loss: 0.10664895759142082\n",
            "Iteration 23560 - Loss: 0.10663836738461074\n",
            "Iteration 23570 - Loss: 0.10662778396679189\n",
            "Iteration 23580 - Loss: 0.10661720733078665\n",
            "Iteration 23590 - Loss: 0.10660663746942815\n",
            "Iteration 23600 - Loss: 0.10659607437555987\n",
            "Iteration 23610 - Loss: 0.10658551804203598\n",
            "Iteration 23620 - Loss: 0.10657496846172099\n",
            "Iteration 23630 - Loss: 0.10656442562748988\n",
            "Iteration 23640 - Loss: 0.10655388953222812\n",
            "Iteration 23650 - Loss: 0.10654336016883169\n",
            "Iteration 23660 - Loss: 0.10653283753020663\n",
            "Iteration 23670 - Loss: 0.10652232160926992\n",
            "Iteration 23680 - Loss: 0.10651181239894836\n",
            "Iteration 23690 - Loss: 0.10650130989217943\n",
            "Iteration 23700 - Loss: 0.10649081408191079\n",
            "Iteration 23710 - Loss: 0.10648032496110046\n",
            "Iteration 23720 - Loss: 0.1064698425227168\n",
            "Iteration 23730 - Loss: 0.10645936675973834\n",
            "Iteration 23740 - Loss: 0.10644889766515395\n",
            "Iteration 23750 - Loss: 0.10643843523196254\n",
            "Iteration 23760 - Loss: 0.10642797945317353\n",
            "Iteration 23770 - Loss: 0.10641753032180631\n",
            "Iteration 23780 - Loss: 0.10640708783089045\n",
            "Iteration 23790 - Loss: 0.10639665197346587\n",
            "Iteration 23800 - Loss: 0.10638622274258236\n",
            "Iteration 23810 - Loss: 0.10637580013130002\n",
            "Iteration 23820 - Loss: 0.1063653841326889\n",
            "Iteration 23830 - Loss: 0.10635497473982927\n",
            "Iteration 23840 - Loss: 0.10634457194581137\n",
            "Iteration 23850 - Loss: 0.10633417574373556\n",
            "Iteration 23860 - Loss: 0.10632378612671203\n",
            "Iteration 23870 - Loss: 0.10631340308786132\n",
            "Iteration 23880 - Loss: 0.10630302662031357\n",
            "Iteration 23890 - Loss: 0.10629265671720914\n",
            "Iteration 23900 - Loss: 0.10628229337169824\n",
            "Iteration 23910 - Loss: 0.10627193657694105\n",
            "Iteration 23920 - Loss: 0.1062615863261076\n",
            "Iteration 23930 - Loss: 0.10625124261237792\n",
            "Iteration 23940 - Loss: 0.10624090542894174\n",
            "Iteration 23950 - Loss: 0.10623057476899886\n",
            "Iteration 23960 - Loss: 0.10622025062575877\n",
            "Iteration 23970 - Loss: 0.10620993299244075\n",
            "Iteration 23980 - Loss: 0.10619962186227405\n",
            "Iteration 23990 - Loss: 0.10618931722849755\n",
            "Iteration 24000 - Loss: 0.10617901908435995\n",
            "Iteration 24010 - Loss: 0.1061687274231197\n",
            "Iteration 24020 - Loss: 0.10615844223804499\n",
            "Iteration 24030 - Loss: 0.10614816352241359\n",
            "Iteration 24040 - Loss: 0.10613789126951322\n",
            "Iteration 24050 - Loss: 0.10612762547264101\n",
            "Iteration 24060 - Loss: 0.10611736612510393\n",
            "Iteration 24070 - Loss: 0.10610711322021854\n",
            "Iteration 24080 - Loss: 0.10609686675131101\n",
            "Iteration 24090 - Loss: 0.106086626711717\n",
            "Iteration 24100 - Loss: 0.10607639309478194\n",
            "Iteration 24110 - Loss: 0.10606616589386078\n",
            "Iteration 24120 - Loss: 0.10605594510231793\n",
            "Iteration 24130 - Loss: 0.10604573071352742\n",
            "Iteration 24140 - Loss: 0.10603552272087267\n",
            "Iteration 24150 - Loss: 0.10602532111774694\n",
            "Iteration 24160 - Loss: 0.10601512589755253\n",
            "Iteration 24170 - Loss: 0.10600493705370144\n",
            "Iteration 24180 - Loss: 0.10599475457961502\n",
            "Iteration 24190 - Loss: 0.10598457846872417\n",
            "Iteration 24200 - Loss: 0.10597440871446918\n",
            "Iteration 24210 - Loss: 0.10596424531029958\n",
            "Iteration 24220 - Loss: 0.10595408824967437\n",
            "Iteration 24230 - Loss: 0.10594393752606207\n",
            "Iteration 24240 - Loss: 0.10593379313294021\n",
            "Iteration 24250 - Loss: 0.10592365506379593\n",
            "Iteration 24260 - Loss: 0.1059135233121256\n",
            "Iteration 24270 - Loss: 0.10590339787143474\n",
            "Iteration 24280 - Loss: 0.1058932787352383\n",
            "Iteration 24290 - Loss: 0.1058831658970605\n",
            "Iteration 24300 - Loss: 0.10587305935043473\n",
            "Iteration 24310 - Loss: 0.10586295908890357\n",
            "Iteration 24320 - Loss: 0.10585286510601885\n",
            "Iteration 24330 - Loss: 0.10584277739534166\n",
            "Iteration 24340 - Loss: 0.10583269595044206\n",
            "Iteration 24350 - Loss: 0.10582262076489957\n",
            "Iteration 24360 - Loss: 0.10581255183230255\n",
            "Iteration 24370 - Loss: 0.10580248914624858\n",
            "Iteration 24380 - Loss: 0.10579243270034452\n",
            "Iteration 24390 - Loss: 0.10578238248820603\n",
            "Iteration 24400 - Loss: 0.10577233850345798\n",
            "Iteration 24410 - Loss: 0.10576230073973444\n",
            "Iteration 24420 - Loss: 0.10575226919067822\n",
            "Iteration 24430 - Loss: 0.1057422438499415\n",
            "Iteration 24440 - Loss: 0.10573222471118507\n",
            "Iteration 24450 - Loss: 0.10572221176807897\n",
            "Iteration 24460 - Loss: 0.10571220501430224\n",
            "Stopping training as loss change is smaller than the stopping threshold.\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.878\n",
            "Confusion Matrix:\n",
            "[[ 952    0    2    2    0    2   13    1    8    0]\n",
            " [   0 1094    4    3    1    1    4    0   28    0]\n",
            " [  12   18  854   27   19    0   25   20   48    9]\n",
            " [   6    1   21  893    1   30    8   16   23   11]\n",
            " [   1    7    5    0  870    1   15    1   14   68]\n",
            " [  21   11    6   71   27  650   25   12   52   17]\n",
            " [  17    3    6    2   11   18  892    1    8    0]\n",
            " [   2   30   30    3   12    0    4  906    6   35]\n",
            " [   8   13   13   37   11   23   14   13  828   14]\n",
            " [  16   11   13   12   51   11    1   36   12  846]]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Read, Normalize and Split Data\n",
        "#########################################################################\n",
        "\n",
        "def load_and_process_data(file_name):\n",
        "    # Read data from CSV file and preprocess it\n",
        "    with open(file_name) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        X = []  # Initialize a list to store input data (pixel values)\n",
        "        y = []  # Initialize a list to store labels\n",
        "        for row in csv_reader:\n",
        "            y.append(int(row[0]))\n",
        "            temp = [float(i) / 255.0 for i in row[1:]]  # Normalize pixel values\n",
        "            X.append(temp)\n",
        "\n",
        "    # Convert data to NumPy arrays for further processing\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    # Normalize the pixel values using Min-Max scaling\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "\n",
        "    # Add a bias term (1) to the input data\n",
        "    X = np.append(X, np.ones((X.shape[0], 1), np.float64), axis=1)\n",
        "\n",
        "    # Convert labels (y) to one-hot encoding\n",
        "    num_classes = len(np.unique(y))\n",
        "    y = np.eye(num_classes)[y]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Read Training Data\n",
        "X_train, y_train = load_and_process_data('mnist_train.csv')\n",
        "\n",
        "# Read Test Data\n",
        "X_test, y_test = load_and_process_data('mnist_test.csv')\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Logistic regression\n",
        "#########################################################################\n",
        "\n",
        "def compute_loss(y_true, y_pred, weights, lambda_reg):\n",
        "    # Compute the binary cross-entropy loss with L2 regularization\n",
        "    epsilon = 1e-9\n",
        "    y1 = y_true * np.log(y_pred + epsilon)\n",
        "    y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
        "    regularization_term = (lambda_reg / (2 * len(y_true))) * np.sum(weights ** 2)\n",
        "    return -np.mean(y1 + y2) + regularization_term\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Sigmoid activation function\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def feed_forward(X, weights, bias):\n",
        "    # Perform feedforward operation\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = sigmoid(z)\n",
        "    return A\n",
        "\n",
        "\n",
        "def fit(X, y, lr, lambda_reg, stopping_threshold):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    weights = np.zeros((n_features, n_classes))\n",
        "    bias = np.zeros(n_classes)\n",
        "    losses = []  # To store loss at each iteration\n",
        "\n",
        "    previous_loss = float('inf')  # Set to a large value initially\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        A = feed_forward(X, weights, bias)\n",
        "        loss = compute_loss(y, A, weights, lambda_reg)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if abs(loss - previous_loss) < stopping_threshold:\n",
        "            print(\"Stopping training as loss change is smaller than the stopping threshold.\")\n",
        "            break\n",
        "\n",
        "        dz = A - y\n",
        "        dw = (1 / n_samples) * (np.dot(X.T, dz) + lambda_reg * weights)\n",
        "        db = (1 / n_samples) * np.sum(dz, axis=0)\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        previous_loss = loss\n",
        "\n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
        "\n",
        "    return weights, bias, losses, iteration\n",
        "\n",
        "# Call the fit function with the stopping_threshold\n",
        "learning_rate = 0.001\n",
        "lambda_reg = 0.01\n",
        "stopping_threshold = 1e-6\n",
        "\n",
        "weights, bias, losses,n_iters = fit(X_train, y_train, learning_rate, lambda_reg, stopping_threshold)\n",
        "\n",
        "\n",
        "\n",
        "# Plot the loss over iterations\n",
        "plt.figure(1)\n",
        "plt.plot(range(n_iters), losses, '-g', label='Logistic Regression')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = feed_forward(X_test, weights, bias)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute and display the confusion matrix and test accuracy\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), predicted_classes)\n",
        "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm)) / np.sum(cm)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(np.array(cm))\n",
        "\n",
        "input('Close app?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwx0jFPIPcA_"
      },
      "source": [
        "With Z-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "mq3Yw_CeYJDC",
        "outputId": "5810ea6d-0fac-4c7c-ad6e-91737b5ccf16"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d7949a23090f>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Read Training Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Read Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d7949a23090f>\u001b[0m in \u001b[0;36mload_and_process_data\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Read data from CSV file and preprocess it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_reader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Skip the header row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_train.csv'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Read, Normalize and Split Data\n",
        "#########################################################################\n",
        "\n",
        "def load_and_process_data(file_name):\n",
        "    # Read data from CSV file and preprocess it\n",
        "    with open(file_name) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        X = []  # Initialize a list to store input data (pixel values)\n",
        "        y = []  # Initialize a list to store labels\n",
        "        for row in csv_reader:\n",
        "            y.append(int(row[0]))\n",
        "            temp = [float(i) / 255.0 for i in row[1:]]  # Normalize pixel values\n",
        "            X.append(temp)\n",
        "\n",
        "    # Convert data to NumPy arrays for further processing\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    # Normalize the pixel values using Min-Max scaling\n",
        "    '''\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Normalize the pixel values using Z-score (standardization)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "    # Add a bias term (1) to the input data\n",
        "    X = np.append(X, np.ones((X.shape[0], 1), np.float64), axis=1)\n",
        "\n",
        "    # Convert labels (y) to one-hot encoding\n",
        "    num_classes = len(np.unique(y))\n",
        "    y = np.eye(num_classes)[y]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Read Training Data\n",
        "X_train, y_train = load_and_process_data('mnist_train.csv')\n",
        "\n",
        "# Read Test Data\n",
        "X_test, y_test = load_and_process_data('mnist_test.csv')\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Logistic regression\n",
        "#########################################################################\n",
        "\n",
        "def compute_loss(y_true, y_pred, weights, lambda_reg):\n",
        "    # Compute the binary cross-entropy loss with L2 regularization\n",
        "    epsilon = 1e-9\n",
        "    y1 = y_true * np.log(y_pred + epsilon)\n",
        "    y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
        "    regularization_term = (lambda_reg / (2 * len(y_true))) * np.sum(weights ** 2)\n",
        "    return -np.mean(y1 + y2) + regularization_term\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Sigmoid activation function\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def feed_forward(X, weights, bias):\n",
        "    # Perform feedforward operation\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = sigmoid(z)\n",
        "    return A\n",
        "\n",
        "\n",
        "def fit(X, y, lr, lambda_reg, stopping_threshold):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    weights = np.zeros((n_features, n_classes))\n",
        "    bias = np.zeros(n_classes)\n",
        "    losses = []  # To store loss at each iteration\n",
        "\n",
        "    previous_loss = float('inf')  # Set to a large value initially\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        A = feed_forward(X, weights, bias)\n",
        "        loss = compute_loss(y, A, weights, lambda_reg)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if abs(loss - previous_loss) < stopping_threshold:\n",
        "            print(\"Stopping training as loss change is smaller than the stopping threshold.\")\n",
        "            break\n",
        "\n",
        "        dz = A - y\n",
        "        dw = (1 / n_samples) * (np.dot(X.T, dz) + lambda_reg * weights)\n",
        "        db = (1 / n_samples) * np.sum(dz, axis=0)\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        previous_loss = loss\n",
        "\n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
        "\n",
        "    return weights, bias, losses, iteration\n",
        "\n",
        "# Call the fit function with the stopping_threshold\n",
        "learning_rate = 0.001\n",
        "lambda_reg = 0.01\n",
        "stopping_threshold = 1e-6\n",
        "\n",
        "weights, bias, losses,n_iters = fit(X_train, y_train, learning_rate, lambda_reg, stopping_threshold)\n",
        "\n",
        "\n",
        "\n",
        "# Plot the loss over iterations\n",
        "plt.figure(1)\n",
        "plt.plot(range(n_iters), losses, '-g', label='Logistic Regression')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = feed_forward(X_test, weights, bias)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute and display the confusion matrix and test accuracy\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), predicted_classes)\n",
        "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm)) / np.sum(cm)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(np.array(cm))\n",
        "\n",
        "input('Close app?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xZ7-CjKPiu88",
        "outputId": "3e0174c0-5e92-4753-b2b1-ca6db375c3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 10 - Loss: 0.6831230848145937\n",
            "Iteration 20 - Loss: 0.6728723238742675\n",
            "Iteration 30 - Loss: 0.6634300499473744\n",
            "Iteration 40 - Loss: 0.6546849800825765\n",
            "Iteration 50 - Loss: 0.6465429835740003\n",
            "Iteration 60 - Loss: 0.6389248494511702\n",
            "Iteration 70 - Loss: 0.6317640060254175\n",
            "Iteration 80 - Loss: 0.6250044342868731\n",
            "Iteration 90 - Loss: 0.6185988634353757\n",
            "Iteration 100 - Loss: 0.6125072569833325\n",
            "Iteration 110 - Loss: 0.606695562900199\n",
            "Iteration 120 - Loss: 0.6011346898636043\n",
            "Iteration 130 - Loss: 0.5957996712699606\n",
            "Iteration 140 - Loss: 0.5906689826660096\n",
            "Iteration 150 - Loss: 0.5857239835430952\n",
            "Iteration 160 - Loss: 0.5809484596112057\n",
            "Iteration 170 - Loss: 0.5763282462229486\n",
            "Iteration 180 - Loss: 0.5718509174233753\n",
            "Iteration 190 - Loss: 0.5675055281987803\n",
            "Iteration 200 - Loss: 0.5632823999824195\n",
            "Iteration 210 - Loss: 0.5591729414541126\n",
            "Iteration 220 - Loss: 0.5551694982418968\n",
            "Iteration 230 - Loss: 0.5512652263804994\n",
            "Iteration 240 - Loss: 0.5474539853714439\n",
            "Iteration 250 - Loss: 0.5437302474774707\n",
            "Iteration 260 - Loss: 0.5400890205125558\n",
            "Iteration 270 - Loss: 0.536525781891922\n",
            "Iteration 280 - Loss: 0.5330364221104218\n",
            "Iteration 290 - Loss: 0.529617196143189\n",
            "Iteration 300 - Loss: 0.526264681525689\n",
            "Iteration 310 - Loss: 0.5229757420839029\n",
            "Iteration 320 - Loss: 0.5197474964593494\n",
            "Iteration 330 - Loss: 0.5165772907158181\n",
            "Iteration 340 - Loss: 0.5134626744312882\n",
            "Iteration 350 - Loss: 0.5104013797744391\n",
            "Iteration 360 - Loss: 0.5073913031443817\n",
            "Iteration 370 - Loss: 0.5044304890178362\n",
            "Iteration 380 - Loss: 0.5015171157025061\n",
            "Iteration 390 - Loss: 0.4986494827408278\n",
            "Iteration 400 - Loss: 0.49582599974625674\n",
            "Iteration 410 - Loss: 0.4930451764861088\n",
            "Iteration 420 - Loss: 0.49030561405172973\n",
            "Iteration 430 - Loss: 0.4876059969793576\n",
            "Iteration 440 - Loss: 0.48494508620411914\n",
            "Iteration 450 - Loss: 0.4823217127457755\n",
            "Iteration 460 - Loss: 0.47973477203857845\n",
            "Iteration 470 - Loss: 0.4771832188293059\n",
            "Iteration 480 - Loss: 0.4746660625775439\n",
            "Iteration 490 - Loss: 0.47218236330083535\n",
            "Iteration 500 - Loss: 0.46973122781466087\n",
            "Iteration 510 - Loss: 0.46731180632352426\n",
            "Iteration 520 - Loss: 0.46492328932485133\n",
            "Iteration 530 - Loss: 0.46256490479211\n",
            "Iteration 540 - Loss: 0.46023591560762006\n",
            "Iteration 550 - Loss: 0.45793561721903886\n",
            "Iteration 560 - Loss: 0.45566333549657645\n",
            "Iteration 570 - Loss: 0.4534184247706462\n",
            "Iteration 580 - Loss: 0.45120026603198854\n",
            "Iteration 590 - Loss: 0.4490082652783308\n",
            "Iteration 600 - Loss: 0.4468418519934141\n",
            "Iteration 610 - Loss: 0.4447004777457876\n",
            "Iteration 620 - Loss: 0.44258361489612724\n",
            "Iteration 630 - Loss: 0.44049075540304305\n",
            "Iteration 640 - Loss: 0.43842140971839894\n",
            "Iteration 650 - Loss: 0.43637510576410193\n",
            "Iteration 660 - Loss: 0.434351387983147\n",
            "Iteration 670 - Loss: 0.43234981645843806\n",
            "Iteration 680 - Loss: 0.4303699660935471\n",
            "Iteration 690 - Loss: 0.42841142585016184\n",
            "Iteration 700 - Loss: 0.4264737980374755\n",
            "Iteration 710 - Loss: 0.42455669764923376\n",
            "Iteration 720 - Loss: 0.42265975174456155\n",
            "Iteration 730 - Loss: 0.420782598869061\n",
            "Iteration 740 - Loss: 0.41892488851298376\n",
            "Iteration 750 - Loss: 0.41708628060359143\n",
            "Iteration 760 - Loss: 0.4152664450290677\n",
            "Iteration 770 - Loss: 0.41346506119158327\n",
            "Iteration 780 - Loss: 0.41168181758732825\n",
            "Iteration 790 - Loss: 0.4099164114115181\n",
            "Iteration 800 - Loss: 0.40816854818654547\n",
            "Iteration 810 - Loss: 0.40643794141161493\n",
            "Iteration 820 - Loss: 0.40472431223232674\n",
            "Iteration 830 - Loss: 0.403027389128812\n",
            "Iteration 840 - Loss: 0.4013469076211315\n",
            "Iteration 850 - Loss: 0.39968260999075644\n",
            "Iteration 860 - Loss: 0.3980342450170455\n",
            "Iteration 870 - Loss: 0.39640156772771173\n",
            "Iteration 880 - Loss: 0.39478433916236644\n",
            "Iteration 890 - Loss: 0.3931823261482759\n",
            "Iteration 900 - Loss: 0.3915953010875591\n",
            "Iteration 910 - Loss: 0.39002304175508495\n",
            "Iteration 920 - Loss: 0.3884653311064118\n",
            "Iteration 930 - Loss: 0.38692195709513255\n",
            "Iteration 940 - Loss: 0.38539271249906065\n",
            "Iteration 950 - Loss: 0.38387739475471205\n",
            "Iteration 960 - Loss: 0.3823758057995927\n",
            "Iteration 970 - Loss: 0.3808877519218254\n",
            "Iteration 980 - Loss: 0.3794130436166916\n",
            "Iteration 990 - Loss: 0.3779514954496827\n",
            "Iteration 1000 - Loss: 0.3765029259256916\n",
            "Iteration 1010 - Loss: 0.3750671573639989\n",
            "Iteration 1020 - Loss: 0.3736440157787271\n",
            "Iteration 1030 - Loss: 0.372233330764461\n",
            "Iteration 1040 - Loss: 0.3708349353867547\n",
            "Iteration 1050 - Loss: 0.36944866607725685\n",
            "Iteration 1060 - Loss: 0.36807436253321046\n",
            "Iteration 1070 - Loss: 0.3667118676210958\n",
            "Iteration 1080 - Loss: 0.36536102728419645\n",
            "Iteration 1090 - Loss: 0.3640216904538895\n",
            "Iteration 1100 - Loss: 0.3626937089644672\n",
            "Iteration 1110 - Loss: 0.361376937471311\n",
            "Iteration 1120 - Loss: 0.3600712333722495\n",
            "Iteration 1130 - Loss: 0.3587764567319457\n",
            "Iteration 1140 - Loss: 0.35749247020915875\n",
            "Iteration 1150 - Loss: 0.35621913898674756\n",
            "Iteration 1160 - Loss: 0.3549563307042776\n",
            "Iteration 1170 - Loss: 0.35370391539311247\n",
            "Iteration 1180 - Loss: 0.3524617654138703\n",
            "Iteration 1190 - Loss: 0.3512297553961348\n",
            "Iteration 1200 - Loss: 0.3500077621803181\n",
            "Iteration 1210 - Loss: 0.3487956647615748\n",
            "Iteration 1220 - Loss: 0.34759334423567684\n",
            "Iteration 1230 - Loss: 0.34640068374676025\n",
            "Iteration 1240 - Loss: 0.3452175684368597\n",
            "Iteration 1250 - Loss: 0.3440438853971532\n",
            "Iteration 1260 - Loss: 0.3428795236208423\n",
            "Iteration 1270 - Loss: 0.34172437395759875\n",
            "Iteration 1280 - Loss: 0.3405783290695068\n",
            "Iteration 1290 - Loss: 0.3394412833884409\n",
            "Iteration 1300 - Loss: 0.3383131330748205\n",
            "Iteration 1310 - Loss: 0.33719377597767786\n",
            "Iteration 1320 - Loss: 0.33608311159599297\n",
            "Iteration 1330 - Loss: 0.3349810410412375\n",
            "Iteration 1340 - Loss: 0.3338874670010836\n",
            "Iteration 1350 - Loss: 0.33280229370422715\n",
            "Iteration 1360 - Loss: 0.3317254268862839\n",
            "Iteration 1370 - Loss: 0.3306567737567148\n",
            "Iteration 1380 - Loss: 0.3295962429667419\n",
            "Iteration 1390 - Loss: 0.3285437445782141\n",
            "Iteration 1400 - Loss: 0.32749919003338934\n",
            "Iteration 1410 - Loss: 0.32646249212559625\n",
            "Iteration 1420 - Loss: 0.32543356497074316\n",
            "Iteration 1430 - Loss: 0.3244123239796415\n",
            "Iteration 1440 - Loss: 0.3233986858311178\n",
            "Iteration 1450 - Loss: 0.32239256844587766\n",
            "Iteration 1460 - Loss: 0.3213938909611034\n",
            "Iteration 1470 - Loss: 0.32040257370575026\n",
            "Iteration 1480 - Loss: 0.31941853817652316\n",
            "Iteration 1490 - Loss: 0.31844170701450614\n",
            "Iteration 1500 - Loss: 0.31747200398242326\n",
            "Iteration 1510 - Loss: 0.3165093539425056\n",
            "Iteration 1520 - Loss: 0.3155536828349498\n",
            "Iteration 1530 - Loss: 0.31460491765694026\n",
            "Iteration 1540 - Loss: 0.3136629864422214\n",
            "Iteration 1550 - Loss: 0.31272781824120044\n",
            "Iteration 1560 - Loss: 0.3117993431015597\n",
            "Iteration 1570 - Loss: 0.310877492049366\n",
            "Iteration 1580 - Loss: 0.30996219707065664\n",
            "Iteration 1590 - Loss: 0.3090533910934901\n",
            "Iteration 1600 - Loss: 0.3081510079704443\n",
            "Iteration 1610 - Loss: 0.3072549824615464\n",
            "Iteration 1620 - Loss: 0.3063652502176259\n",
            "Iteration 1630 - Loss: 0.30548174776407033\n",
            "Iteration 1640 - Loss: 0.30460441248497616\n",
            "Iteration 1650 - Loss: 0.3037331826076821\n",
            "Iteration 1660 - Loss: 0.3028679971876692\n",
            "Iteration 1670 - Loss: 0.30200879609382114\n",
            "Iteration 1680 - Loss: 0.301155519994031\n",
            "Iteration 1690 - Loss: 0.3003081103411436\n",
            "Iteration 1700 - Loss: 0.2994665093592265\n",
            "Iteration 1710 - Loss: 0.29863066003015526\n",
            "Iteration 1720 - Loss: 0.2978005060805075\n",
            "Iteration 1730 - Loss: 0.2969759919687555\n",
            "Iteration 1740 - Loss: 0.29615706287274607\n",
            "Iteration 1750 - Loss: 0.29534366467746465\n",
            "Iteration 1760 - Loss: 0.2945357439630705\n",
            "Iteration 1770 - Loss: 0.2937332479931963\n",
            "Iteration 1780 - Loss: 0.2929361247035082\n",
            "Iteration 1790 - Loss: 0.2921443226905138\n",
            "Iteration 1800 - Loss: 0.2913577912006137\n",
            "Iteration 1810 - Loss: 0.2905764801193905\n",
            "Iteration 1820 - Loss: 0.2898003399611282\n",
            "Iteration 1830 - Loss: 0.28902932185855346\n",
            "Iteration 1840 - Loss: 0.2882633775527953\n",
            "Iteration 1850 - Loss: 0.2875024593835579\n",
            "Iteration 1860 - Loss: 0.28674652027949665\n",
            "Iteration 1870 - Loss: 0.2859955137487976\n",
            "Iteration 1880 - Loss: 0.28524939386994885\n",
            "Iteration 1890 - Loss: 0.28450811528270503\n",
            "Iteration 1900 - Loss: 0.2837716331792338\n",
            "Iteration 1910 - Loss: 0.2830399032954454\n",
            "Iteration 1920 - Loss: 0.28231288190249565\n",
            "Iteration 1930 - Loss: 0.28159052579846006\n",
            "Iteration 1940 - Loss: 0.2808727923001748\n",
            "Iteration 1950 - Loss: 0.28015963923524145\n",
            "Iteration 1960 - Loss: 0.279451024934187\n",
            "Iteration 1970 - Loss: 0.2787469082227802\n",
            "Iteration 1980 - Loss: 0.27804724841449846\n",
            "Iteration 1990 - Loss: 0.2773520053031395\n",
            "Iteration 2000 - Loss: 0.2766611391555787\n",
            "Iteration 2010 - Loss: 0.275974610704663\n",
            "Iteration 2020 - Loss: 0.2752923811422436\n",
            "Iteration 2030 - Loss: 0.27461441211233956\n",
            "Iteration 2040 - Loss: 0.2739406657044321\n",
            "Iteration 2050 - Loss: 0.27327110444688507\n",
            "Iteration 2060 - Loss: 0.2726056913004886\n",
            "Iteration 2070 - Loss: 0.27194438965212353\n",
            "Iteration 2080 - Loss: 0.271287163308544\n",
            "Iteration 2090 - Loss: 0.27063397649027415\n",
            "Iteration 2100 - Loss: 0.269984793825618\n",
            "Iteration 2110 - Loss: 0.26933958034477795\n",
            "Iteration 2120 - Loss: 0.2686983014740815\n",
            "Iteration 2130 - Loss: 0.2680609230303119\n",
            "Iteration 2140 - Loss: 0.26742741121514135\n",
            "Iteration 2150 - Loss: 0.26679773260966466\n",
            "Iteration 2160 - Loss: 0.26617185416902944\n",
            "Iteration 2170 - Loss: 0.26554974321716357\n",
            "Iteration 2180 - Loss: 0.2649313674415938\n",
            "Iteration 2190 - Loss: 0.2643166948883581\n",
            "Iteration 2200 - Loss: 0.2637056939570056\n",
            "Iteration 2210 - Loss: 0.263098333395685\n",
            "Iteration 2220 - Loss: 0.2624945822963169\n",
            "Iteration 2230 - Loss: 0.2618944100898519\n",
            "Iteration 2240 - Loss: 0.2612977865416088\n",
            "Iteration 2250 - Loss: 0.2607046817466933\n",
            "Iteration 2260 - Loss: 0.26011506612549457\n",
            "Iteration 2270 - Loss: 0.25952891041926013\n",
            "Iteration 2280 - Loss: 0.25894618568574357\n",
            "Iteration 2290 - Loss: 0.25836686329492686\n",
            "Iteration 2300 - Loss: 0.25779091492481476\n",
            "Iteration 2310 - Loss: 0.257218312557299\n",
            "Iteration 2320 - Loss: 0.25664902847409127\n",
            "Iteration 2330 - Loss: 0.2560830352527248\n",
            "Iteration 2340 - Loss: 0.25552030576262097\n",
            "Iteration 2350 - Loss: 0.25496081316122127\n",
            "Iteration 2360 - Loss: 0.25440453089018195\n",
            "Iteration 2370 - Loss: 0.253851432671632\n",
            "Iteration 2380 - Loss: 0.25330149250449063\n",
            "Iteration 2390 - Loss: 0.2527546846608464\n",
            "Iteration 2400 - Loss: 0.2522109836823923\n",
            "Iteration 2410 - Loss: 0.2516703643769202\n",
            "Iteration 2420 - Loss: 0.2511328018148707\n",
            "Iteration 2430 - Loss: 0.2505982713259379\n",
            "Iteration 2440 - Loss: 0.25006674849572835\n",
            "Iteration 2450 - Loss: 0.24953820916247269\n",
            "Iteration 2460 - Loss: 0.2490126294137906\n",
            "Iteration 2470 - Loss: 0.24848998558350358\n",
            "Iteration 2480 - Loss: 0.2479702542485012\n",
            "Iteration 2490 - Loss: 0.24745341222565345\n",
            "Iteration 2500 - Loss: 0.24693943656877343\n",
            "Iteration 2510 - Loss: 0.24642830456562492\n",
            "Iteration 2520 - Loss: 0.24591999373497767\n",
            "Iteration 2530 - Loss: 0.2454144818237082\n",
            "Iteration 2540 - Loss: 0.2449117468039427\n",
            "Iteration 2550 - Loss: 0.24441176687024777\n",
            "Iteration 2560 - Loss: 0.24391452043685916\n",
            "Iteration 2570 - Loss: 0.2434199861349568\n",
            "Iteration 2580 - Loss: 0.24292814280997818\n",
            "Iteration 2590 - Loss: 0.2424389695189734\n",
            "Iteration 2600 - Loss: 0.2419524455280003\n",
            "Iteration 2610 - Loss: 0.24146855030955805\n",
            "Iteration 2620 - Loss: 0.24098726354005798\n",
            "Iteration 2630 - Loss: 0.24050856509733534\n",
            "Iteration 2640 - Loss: 0.2400324350581938\n",
            "Iteration 2650 - Loss: 0.23955885369598973\n",
            "Iteration 2660 - Loss: 0.23908780147824998\n",
            "Iteration 2670 - Loss: 0.2386192590643253\n",
            "Iteration 2680 - Loss: 0.23815320730307835\n",
            "Iteration 2690 - Loss: 0.23768962723060524\n",
            "Iteration 2700 - Loss: 0.2372285000679902\n",
            "Iteration 2710 - Loss: 0.23676980721909296\n",
            "Iteration 2720 - Loss: 0.23631353026836757\n",
            "Iteration 2730 - Loss: 0.23585965097871406\n",
            "Iteration 2740 - Loss: 0.2354081512893592\n",
            "Iteration 2750 - Loss: 0.23495901331376867\n",
            "Iteration 2760 - Loss: 0.23451221933758942\n",
            "Iteration 2770 - Loss: 0.23406775181661998\n",
            "Iteration 2780 - Loss: 0.23362559337481123\n",
            "Iteration 2790 - Loss: 0.23318572680229419\n",
            "Iteration 2800 - Loss: 0.23274813505343647\n",
            "Iteration 2810 - Loss: 0.23231280124492507\n",
            "Iteration 2820 - Loss: 0.23187970865387808\n",
            "Iteration 2830 - Loss: 0.23144884071598004\n",
            "Iteration 2840 - Loss: 0.23102018102364621\n",
            "Iteration 2850 - Loss: 0.2305937133242101\n",
            "Iteration 2860 - Loss: 0.23016942151813705\n",
            "Iteration 2870 - Loss: 0.22974728965726343\n",
            "Iteration 2880 - Loss: 0.22932730194305853\n",
            "Iteration 2890 - Loss: 0.22890944272491093\n",
            "Iteration 2900 - Loss: 0.22849369649843967\n",
            "Iteration 2910 - Loss: 0.2280800479038261\n",
            "Iteration 2920 - Loss: 0.22766848172417106\n",
            "Iteration 2930 - Loss: 0.2272589828838727\n",
            "Iteration 2940 - Loss: 0.22685153644702657\n",
            "Iteration 2950 - Loss: 0.2264461276158483\n",
            "Iteration 2960 - Loss: 0.22604274172911687\n",
            "Iteration 2970 - Loss: 0.2256413642606386\n",
            "Iteration 2980 - Loss: 0.22524198081773256\n",
            "Iteration 2990 - Loss: 0.22484457713973538\n",
            "Iteration 3000 - Loss: 0.22444913909652733\n",
            "Iteration 3010 - Loss: 0.22405565268707667\n",
            "Iteration 3020 - Loss: 0.22366410403800388\n",
            "Iteration 3030 - Loss: 0.22327447940216547\n",
            "Iteration 3040 - Loss: 0.2228867651572556\n",
            "Iteration 3050 - Loss: 0.22250094780442736\n",
            "Iteration 3060 - Loss: 0.22211701396693073\n",
            "Iteration 3070 - Loss: 0.22173495038876936\n",
            "Iteration 3080 - Loss: 0.22135474393337523\n",
            "Iteration 3090 - Loss: 0.2209763815822999\n",
            "Iteration 3100 - Loss: 0.220599850433923\n",
            "Iteration 3110 - Loss: 0.22022513770217758\n",
            "Iteration 3120 - Loss: 0.21985223071529258\n",
            "Iteration 3130 - Loss: 0.21948111691455022\n",
            "Iteration 3140 - Loss: 0.21911178385306132\n",
            "Iteration 3150 - Loss: 0.21874421919455425\n",
            "Iteration 3160 - Loss: 0.21837841071218156\n",
            "Iteration 3170 - Loss: 0.2180143462873401\n",
            "Iteration 3180 - Loss: 0.21765201390850744\n",
            "Iteration 3190 - Loss: 0.21729140167009262\n",
            "Iteration 3200 - Loss: 0.21693249777130175\n",
            "Iteration 3210 - Loss: 0.216575290515018\n",
            "Iteration 3220 - Loss: 0.21621976830669526\n",
            "Iteration 3230 - Loss: 0.2158659196532669\n",
            "Iteration 3240 - Loss: 0.21551373316206698\n",
            "Iteration 3250 - Loss: 0.21516319753976654\n",
            "Iteration 3260 - Loss: 0.2148143015913215\n",
            "Iteration 3270 - Loss: 0.21446703421893523\n",
            "Iteration 3280 - Loss: 0.21412138442103346\n",
            "Iteration 3290 - Loss: 0.2137773412912519\n",
            "Iteration 3300 - Loss: 0.21343489401743668\n",
            "Iteration 3310 - Loss: 0.21309403188065743\n",
            "Iteration 3320 - Loss: 0.21275474425423171\n",
            "Iteration 3330 - Loss: 0.21241702060276274\n",
            "Iteration 3340 - Loss: 0.21208085048118808\n",
            "Iteration 3350 - Loss: 0.21174622353384004\n",
            "Iteration 3360 - Loss: 0.2114131294935186\n",
            "Iteration 3370 - Loss: 0.21108155818057422\n",
            "Iteration 3380 - Loss: 0.21075149950200375\n",
            "Iteration 3390 - Loss: 0.21042294345055504\n",
            "Iteration 3400 - Loss: 0.21009588010384456\n",
            "Iteration 3410 - Loss: 0.20977029962348473\n",
            "Iteration 3420 - Loss: 0.20944619225422226\n",
            "Iteration 3430 - Loss: 0.20912354832308583\n",
            "Iteration 3440 - Loss: 0.20880235823854618\n",
            "Iteration 3450 - Loss: 0.20848261248968428\n",
            "Iteration 3460 - Loss: 0.2081643016453702\n",
            "Iteration 3470 - Loss: 0.20784741635345272\n",
            "Iteration 3480 - Loss: 0.20753194733995717\n",
            "Iteration 3490 - Loss: 0.2072178854082938\n",
            "Iteration 3500 - Loss: 0.2069052214384758\n",
            "Iteration 3510 - Loss: 0.2065939463863456\n",
            "Iteration 3520 - Loss: 0.20628405128281177\n",
            "Iteration 3530 - Loss: 0.20597552723309295\n",
            "Iteration 3540 - Loss: 0.20566836541597353\n",
            "Iteration 3550 - Loss: 0.20536255708306558\n",
            "Iteration 3560 - Loss: 0.20505809355808072\n",
            "Iteration 3570 - Loss: 0.20475496623611059\n",
            "Iteration 3580 - Loss: 0.2044531665829154\n",
            "Iteration 3590 - Loss: 0.20415268613422055\n",
            "Iteration 3600 - Loss: 0.20385351649502287\n",
            "Iteration 3610 - Loss: 0.20355564933890277\n",
            "Iteration 3620 - Loss: 0.20325907640734653\n",
            "Iteration 3630 - Loss: 0.20296378950907543\n",
            "Iteration 3640 - Loss: 0.2026697805193825\n",
            "Iteration 3650 - Loss: 0.20237704137947757\n",
            "Iteration 3660 - Loss: 0.20208556409583928\n",
            "Iteration 3670 - Loss: 0.20179534073957564\n",
            "Iteration 3680 - Loss: 0.20150636344579032\n",
            "Iteration 3690 - Loss: 0.20121862441295757\n",
            "Iteration 3700 - Loss: 0.200932115902304\n",
            "Iteration 3710 - Loss: 0.20064683023719768\n",
            "Iteration 3720 - Loss: 0.2003627598025427\n",
            "Iteration 3730 - Loss: 0.20007989704418302\n",
            "Iteration 3740 - Loss: 0.19979823446831155\n",
            "Iteration 3750 - Loss: 0.1995177646408861\n",
            "Iteration 3760 - Loss: 0.19923848018705284\n",
            "Iteration 3770 - Loss: 0.19896037379057488\n",
            "Iteration 3780 - Loss: 0.19868343819326842\n",
            "Iteration 3790 - Loss: 0.1984076661944456\n",
            "Iteration 3800 - Loss: 0.19813305065036205\n",
            "Iteration 3810 - Loss: 0.19785958447367194\n",
            "Iteration 3820 - Loss: 0.19758726063288917\n",
            "Iteration 3830 - Loss: 0.19731607215185382\n",
            "Iteration 3840 - Loss: 0.1970460121092051\n",
            "Iteration 3850 - Loss: 0.19677707363786026\n",
            "Iteration 3860 - Loss: 0.19650924992449914\n",
            "Iteration 3870 - Loss: 0.1962425342090542\n",
            "Iteration 3880 - Loss: 0.19597691978420698\n",
            "Iteration 3890 - Loss: 0.19571239999488949\n",
            "Iteration 3900 - Loss: 0.19544896823779073\n",
            "Iteration 3910 - Loss: 0.1951866179608704\n",
            "Iteration 3920 - Loss: 0.19492534266287578\n",
            "Iteration 3930 - Loss: 0.19466513589286566\n",
            "Iteration 3940 - Loss: 0.19440599124973837\n",
            "Iteration 3950 - Loss: 0.1941479023817658\n",
            "Iteration 3960 - Loss: 0.19389086298613223\n",
            "Iteration 3970 - Loss: 0.19363486680847788\n",
            "Iteration 3980 - Loss: 0.1933799076424477\n",
            "Iteration 3990 - Loss: 0.19312597932924566\n",
            "Iteration 4000 - Loss: 0.19287307575719287\n",
            "Iteration 4010 - Loss: 0.19262119086129123\n",
            "Iteration 4020 - Loss: 0.19237031862279141\n",
            "Iteration 4030 - Loss: 0.19212045306876596\n",
            "Iteration 4040 - Loss: 0.19187158827168652\n",
            "Iteration 4050 - Loss: 0.19162371834900602\n",
            "Iteration 4060 - Loss: 0.19137683746274498\n",
            "Iteration 4070 - Loss: 0.19113093981908288\n",
            "Iteration 4080 - Loss: 0.19088601966795346\n",
            "Iteration 4090 - Loss: 0.1906420713026439\n",
            "Iteration 4100 - Loss: 0.19039908905939978\n",
            "Iteration 4110 - Loss: 0.19015706731703222\n",
            "Iteration 4120 - Loss: 0.18991600049653148\n",
            "Iteration 4130 - Loss: 0.18967588306068306\n",
            "Iteration 4140 - Loss: 0.18943670951368774\n",
            "Iteration 4150 - Loss: 0.18919847440078716\n",
            "Iteration 4160 - Loss: 0.18896117230789194\n",
            "Iteration 4170 - Loss: 0.18872479786121482\n",
            "Iteration 4180 - Loss: 0.1884893457269067\n",
            "Iteration 4190 - Loss: 0.18825481061069668\n",
            "Iteration 4200 - Loss: 0.18802118725753733\n",
            "Iteration 4210 - Loss: 0.18778847045125088\n",
            "Iteration 4220 - Loss: 0.18755665501418287\n",
            "Iteration 4230 - Loss: 0.18732573580685513\n",
            "Iteration 4240 - Loss: 0.1870957077276264\n",
            "Iteration 4250 - Loss: 0.18686656571235388\n",
            "Iteration 4260 - Loss: 0.1866383047340592\n",
            "Iteration 4270 - Loss: 0.1864109198025983\n",
            "Iteration 4280 - Loss: 0.18618440596433364\n",
            "Iteration 4290 - Loss: 0.185958758301811\n",
            "Iteration 4300 - Loss: 0.18573397193343882\n",
            "Iteration 4310 - Loss: 0.18551004201317114\n",
            "Iteration 4320 - Loss: 0.1852869637301939\n",
            "Iteration 4330 - Loss: 0.18506473230861453\n",
            "Iteration 4340 - Loss: 0.18484334300715444\n",
            "Iteration 4350 - Loss: 0.18462279111884472\n",
            "Iteration 4360 - Loss: 0.1844030719707255\n",
            "Iteration 4370 - Loss: 0.1841841809235476\n",
            "Iteration 4380 - Loss: 0.1839661133714777\n",
            "Iteration 4390 - Loss: 0.1837488647418071\n",
            "Iteration 4400 - Loss: 0.18353243049466159\n",
            "Iteration 4410 - Loss: 0.1833168061227167\n",
            "Iteration 4420 - Loss: 0.18310198715091394\n",
            "Iteration 4430 - Loss: 0.18288796913618105\n",
            "Iteration 4440 - Loss: 0.18267474766715444\n",
            "Iteration 4450 - Loss: 0.1824623183639047\n",
            "Iteration 4460 - Loss: 0.18225067687766522\n",
            "Iteration 4470 - Loss: 0.18203981889056278\n",
            "Iteration 4480 - Loss: 0.18182974011535152\n",
            "Iteration 4490 - Loss: 0.1816204362951497\n",
            "Iteration 4500 - Loss: 0.18141190320317782\n",
            "Iteration 4510 - Loss: 0.18120413664250168\n",
            "Iteration 4520 - Loss: 0.18099713244577537\n",
            "Iteration 4530 - Loss: 0.18079088647498923\n",
            "Iteration 4540 - Loss: 0.18058539462121842\n",
            "Iteration 4550 - Loss: 0.18038065280437532\n",
            "Iteration 4560 - Loss: 0.180176656972964\n",
            "Iteration 4570 - Loss: 0.1799734031038365\n",
            "Iteration 4580 - Loss: 0.17977088720195242\n",
            "Iteration 4590 - Loss: 0.17956910530014078\n",
            "Iteration 4600 - Loss: 0.17936805345886364\n",
            "Iteration 4610 - Loss: 0.17916772776598286\n",
            "Iteration 4620 - Loss: 0.1789681243365279\n",
            "Iteration 4630 - Loss: 0.17876923931246827\n",
            "Iteration 4640 - Loss: 0.17857106886248503\n",
            "Iteration 4650 - Loss: 0.17837360918174755\n",
            "Iteration 4660 - Loss: 0.17817685649169013\n",
            "Iteration 4670 - Loss: 0.17798080703979302\n",
            "Iteration 4680 - Loss: 0.1777854570993631\n",
            "Iteration 4690 - Loss: 0.17759080296931917\n",
            "Iteration 4700 - Loss: 0.17739684097397704\n",
            "Iteration 4710 - Loss: 0.17720356746283897\n",
            "Iteration 4720 - Loss: 0.17701097881038333\n",
            "Iteration 4730 - Loss: 0.17681907141585732\n",
            "Iteration 4740 - Loss: 0.17662784170307114\n",
            "Iteration 4750 - Loss: 0.17643728612019502\n",
            "Iteration 4760 - Loss: 0.17624740113955678\n",
            "Iteration 4770 - Loss: 0.17605818325744307\n",
            "Iteration 4780 - Loss: 0.17586962899390093\n",
            "Iteration 4790 - Loss: 0.1756817348925427\n",
            "Iteration 4800 - Loss: 0.17549449752035146\n",
            "Iteration 4810 - Loss: 0.1753079134674893\n",
            "Iteration 4820 - Loss: 0.1751219793471073\n",
            "Iteration 4830 - Loss: 0.17493669179515672\n",
            "Iteration 4840 - Loss: 0.1747520474702029\n",
            "Iteration 4850 - Loss: 0.17456804305324014\n",
            "Iteration 4860 - Loss: 0.17438467524750892\n",
            "Iteration 4870 - Loss: 0.17420194077831444\n",
            "Iteration 4880 - Loss: 0.17401983639284743\n",
            "Iteration 4890 - Loss: 0.17383835886000598\n",
            "Iteration 4900 - Loss: 0.1736575049702194\n",
            "Iteration 4910 - Loss: 0.173477271535274\n",
            "Iteration 4920 - Loss: 0.17329765538814032\n",
            "Iteration 4930 - Loss: 0.17311865338280158\n",
            "Iteration 4940 - Loss: 0.17294026239408442\n",
            "Iteration 4950 - Loss: 0.17276247931749122\n",
            "Iteration 4960 - Loss: 0.17258530106903294\n",
            "Iteration 4970 - Loss: 0.17240872458506523\n",
            "Iteration 4980 - Loss: 0.17223274682212442\n",
            "Iteration 4990 - Loss: 0.17205736475676636\n",
            "Iteration 5000 - Loss: 0.171882575385406\n",
            "Iteration 5010 - Loss: 0.1717083757241586\n",
            "Iteration 5020 - Loss: 0.17153476280868268\n",
            "Iteration 5030 - Loss: 0.17136173369402422\n",
            "Iteration 5040 - Loss: 0.17118928545446233\n",
            "Iteration 5050 - Loss: 0.1710174151833565\n",
            "Iteration 5060 - Loss: 0.17084611999299518\n",
            "Iteration 5070 - Loss: 0.1706753970144453\n",
            "Iteration 5080 - Loss: 0.17050524339740442\n",
            "Iteration 5090 - Loss: 0.17033565631005312\n",
            "Iteration 5100 - Loss: 0.17016663293890855\n",
            "Iteration 5110 - Loss: 0.1699981704886809\n",
            "Iteration 5120 - Loss: 0.16983026618212954\n",
            "Iteration 5130 - Loss: 0.16966291725992116\n",
            "Iteration 5140 - Loss: 0.16949612098048933\n",
            "Iteration 5150 - Loss: 0.16932987461989532\n",
            "Iteration 5160 - Loss: 0.16916417547168988\n",
            "Iteration 5170 - Loss: 0.16899902084677657\n",
            "Iteration 5180 - Loss: 0.16883440807327604\n",
            "Iteration 5190 - Loss: 0.1686703344963924\n",
            "Iteration 5200 - Loss: 0.16850679747827949\n",
            "Iteration 5210 - Loss: 0.16834379439790934\n",
            "Iteration 5220 - Loss: 0.16818132265094157\n",
            "Iteration 5230 - Loss: 0.16801937964959435\n",
            "Iteration 5240 - Loss: 0.16785796282251503\n",
            "Iteration 5250 - Loss: 0.16769706961465441\n",
            "Iteration 5260 - Loss: 0.1675366974871399\n",
            "Iteration 5270 - Loss: 0.16737684391715088\n",
            "Iteration 5280 - Loss: 0.16721750639779526\n",
            "Iteration 5290 - Loss: 0.16705868243798716\n",
            "Iteration 5300 - Loss: 0.1669003695623242\n",
            "Iteration 5310 - Loss: 0.16674256531096904\n",
            "Iteration 5320 - Loss: 0.1665852672395283\n",
            "Iteration 5330 - Loss: 0.16642847291893564\n",
            "Iteration 5340 - Loss: 0.1662721799353341\n",
            "Iteration 5350 - Loss: 0.16611638588995986\n",
            "Iteration 5360 - Loss: 0.1659610883990276\n",
            "Iteration 5370 - Loss: 0.1658062850936159\n",
            "Iteration 5380 - Loss: 0.16565197361955483\n",
            "Iteration 5390 - Loss: 0.1654981516373137\n",
            "Iteration 5400 - Loss: 0.16534481682188998\n",
            "Iteration 5410 - Loss: 0.16519196686269913\n",
            "Iteration 5420 - Loss: 0.16503959946346602\n",
            "Iteration 5430 - Loss: 0.1648877123421165\n",
            "Iteration 5440 - Loss: 0.16473630323067026\n",
            "Iteration 5450 - Loss: 0.164585369875135\n",
            "Iteration 5460 - Loss: 0.16443491003540076\n",
            "Iteration 5470 - Loss: 0.16428492148513577\n",
            "Iteration 5480 - Loss: 0.164135402011683\n",
            "Iteration 5490 - Loss: 0.16398634941595805\n",
            "Iteration 5500 - Loss: 0.1638377615123472\n",
            "Iteration 5510 - Loss: 0.16368963612860662\n",
            "Iteration 5520 - Loss: 0.16354197110576285\n",
            "Iteration 5530 - Loss: 0.16339476429801378\n",
            "Iteration 5540 - Loss: 0.16324801357263058\n",
            "Iteration 5550 - Loss: 0.16310171680986027\n",
            "Iteration 5560 - Loss: 0.16295587190282987\n",
            "Iteration 5570 - Loss: 0.1628104767574503\n",
            "Iteration 5580 - Loss: 0.16266552929232203\n",
            "Iteration 5590 - Loss: 0.16252102743864086\n",
            "Iteration 5600 - Loss: 0.16237696914010552\n",
            "Iteration 5610 - Loss: 0.1622333523528249\n",
            "Iteration 5620 - Loss: 0.16209017504522624\n",
            "Iteration 5630 - Loss: 0.16194743519796573\n",
            "Iteration 5640 - Loss: 0.16180513080383696\n",
            "Iteration 5650 - Loss: 0.16166325986768337\n",
            "Iteration 5660 - Loss: 0.16152182040630877\n",
            "Iteration 5670 - Loss: 0.16138081044839037\n",
            "Iteration 5680 - Loss: 0.16124022803439184\n",
            "Iteration 5690 - Loss: 0.16110007121647693\n",
            "Iteration 5700 - Loss: 0.16096033805842463\n",
            "Iteration 5710 - Loss: 0.16082102663554426\n",
            "Iteration 5720 - Loss: 0.16068213503459133\n",
            "Iteration 5730 - Loss: 0.16054366135368495\n",
            "Iteration 5740 - Loss: 0.16040560370222495\n",
            "Iteration 5750 - Loss: 0.16026796020081016\n",
            "Iteration 5760 - Loss: 0.16013072898115724\n",
            "Iteration 5770 - Loss: 0.15999390818602086\n",
            "Iteration 5780 - Loss: 0.1598574959691133\n",
            "Iteration 5790 - Loss: 0.15972149049502585\n",
            "Iteration 5800 - Loss: 0.15958588993914993\n",
            "Iteration 5810 - Loss: 0.15945069248760044\n",
            "Iteration 5820 - Loss: 0.15931589633713775\n",
            "Iteration 5830 - Loss: 0.1591814996950914\n",
            "Iteration 5840 - Loss: 0.15904750077928487\n",
            "Iteration 5850 - Loss: 0.15891389781796017\n",
            "Iteration 5860 - Loss: 0.1587806890497034\n",
            "Iteration 5870 - Loss: 0.1586478727233708\n",
            "Iteration 5880 - Loss: 0.15851544709801565\n",
            "Iteration 5890 - Loss: 0.15838341044281548\n",
            "Iteration 5900 - Loss: 0.1582517610370004\n",
            "Iteration 5910 - Loss: 0.15812049716978113\n",
            "Iteration 5920 - Loss: 0.15798961714027906\n",
            "Iteration 5930 - Loss: 0.15785911925745497\n",
            "Iteration 5940 - Loss: 0.15772900184004002\n",
            "Iteration 5950 - Loss: 0.15759926321646683\n",
            "Iteration 5960 - Loss: 0.1574699017248005\n",
            "Iteration 5970 - Loss: 0.15734091571267086\n",
            "Iteration 5980 - Loss: 0.1572123035372054\n",
            "Iteration 5990 - Loss: 0.15708406356496205\n",
            "Iteration 6000 - Loss: 0.1569561941718633\n",
            "Iteration 6010 - Loss: 0.15682869374313\n",
            "Iteration 6020 - Loss: 0.15670156067321686\n",
            "Iteration 6030 - Loss: 0.15657479336574742\n",
            "Iteration 6040 - Loss: 0.15644839023344975\n",
            "Iteration 6050 - Loss: 0.15632234969809328\n",
            "Iteration 6060 - Loss: 0.15619667019042588\n",
            "Iteration 6070 - Loss: 0.1560713501501106\n",
            "Iteration 6080 - Loss: 0.15594638802566457\n",
            "Iteration 6090 - Loss: 0.15582178227439653\n",
            "Iteration 6100 - Loss: 0.15569753136234693\n",
            "Iteration 6110 - Loss: 0.1555736337642262\n",
            "Iteration 6120 - Loss: 0.15545008796335574\n",
            "Iteration 6130 - Loss: 0.15532689245160797\n",
            "Iteration 6140 - Loss: 0.15520404572934718\n",
            "Iteration 6150 - Loss: 0.1550815463053709\n",
            "Iteration 6160 - Loss: 0.15495939269685247\n",
            "Iteration 6170 - Loss: 0.15483758342928253\n",
            "Iteration 6180 - Loss: 0.15471611703641244\n",
            "Iteration 6190 - Loss: 0.15459499206019725\n",
            "Iteration 6200 - Loss: 0.15447420705073994\n",
            "Iteration 6210 - Loss: 0.1543537605662349\n",
            "Iteration 6220 - Loss: 0.15423365117291338\n",
            "Iteration 6230 - Loss: 0.15411387744498797\n",
            "Iteration 6240 - Loss: 0.1539944379645985\n",
            "Iteration 6250 - Loss: 0.1538753313217579\n",
            "Iteration 6260 - Loss: 0.15375655611429875\n",
            "Iteration 6270 - Loss: 0.1536381109478199\n",
            "Iteration 6280 - Loss: 0.1535199944356339\n",
            "Iteration 6290 - Loss: 0.15340220519871467\n",
            "Iteration 6300 - Loss: 0.1532847418656455\n",
            "Iteration 6310 - Loss: 0.15316760307256794\n",
            "Iteration 6320 - Loss: 0.15305078746313017\n",
            "Iteration 6330 - Loss: 0.15293429368843664\n",
            "Iteration 6340 - Loss: 0.15281812040699802\n",
            "Iteration 6350 - Loss: 0.1527022662846811\n",
            "Iteration 6360 - Loss: 0.15258672999465903\n",
            "Iteration 6370 - Loss: 0.15247151021736288\n",
            "Iteration 6380 - Loss: 0.15235660564043246\n",
            "Iteration 6390 - Loss: 0.15224201495866826\n",
            "Iteration 6400 - Loss: 0.15212773687398343\n",
            "Iteration 6410 - Loss: 0.1520137700953562\n",
            "Iteration 6420 - Loss: 0.15190011333878284\n",
            "Iteration 6430 - Loss: 0.15178676532723073\n",
            "Iteration 6440 - Loss: 0.1516737247905921\n",
            "Iteration 6450 - Loss: 0.15156099046563762\n",
            "Iteration 6460 - Loss: 0.15144856109597096\n",
            "Iteration 6470 - Loss: 0.15133643543198388\n",
            "Iteration 6480 - Loss: 0.15122461223080969\n",
            "Iteration 6490 - Loss: 0.15111309025628075\n",
            "Iteration 6500 - Loss: 0.15100186827888237\n",
            "Iteration 6510 - Loss: 0.15089094507570983\n",
            "Iteration 6520 - Loss: 0.1507803194304245\n",
            "Iteration 6530 - Loss: 0.15066999013321045\n",
            "Iteration 6540 - Loss: 0.15055995598073163\n",
            "Iteration 6550 - Loss: 0.1504502157760894\n",
            "Iteration 6560 - Loss: 0.15034076832878002\n",
            "Iteration 6570 - Loss: 0.15023161245465272\n",
            "Iteration 6580 - Loss: 0.15012274697586828\n",
            "Iteration 6590 - Loss: 0.1500141707208575\n",
            "Iteration 6600 - Loss: 0.14990588252428022\n",
            "Iteration 6610 - Loss: 0.1497978812269852\n",
            "Iteration 6620 - Loss: 0.1496901656759685\n",
            "Iteration 6630 - Loss: 0.14958273472433498\n",
            "Iteration 6640 - Loss: 0.14947558723125734\n",
            "Iteration 6650 - Loss: 0.14936872206193755\n",
            "Iteration 6660 - Loss: 0.14926213808756691\n",
            "Iteration 6670 - Loss: 0.1491558341852878\n",
            "Iteration 6680 - Loss: 0.14904980923815495\n",
            "Iteration 6690 - Loss: 0.1489440621350973\n",
            "Iteration 6700 - Loss: 0.14883859177087988\n",
            "Iteration 6710 - Loss: 0.1487333970460661\n",
            "Iteration 6720 - Loss: 0.14862847686698075\n",
            "Iteration 6730 - Loss: 0.14852383014567247\n",
            "Iteration 6740 - Loss: 0.14841945579987714\n",
            "Iteration 6750 - Loss: 0.14831535275298127\n",
            "Iteration 6760 - Loss: 0.14821151993398593\n",
            "Iteration 6770 - Loss: 0.1481079562774705\n",
            "Iteration 6780 - Loss: 0.14800466072355709\n",
            "Iteration 6790 - Loss: 0.14790163221787517\n",
            "Iteration 6800 - Loss: 0.1477988697115263\n",
            "Iteration 6810 - Loss: 0.14769637216104914\n",
            "Iteration 6820 - Loss: 0.14759413852838485\n",
            "Iteration 6830 - Loss: 0.14749216778084315\n",
            "Iteration 6840 - Loss: 0.14739045889106717\n",
            "Iteration 6850 - Loss: 0.1472890108370007\n",
            "Iteration 6860 - Loss: 0.14718782260185376\n",
            "Iteration 6870 - Loss: 0.14708689317406953\n",
            "Iteration 6880 - Loss: 0.1469862215472916\n",
            "Iteration 6890 - Loss: 0.14688580672033033\n",
            "Iteration 6900 - Loss: 0.14678564769713054\n",
            "Iteration 6910 - Loss: 0.14668574348673966\n",
            "Iteration 6920 - Loss: 0.14658609310327472\n",
            "Iteration 6930 - Loss: 0.14648669556589092\n",
            "Iteration 6940 - Loss: 0.14638754989874994\n",
            "Iteration 6950 - Loss: 0.1462886551309885\n",
            "Iteration 6960 - Loss: 0.14619001029668716\n",
            "Iteration 6970 - Loss: 0.1460916144348389\n",
            "Iteration 6980 - Loss: 0.14599346658931922\n",
            "Iteration 6990 - Loss: 0.145895565808855\n",
            "Iteration 7000 - Loss: 0.1457979111469944\n",
            "Iteration 7010 - Loss: 0.14570050166207688\n",
            "Iteration 7020 - Loss: 0.1456033364172029\n",
            "Iteration 7030 - Loss: 0.145506414480205\n",
            "Iteration 7040 - Loss: 0.14540973492361792\n",
            "Iteration 7050 - Loss: 0.14531329682464952\n",
            "Iteration 7060 - Loss: 0.1452170992651516\n",
            "Iteration 7070 - Loss: 0.14512114133159168\n",
            "Iteration 7080 - Loss: 0.1450254221150239\n",
            "Iteration 7090 - Loss: 0.14492994071106063\n",
            "Iteration 7100 - Loss: 0.14483469621984502\n",
            "Iteration 7110 - Loss: 0.14473968774602233\n",
            "Iteration 7120 - Loss: 0.14464491439871222\n",
            "Iteration 7130 - Loss: 0.1445503752914819\n",
            "Iteration 7140 - Loss: 0.14445606954231807\n",
            "Iteration 7150 - Loss: 0.14436199627360022\n",
            "Iteration 7160 - Loss: 0.1442681546120734\n",
            "Iteration 7170 - Loss: 0.14417454368882163\n",
            "Iteration 7180 - Loss: 0.14408116263924117\n",
            "Iteration 7190 - Loss: 0.1439880106030147\n",
            "Iteration 7200 - Loss: 0.1438950867240845\n",
            "Iteration 7210 - Loss: 0.14380239015062685\n",
            "Iteration 7220 - Loss: 0.1437099200350258\n",
            "Iteration 7230 - Loss: 0.14361767553384835\n",
            "Iteration 7240 - Loss: 0.14352565580781831\n",
            "Iteration 7250 - Loss: 0.14343386002179134\n",
            "Iteration 7260 - Loss: 0.14334228734472976\n",
            "Iteration 7270 - Loss: 0.14325093694967778\n",
            "Iteration 7280 - Loss: 0.14315980801373712\n",
            "Iteration 7290 - Loss: 0.14306889971804165\n",
            "Iteration 7300 - Loss: 0.1429782112477338\n",
            "Iteration 7310 - Loss: 0.1428877417919399\n",
            "Iteration 7320 - Loss: 0.14279749054374677\n",
            "Iteration 7330 - Loss: 0.14270745670017707\n",
            "Iteration 7340 - Loss: 0.14261763946216627\n",
            "Iteration 7350 - Loss: 0.14252803803453903\n",
            "Iteration 7360 - Loss: 0.14243865162598546\n",
            "Iteration 7370 - Loss: 0.14234947944903864\n",
            "Iteration 7380 - Loss: 0.14226052072005102\n",
            "Iteration 7390 - Loss: 0.14217177465917183\n",
            "Iteration 7400 - Loss: 0.14208324049032442\n",
            "Iteration 7410 - Loss: 0.14199491744118373\n",
            "Iteration 7420 - Loss: 0.14190680474315379\n",
            "Iteration 7430 - Loss: 0.14181890163134586\n",
            "Iteration 7440 - Loss: 0.14173120734455588\n",
            "Iteration 7450 - Loss: 0.1416437211252429\n",
            "Iteration 7460 - Loss: 0.14155644221950747\n",
            "Iteration 7470 - Loss: 0.1414693698770697\n",
            "Iteration 7480 - Loss: 0.14138250335124808\n",
            "Iteration 7490 - Loss: 0.14129584189893812\n",
            "Iteration 7500 - Loss: 0.1412093847805911\n",
            "Iteration 7510 - Loss: 0.14112313126019332\n",
            "Iteration 7520 - Loss: 0.1410370806052451\n",
            "Iteration 7530 - Loss: 0.14095123208673999\n",
            "Iteration 7540 - Loss: 0.14086558497914461\n",
            "Iteration 7550 - Loss: 0.14078013856037766\n",
            "Iteration 7560 - Loss: 0.1406948921117901\n",
            "Iteration 7570 - Loss: 0.14060984491814488\n",
            "Iteration 7580 - Loss: 0.14052499626759712\n",
            "Iteration 7590 - Loss: 0.14044034545167364\n",
            "Iteration 7600 - Loss: 0.14035589176525426\n",
            "Iteration 7610 - Loss: 0.14027163450655114\n",
            "Iteration 7620 - Loss: 0.14018757297709034\n",
            "Iteration 7630 - Loss: 0.1401037064816916\n",
            "Iteration 7640 - Loss: 0.14002003432845\n",
            "Iteration 7650 - Loss: 0.13993655582871622\n",
            "Iteration 7660 - Loss: 0.13985327029707806\n",
            "Iteration 7670 - Loss: 0.1397701770513415\n",
            "Iteration 7680 - Loss: 0.13968727541251202\n",
            "Iteration 7690 - Loss: 0.1396045647047764\n",
            "Iteration 7700 - Loss: 0.1395220442554834\n",
            "Iteration 7710 - Loss: 0.139439713395127\n",
            "Iteration 7720 - Loss: 0.13935757145732672\n",
            "Iteration 7730 - Loss: 0.13927561777881053\n",
            "Iteration 7740 - Loss: 0.13919385169939658\n",
            "Iteration 7750 - Loss: 0.13911227256197586\n",
            "Iteration 7760 - Loss: 0.13903087971249367\n",
            "Iteration 7770 - Loss: 0.13894967249993304\n",
            "Iteration 7780 - Loss: 0.13886865027629658\n",
            "Iteration 7790 - Loss: 0.1387878123965897\n",
            "Iteration 7800 - Loss: 0.1387071582188033\n",
            "Iteration 7810 - Loss: 0.1386266871038963\n",
            "Iteration 7820 - Loss: 0.1385463984157793\n",
            "Iteration 7830 - Loss: 0.13846629152129725\n",
            "Iteration 7840 - Loss: 0.13838636579021293\n",
            "Iteration 7850 - Loss: 0.13830662059519055\n",
            "Iteration 7860 - Loss: 0.1382270553117787\n",
            "Iteration 7870 - Loss: 0.13814766931839453\n",
            "Iteration 7880 - Loss: 0.13806846199630726\n",
            "Iteration 7890 - Loss: 0.13798943272962186\n",
            "Iteration 7900 - Loss: 0.1379105809052631\n",
            "Iteration 7910 - Loss: 0.13783190591295963\n",
            "Iteration 7920 - Loss: 0.13775340714522818\n",
            "Iteration 7930 - Loss: 0.13767508399735792\n",
            "Iteration 7940 - Loss: 0.13759693586739424\n",
            "Iteration 7950 - Loss: 0.13751896215612383\n",
            "Iteration 7960 - Loss: 0.13744116226705924\n",
            "Iteration 7970 - Loss: 0.1373635356064231\n",
            "Iteration 7980 - Loss: 0.13728608158313352\n",
            "Iteration 7990 - Loss: 0.1372087996087882\n",
            "Iteration 8000 - Loss: 0.13713168909765008\n",
            "Iteration 8010 - Loss: 0.13705474946663224\n",
            "Iteration 8020 - Loss: 0.13697798013528278\n",
            "Iteration 8030 - Loss: 0.13690138052577056\n",
            "Iteration 8040 - Loss: 0.13682495006287018\n",
            "Iteration 8050 - Loss: 0.13674868817394747\n",
            "Iteration 8060 - Loss: 0.13667259428894574\n",
            "Iteration 8070 - Loss: 0.1365966678403702\n",
            "Iteration 8080 - Loss: 0.13652090826327481\n",
            "Iteration 8090 - Loss: 0.13644531499524773\n",
            "Iteration 8100 - Loss: 0.13636988747639708\n",
            "Iteration 8110 - Loss: 0.13629462514933735\n",
            "Iteration 8120 - Loss: 0.13621952745917523\n",
            "Iteration 8130 - Loss: 0.13614459385349575\n",
            "Iteration 8140 - Loss: 0.13606982378234908\n",
            "Iteration 8150 - Loss: 0.13599521669823644\n",
            "Iteration 8160 - Loss: 0.13592077205609682\n",
            "Iteration 8170 - Loss: 0.13584648931329343\n",
            "Iteration 8180 - Loss: 0.1357723679296003\n",
            "Iteration 8190 - Loss: 0.13569840736718933\n",
            "Iteration 8200 - Loss: 0.1356246070906171\n",
            "Iteration 8210 - Loss: 0.13555096656681115\n",
            "Iteration 8220 - Loss: 0.13547748526505796\n",
            "Iteration 8230 - Loss: 0.13540416265698915\n",
            "Iteration 8240 - Loss: 0.13533099821656933\n",
            "Iteration 8250 - Loss: 0.1352579914200829\n",
            "Iteration 8260 - Loss: 0.1351851417461217\n",
            "Iteration 8270 - Loss: 0.13511244867557223\n",
            "Iteration 8280 - Loss: 0.13503991169160318\n",
            "Iteration 8290 - Loss: 0.13496753027965303\n",
            "Iteration 8300 - Loss: 0.13489530392741786\n",
            "Iteration 8310 - Loss: 0.13482323212483885\n",
            "Iteration 8320 - Loss: 0.13475131436409027\n",
            "Iteration 8330 - Loss: 0.13467955013956726\n",
            "Iteration 8340 - Loss: 0.1346079389478739\n",
            "Iteration 8350 - Loss: 0.1345364802878112\n",
            "Iteration 8360 - Loss: 0.1344651736603655\n",
            "Iteration 8370 - Loss: 0.13439401856869593\n",
            "Iteration 8380 - Loss: 0.13432301451812378\n",
            "Iteration 8390 - Loss: 0.1342521610161198\n",
            "Iteration 8400 - Loss: 0.13418145757229344\n",
            "Iteration 8410 - Loss: 0.13411090369838063\n",
            "Iteration 8420 - Loss: 0.13404049890823339\n",
            "Iteration 8430 - Loss: 0.1339702427178073\n",
            "Iteration 8440 - Loss: 0.13390013464515083\n",
            "Iteration 8450 - Loss: 0.1338301742103941\n",
            "Iteration 8460 - Loss: 0.13376036093573807\n",
            "Iteration 8470 - Loss: 0.13369069434544253\n",
            "Iteration 8480 - Loss: 0.13362117396581613\n",
            "Iteration 8490 - Loss: 0.1335517993252048\n",
            "Iteration 8500 - Loss: 0.13348256995398114\n",
            "Iteration 8510 - Loss: 0.13341348538453385\n",
            "Iteration 8520 - Loss: 0.1333445451512563\n",
            "Iteration 8530 - Loss: 0.13327574879053664\n",
            "Iteration 8540 - Loss: 0.1332070958407473\n",
            "Iteration 8550 - Loss: 0.1331385858422333\n",
            "Iteration 8560 - Loss: 0.13307021833730304\n",
            "Iteration 8570 - Loss: 0.1330019928702175\n",
            "Iteration 8580 - Loss: 0.13293390898717977\n",
            "Iteration 8590 - Loss: 0.13286596623632518\n",
            "Iteration 8600 - Loss: 0.13279816416771023\n",
            "Iteration 8610 - Loss: 0.13273050233330363\n",
            "Iteration 8620 - Loss: 0.13266298028697568\n",
            "Iteration 8630 - Loss: 0.13259559758448794\n",
            "Iteration 8640 - Loss: 0.13252835378348363\n",
            "Iteration 8650 - Loss: 0.1324612484434782\n",
            "Iteration 8660 - Loss: 0.13239428112584842\n",
            "Iteration 8670 - Loss: 0.13232745139382385\n",
            "Iteration 8680 - Loss: 0.132260758812476\n",
            "Iteration 8690 - Loss: 0.13219420294870982\n",
            "Iteration 8700 - Loss: 0.13212778337125305\n",
            "Iteration 8710 - Loss: 0.13206149965064742\n",
            "Iteration 8720 - Loss: 0.13199535135923915\n",
            "Iteration 8730 - Loss: 0.1319293380711691\n",
            "Iteration 8740 - Loss: 0.13186345936236393\n",
            "Iteration 8750 - Loss: 0.1317977148105264\n",
            "Iteration 8760 - Loss: 0.13173210399512658\n",
            "Iteration 8770 - Loss: 0.13166662649739233\n",
            "Iteration 8780 - Loss: 0.13160128190030038\n",
            "Iteration 8790 - Loss: 0.131536069788567\n",
            "Iteration 8800 - Loss: 0.13147098974863938\n",
            "Iteration 8810 - Loss: 0.13140604136868655\n",
            "Iteration 8820 - Loss: 0.13134122423859038\n",
            "Iteration 8830 - Loss: 0.13127653794993685\n",
            "Iteration 8840 - Loss: 0.131211982096007\n",
            "Iteration 8850 - Loss: 0.13114755627176902\n",
            "Iteration 8860 - Loss: 0.13108326007386828\n",
            "Iteration 8870 - Loss: 0.13101909310061996\n",
            "Iteration 8880 - Loss: 0.13095505495199974\n",
            "Iteration 8890 - Loss: 0.13089114522963535\n",
            "Iteration 8900 - Loss: 0.13082736353679844\n",
            "Iteration 8910 - Loss: 0.13076370947839605\n",
            "Iteration 8920 - Loss: 0.13070018266096176\n",
            "Iteration 8930 - Loss: 0.13063678269264825\n",
            "Iteration 8940 - Loss: 0.13057350918321844\n",
            "Iteration 8950 - Loss: 0.13051036174403716\n",
            "Iteration 8960 - Loss: 0.13044733998806352\n",
            "Iteration 8970 - Loss: 0.13038444352984238\n",
            "Iteration 8980 - Loss: 0.1303216719854966\n",
            "Iteration 8990 - Loss: 0.13025902497271866\n",
            "Iteration 9000 - Loss: 0.130196502110763\n",
            "Iteration 9010 - Loss: 0.13013410302043785\n",
            "Iteration 9020 - Loss: 0.13007182732409786\n",
            "Iteration 9030 - Loss: 0.13000967464563534\n",
            "Iteration 9040 - Loss: 0.12994764461047356\n",
            "Iteration 9050 - Loss: 0.12988573684555824\n",
            "Iteration 9060 - Loss: 0.1298239509793504\n",
            "Iteration 9070 - Loss: 0.12976228664181827\n",
            "Iteration 9080 - Loss: 0.12970074346442984\n",
            "Iteration 9090 - Loss: 0.1296393210801456\n",
            "Iteration 9100 - Loss: 0.12957801912341071\n",
            "Iteration 9110 - Loss: 0.12951683723014756\n",
            "Iteration 9120 - Loss: 0.1294557750377486\n",
            "Iteration 9130 - Loss: 0.12939483218506867\n",
            "Iteration 9140 - Loss: 0.12933400831241795\n",
            "Iteration 9150 - Loss: 0.1292733030615546\n",
            "Iteration 9160 - Loss: 0.12921271607567722\n",
            "Iteration 9170 - Loss: 0.12915224699941824\n",
            "Iteration 9180 - Loss: 0.12909189547883618\n",
            "Iteration 9190 - Loss: 0.129031661161409\n",
            "Iteration 9200 - Loss: 0.12897154369602665\n",
            "Iteration 9210 - Loss: 0.12891154273298447\n",
            "Iteration 9220 - Loss: 0.12885165792397582\n",
            "Iteration 9230 - Loss: 0.12879188892208523\n",
            "Iteration 9240 - Loss: 0.12873223538178175\n",
            "Iteration 9250 - Loss: 0.12867269695891173\n",
            "Iteration 9260 - Loss: 0.12861327331069225\n",
            "Iteration 9270 - Loss: 0.1285539640957042\n",
            "Iteration 9280 - Loss: 0.12849476897388576\n",
            "Iteration 9290 - Loss: 0.12843568760652535\n",
            "Iteration 9300 - Loss: 0.12837671965625508\n",
            "Iteration 9310 - Loss: 0.12831786478704463\n",
            "Iteration 9320 - Loss: 0.12825912266419373\n",
            "Iteration 9330 - Loss: 0.1282004929543265\n",
            "Iteration 9340 - Loss: 0.12814197532538432\n",
            "Iteration 9350 - Loss: 0.1280835694466199\n",
            "Iteration 9360 - Loss: 0.12802527498859015\n",
            "Iteration 9370 - Loss: 0.1279670916231506\n",
            "Iteration 9380 - Loss: 0.12790901902344845\n",
            "Iteration 9390 - Loss: 0.12785105686391643\n",
            "Iteration 9400 - Loss: 0.12779320482026643\n",
            "Iteration 9410 - Loss: 0.12773546256948376\n",
            "Iteration 9420 - Loss: 0.1276778297898201\n",
            "Iteration 9430 - Loss: 0.12762030616078793\n",
            "Iteration 9440 - Loss: 0.12756289136315424\n",
            "Iteration 9450 - Loss: 0.1275055850789344\n",
            "Iteration 9460 - Loss: 0.12744838699138614\n",
            "Iteration 9470 - Loss: 0.12739129678500316\n",
            "Iteration 9480 - Loss: 0.12733431414551014\n",
            "Iteration 9490 - Loss: 0.12727743875985534\n",
            "Iteration 9500 - Loss: 0.127220670316206\n",
            "Iteration 9510 - Loss: 0.12716400850394155\n",
            "Iteration 9520 - Loss: 0.12710745301364812\n",
            "Iteration 9530 - Loss: 0.12705100353711268\n",
            "Iteration 9540 - Loss: 0.12699465976731725\n",
            "Iteration 9550 - Loss: 0.12693842139843317\n",
            "Iteration 9560 - Loss: 0.12688228812581523\n",
            "Iteration 9570 - Loss: 0.12682625964599606\n",
            "Iteration 9580 - Loss: 0.12677033565668064\n",
            "Iteration 9590 - Loss: 0.1267145158567402\n",
            "Iteration 9600 - Loss: 0.12665879994620732\n",
            "Iteration 9610 - Loss: 0.12660318762626976\n",
            "Iteration 9620 - Loss: 0.12654767859926497\n",
            "Iteration 9630 - Loss: 0.12649227256867512\n",
            "Iteration 9640 - Loss: 0.126436969239121\n",
            "Iteration 9650 - Loss: 0.1263817683163571\n",
            "Iteration 9660 - Loss: 0.1263266695072657\n",
            "Iteration 9670 - Loss: 0.12627167251985177\n",
            "Iteration 9680 - Loss: 0.1262167770632378\n",
            "Iteration 9690 - Loss: 0.1261619828476582\n",
            "Iteration 9700 - Loss: 0.1261072895844539\n",
            "Iteration 9710 - Loss: 0.1260526969860675\n",
            "Iteration 9720 - Loss: 0.1259982047660378\n",
            "Iteration 9730 - Loss: 0.12594381263899457\n",
            "Iteration 9740 - Loss: 0.12588952032065331\n",
            "Iteration 9750 - Loss: 0.12583532752781074\n",
            "Iteration 9760 - Loss: 0.12578123397833854\n",
            "Iteration 9770 - Loss: 0.12572723939117936\n",
            "Iteration 9780 - Loss: 0.12567334348634132\n",
            "Iteration 9790 - Loss: 0.12561954598489264\n",
            "Iteration 9800 - Loss: 0.12556584660895742\n",
            "Iteration 9810 - Loss: 0.12551224508170997\n",
            "Iteration 9820 - Loss: 0.1254587411273702\n",
            "Iteration 9830 - Loss: 0.12540533447119878\n",
            "Iteration 9840 - Loss: 0.12535202483949207\n",
            "Iteration 9850 - Loss: 0.12529881195957726\n",
            "Iteration 9860 - Loss: 0.12524569555980747\n",
            "Iteration 9870 - Loss: 0.1251926753695574\n",
            "Iteration 9880 - Loss: 0.12513975111921818\n",
            "Iteration 9890 - Loss: 0.12508692254019232\n",
            "Iteration 9900 - Loss: 0.12503418936488953\n",
            "Iteration 9910 - Loss: 0.12498155132672217\n",
            "Iteration 9920 - Loss: 0.12492900816009961\n",
            "Iteration 9930 - Loss: 0.12487655960042462\n",
            "Iteration 9940 - Loss: 0.12482420538408832\n",
            "Iteration 9950 - Loss: 0.12477194524846545\n",
            "Iteration 9960 - Loss: 0.12471977893190997\n",
            "Iteration 9970 - Loss: 0.12466770617375049\n",
            "Iteration 9980 - Loss: 0.12461572671428599\n",
            "Iteration 9990 - Loss: 0.12456384029478064\n",
            "Iteration 10000 - Loss: 0.12451204665746028\n",
            "Iteration 10010 - Loss: 0.12446034554550728\n",
            "Iteration 10020 - Loss: 0.12440873670305608\n",
            "Iteration 10030 - Loss: 0.12435721987518947\n",
            "Iteration 10040 - Loss: 0.1243057948079335\n",
            "Iteration 10050 - Loss: 0.12425446124825322\n",
            "Iteration 10060 - Loss: 0.12420321894404919\n",
            "Iteration 10070 - Loss: 0.124152067644152\n",
            "Iteration 10080 - Loss: 0.12410100709831859\n",
            "Iteration 10090 - Loss: 0.12405003705722821\n",
            "Iteration 10100 - Loss: 0.12399915727247761\n",
            "Iteration 10110 - Loss: 0.12394836749657734\n",
            "Iteration 10120 - Loss: 0.12389766748294731\n",
            "Iteration 10130 - Loss: 0.12384705698591261\n",
            "Iteration 10140 - Loss: 0.12379653576069968\n",
            "Iteration 10150 - Loss: 0.12374610356343169\n",
            "Iteration 10160 - Loss: 0.1236957601511249\n",
            "Iteration 10170 - Loss: 0.12364550528168429\n",
            "Iteration 10180 - Loss: 0.12359533871389984\n",
            "Iteration 10190 - Loss: 0.12354526020744198\n",
            "Iteration 10200 - Loss: 0.12349526952285805\n",
            "Iteration 10210 - Loss: 0.12344536642156838\n",
            "Iteration 10220 - Loss: 0.12339555066586165\n",
            "Iteration 10230 - Loss: 0.1233458220188918\n",
            "Iteration 10240 - Loss: 0.12329618024467368\n",
            "Iteration 10250 - Loss: 0.12324662510807895\n",
            "Iteration 10260 - Loss: 0.12319715637483257\n",
            "Iteration 10270 - Loss: 0.12314777381150897\n",
            "Iteration 10280 - Loss: 0.12309847718552801\n",
            "Iteration 10290 - Loss: 0.12304926626515095\n",
            "Iteration 10300 - Loss: 0.12300014081947712\n",
            "Iteration 10310 - Loss: 0.12295110061844015\n",
            "Iteration 10320 - Loss: 0.12290214543280364\n",
            "Iteration 10330 - Loss: 0.12285327503415813\n",
            "Iteration 10340 - Loss: 0.12280448919491674\n",
            "Iteration 10350 - Loss: 0.12275578768831213\n",
            "Iteration 10360 - Loss: 0.12270717028839233\n",
            "Iteration 10370 - Loss: 0.12265863677001712\n",
            "Iteration 10380 - Loss: 0.1226101869088547\n",
            "Iteration 10390 - Loss: 0.1225618204813778\n",
            "Iteration 10400 - Loss: 0.1225135372648604\n",
            "Iteration 10410 - Loss: 0.1224653370373737\n",
            "Iteration 10420 - Loss: 0.12241721957778263\n",
            "Iteration 10430 - Loss: 0.12236918466574302\n",
            "Iteration 10440 - Loss: 0.12232123208169701\n",
            "Iteration 10450 - Loss: 0.12227336160687038\n",
            "Iteration 10460 - Loss: 0.12222557302326864\n",
            "Iteration 10470 - Loss: 0.12217786611367368\n",
            "Iteration 10480 - Loss: 0.12213024066164041\n",
            "Iteration 10490 - Loss: 0.12208269645149318\n",
            "Iteration 10500 - Loss: 0.12203523326832237\n",
            "Iteration 10510 - Loss: 0.12198785089798131\n",
            "Iteration 10520 - Loss: 0.12194054912708245\n",
            "Iteration 10530 - Loss: 0.1218933277429943\n",
            "Iteration 10540 - Loss: 0.121846186533838\n",
            "Iteration 10550 - Loss: 0.121799125288484\n",
            "Iteration 10560 - Loss: 0.12175214379654882\n",
            "Iteration 10570 - Loss: 0.12170524184839163\n",
            "Iteration 10580 - Loss: 0.12165841923511109\n",
            "Iteration 10590 - Loss: 0.12161167574854215\n",
            "Iteration 10600 - Loss: 0.12156501118125258\n",
            "Iteration 10610 - Loss: 0.1215184253265401\n",
            "Iteration 10620 - Loss: 0.1214719179784288\n",
            "Iteration 10630 - Loss: 0.12142548893166624\n",
            "Iteration 10640 - Loss: 0.12137913798172019\n",
            "Iteration 10650 - Loss: 0.1213328649247754\n",
            "Iteration 10660 - Loss: 0.12128666955773065\n",
            "Iteration 10670 - Loss: 0.12124055167819552\n",
            "Iteration 10680 - Loss: 0.1211945110844871\n",
            "Iteration 10690 - Loss: 0.12114854757562735\n",
            "Iteration 10700 - Loss: 0.12110266095133956\n",
            "Iteration 10710 - Loss: 0.12105685101204561\n",
            "Iteration 10720 - Loss: 0.12101111755886287\n",
            "Iteration 10730 - Loss: 0.12096546039360112\n",
            "Iteration 10740 - Loss: 0.12091987931875947\n",
            "Iteration 10750 - Loss: 0.12087437413752347\n",
            "Iteration 10760 - Loss: 0.12082894465376227\n",
            "Iteration 10770 - Loss: 0.12078359067202556\n",
            "Iteration 10780 - Loss: 0.1207383119975402\n",
            "Iteration 10790 - Loss: 0.12069310843620804\n",
            "Iteration 10800 - Loss: 0.12064797979460254\n",
            "Iteration 10810 - Loss: 0.1206029258799659\n",
            "Iteration 10820 - Loss: 0.12055794650020633\n",
            "Iteration 10830 - Loss: 0.12051304146389495\n",
            "Iteration 10840 - Loss: 0.12046821058026334\n",
            "Iteration 10850 - Loss: 0.1204234536592002\n",
            "Iteration 10860 - Loss: 0.12037877051124882\n",
            "Iteration 10870 - Loss: 0.1203341609476043\n",
            "Iteration 10880 - Loss: 0.12028962478011053\n",
            "Iteration 10890 - Loss: 0.12024516182125762\n",
            "Iteration 10900 - Loss: 0.12020077188417898\n",
            "Iteration 10910 - Loss: 0.12015645478264886\n",
            "Iteration 10920 - Loss: 0.12011221033107908\n",
            "Iteration 10930 - Loss: 0.1200680383445168\n",
            "Iteration 10940 - Loss: 0.12002393863864161\n",
            "Iteration 10950 - Loss: 0.1199799110297627\n",
            "Iteration 10960 - Loss: 0.11993595533481656\n",
            "Iteration 10970 - Loss: 0.11989207137136387\n",
            "Iteration 10980 - Loss: 0.11984825895758701\n",
            "Iteration 10990 - Loss: 0.11980451791228777\n",
            "Iteration 11000 - Loss: 0.11976084805488396\n",
            "Iteration 11010 - Loss: 0.11971724920540745\n",
            "Iteration 11020 - Loss: 0.11967372118450137\n",
            "Iteration 11030 - Loss: 0.11963026381341732\n",
            "Iteration 11040 - Loss: 0.11958687691401321\n",
            "Iteration 11050 - Loss: 0.11954356030875038\n",
            "Iteration 11060 - Loss: 0.11950031382069083\n",
            "Iteration 11070 - Loss: 0.11945713727349533\n",
            "Iteration 11080 - Loss: 0.1194140304914204\n",
            "Iteration 11090 - Loss: 0.11937099329931591\n",
            "Iteration 11100 - Loss: 0.1193280255226226\n",
            "Iteration 11110 - Loss: 0.11928512698736966\n",
            "Iteration 11120 - Loss: 0.11924229752017208\n",
            "Iteration 11130 - Loss: 0.11919953694822842\n",
            "Iteration 11140 - Loss: 0.11915684509931805\n",
            "Iteration 11150 - Loss: 0.11911422180179906\n",
            "Iteration 11160 - Loss: 0.11907166688460552\n",
            "Iteration 11170 - Loss: 0.11902918017724542\n",
            "Iteration 11180 - Loss: 0.1189867615097978\n",
            "Iteration 11190 - Loss: 0.11894441071291088\n",
            "Iteration 11200 - Loss: 0.11890212761779927\n",
            "Iteration 11210 - Loss: 0.11885991205624179\n",
            "Iteration 11220 - Loss: 0.1188177638605789\n",
            "Iteration 11230 - Loss: 0.11877568286371097\n",
            "Iteration 11240 - Loss: 0.1187336688990951\n",
            "Iteration 11250 - Loss: 0.11869172180074342\n",
            "Iteration 11260 - Loss: 0.11864984140322049\n",
            "Iteration 11270 - Loss: 0.11860802754164121\n",
            "Iteration 11280 - Loss: 0.11856628005166824\n",
            "Iteration 11290 - Loss: 0.1185245987695101\n",
            "Iteration 11300 - Loss: 0.11848298353191862\n",
            "Iteration 11310 - Loss: 0.11844143417618674\n",
            "Iteration 11320 - Loss: 0.11839995054014639\n",
            "Iteration 11330 - Loss: 0.11835853246216609\n",
            "Iteration 11340 - Loss: 0.11831717978114892\n",
            "Iteration 11350 - Loss: 0.11827589233653017\n",
            "Iteration 11360 - Loss: 0.11823466996827521\n",
            "Iteration 11370 - Loss: 0.11819351251687721\n",
            "Iteration 11380 - Loss: 0.11815241982335506\n",
            "Iteration 11390 - Loss: 0.11811139172925131\n",
            "Iteration 11400 - Loss: 0.11807042807662979\n",
            "Iteration 11410 - Loss: 0.11802952870807358\n",
            "Iteration 11420 - Loss: 0.11798869346668278\n",
            "Iteration 11430 - Loss: 0.11794792219607268\n",
            "Iteration 11440 - Loss: 0.11790721474037129\n",
            "Iteration 11450 - Loss: 0.11786657094421736\n",
            "Iteration 11460 - Loss: 0.11782599065275848\n",
            "Iteration 11470 - Loss: 0.1177854737116487\n",
            "Iteration 11480 - Loss: 0.11774501996704667\n",
            "Iteration 11490 - Loss: 0.11770462926561355\n",
            "Iteration 11500 - Loss: 0.11766430145451075\n",
            "Iteration 11510 - Loss: 0.11762403638139818\n",
            "Iteration 11520 - Loss: 0.11758383389443222\n",
            "Iteration 11530 - Loss: 0.11754369384226333\n",
            "Iteration 11540 - Loss: 0.11750361607403444\n",
            "Iteration 11550 - Loss: 0.11746360043937874\n",
            "Iteration 11560 - Loss: 0.11742364678841784\n",
            "Iteration 11570 - Loss: 0.11738375497175968\n",
            "Iteration 11580 - Loss: 0.1173439248404964\n",
            "Iteration 11590 - Loss: 0.11730415624620268\n",
            "Iteration 11600 - Loss: 0.11726444904093368\n",
            "Iteration 11610 - Loss: 0.11722480307722312\n",
            "Iteration 11620 - Loss: 0.1171852182080811\n",
            "Iteration 11630 - Loss: 0.11714569428699259\n",
            "Iteration 11640 - Loss: 0.11710623116791516\n",
            "Iteration 11650 - Loss: 0.11706682870527726\n",
            "Iteration 11660 - Loss: 0.11702748675397635\n",
            "Iteration 11670 - Loss: 0.1169882051693768\n",
            "Iteration 11680 - Loss: 0.11694898380730835\n",
            "Iteration 11690 - Loss: 0.11690982252406378\n",
            "Iteration 11700 - Loss: 0.11687072117639744\n",
            "Iteration 11710 - Loss: 0.11683167962152333\n",
            "Iteration 11720 - Loss: 0.11679269771711316\n",
            "Iteration 11730 - Loss: 0.11675377532129448\n",
            "Iteration 11740 - Loss: 0.11671491229264895\n",
            "Iteration 11750 - Loss: 0.11667610849021064\n",
            "Iteration 11760 - Loss: 0.11663736377346384\n",
            "Iteration 11770 - Loss: 0.11659867800234167\n",
            "Iteration 11780 - Loss: 0.11656005103722408\n",
            "Iteration 11790 - Loss: 0.11652148273893619\n",
            "Iteration 11800 - Loss: 0.1164829729687463\n",
            "Iteration 11810 - Loss: 0.11644452158836428\n",
            "Iteration 11820 - Loss: 0.11640612845993976\n",
            "Iteration 11830 - Loss: 0.11636779344606057\n",
            "Iteration 11840 - Loss: 0.11632951640975088\n",
            "Iteration 11850 - Loss: 0.1162912972144691\n",
            "Iteration 11860 - Loss: 0.11625313572410688\n",
            "Iteration 11870 - Loss: 0.11621503180298666\n",
            "Iteration 11880 - Loss: 0.11617698531586056\n",
            "Iteration 11890 - Loss: 0.1161389961279083\n",
            "Iteration 11900 - Loss: 0.1161010641047357\n",
            "Iteration 11910 - Loss: 0.11606318911237296\n",
            "Iteration 11920 - Loss: 0.1160253710172728\n",
            "Iteration 11930 - Loss: 0.1159876096863092\n",
            "Iteration 11940 - Loss: 0.11594990498677506\n",
            "Iteration 11950 - Loss: 0.11591225678638169\n",
            "Iteration 11960 - Loss: 0.11587466495325563\n",
            "Iteration 11970 - Loss: 0.11583712935593847\n",
            "Iteration 11980 - Loss: 0.1157996498633842\n",
            "Iteration 11990 - Loss: 0.11576222634495827\n",
            "Iteration 12000 - Loss: 0.11572485867043553\n",
            "Iteration 12010 - Loss: 0.11568754670999867\n",
            "Iteration 12020 - Loss: 0.11565029033423704\n",
            "Iteration 12030 - Loss: 0.11561308941414443\n",
            "Iteration 12040 - Loss: 0.11557594382111809\n",
            "Iteration 12050 - Loss: 0.1155388534269566\n",
            "Iteration 12060 - Loss: 0.11550181810385877\n",
            "Iteration 12070 - Loss: 0.11546483772442179\n",
            "Iteration 12080 - Loss: 0.1154279121616399\n",
            "Iteration 12090 - Loss: 0.1153910412889024\n",
            "Iteration 12100 - Loss: 0.11535422497999281\n",
            "Iteration 12110 - Loss: 0.11531746310908678\n",
            "Iteration 12120 - Loss: 0.11528075555075061\n",
            "Iteration 12130 - Loss: 0.11524410217994005\n",
            "Iteration 12140 - Loss: 0.11520750287199848\n",
            "Iteration 12150 - Loss: 0.11517095750265562\n",
            "Iteration 12160 - Loss: 0.11513446594802572\n",
            "Iteration 12170 - Loss: 0.11509802808460662\n",
            "Iteration 12180 - Loss: 0.11506164378927768\n",
            "Iteration 12190 - Loss: 0.11502531293929862\n",
            "Iteration 12200 - Loss: 0.11498903541230805\n",
            "Iteration 12210 - Loss: 0.11495281108632187\n",
            "Iteration 12220 - Loss: 0.11491663983973177\n",
            "Iteration 12230 - Loss: 0.11488052155130427\n",
            "Iteration 12240 - Loss: 0.1148444561001784\n",
            "Iteration 12250 - Loss: 0.11480844336586529\n",
            "Iteration 12260 - Loss: 0.11477248322824582\n",
            "Iteration 12270 - Loss: 0.11473657556756983\n",
            "Iteration 12280 - Loss: 0.11470072026445423\n",
            "Iteration 12290 - Loss: 0.11466491719988216\n",
            "Iteration 12300 - Loss: 0.114629166255201\n",
            "Iteration 12310 - Loss: 0.11459346731212139\n",
            "Iteration 12320 - Loss: 0.11455782025271553\n",
            "Iteration 12330 - Loss: 0.11452222495941598\n",
            "Iteration 12340 - Loss: 0.11448668131501452\n",
            "Iteration 12350 - Loss: 0.11445118920266005\n",
            "Iteration 12360 - Loss: 0.114415748505858\n",
            "Iteration 12370 - Loss: 0.11438035910846862\n",
            "Iteration 12380 - Loss: 0.1143450208947056\n",
            "Iteration 12390 - Loss: 0.11430973374913475\n",
            "Iteration 12400 - Loss: 0.11427449755667282\n",
            "Iteration 12410 - Loss: 0.11423931220258593\n",
            "Iteration 12420 - Loss: 0.11420417757248832\n",
            "Iteration 12430 - Loss: 0.11416909355234123\n",
            "Iteration 12440 - Loss: 0.1141340600284514\n",
            "Iteration 12450 - Loss: 0.11409907688746945\n",
            "Iteration 12460 - Loss: 0.11406414401638927\n",
            "Iteration 12470 - Loss: 0.1140292613025463\n",
            "Iteration 12480 - Loss: 0.11399442863361607\n",
            "Iteration 12490 - Loss: 0.11395964589761336\n",
            "Iteration 12500 - Loss: 0.11392491298289051\n",
            "Iteration 12510 - Loss: 0.11389022977813655\n",
            "Iteration 12520 - Loss: 0.1138555961723756\n",
            "Iteration 12530 - Loss: 0.11382101205496555\n",
            "Iteration 12540 - Loss: 0.11378647731559711\n",
            "Iteration 12550 - Loss: 0.11375199184429244\n",
            "Iteration 12560 - Loss: 0.11371755553140402\n",
            "Iteration 12570 - Loss: 0.11368316826761289\n",
            "Iteration 12580 - Loss: 0.11364882994392822\n",
            "Iteration 12590 - Loss: 0.11361454045168523\n",
            "Iteration 12600 - Loss: 0.11358029968254467\n",
            "Iteration 12610 - Loss: 0.11354610752849115\n",
            "Iteration 12620 - Loss: 0.11351196388183235\n",
            "Iteration 12630 - Loss: 0.1134778686351973\n",
            "Iteration 12640 - Loss: 0.11344382168153559\n",
            "Iteration 12650 - Loss: 0.1134098229141159\n",
            "Iteration 12660 - Loss: 0.11337587222652504\n",
            "Iteration 12670 - Loss: 0.11334196951266651\n",
            "Iteration 12680 - Loss: 0.11330811466675968\n",
            "Iteration 12690 - Loss: 0.11327430758333822\n",
            "Iteration 12700 - Loss: 0.11324054815724899\n",
            "Iteration 12710 - Loss: 0.11320683628365132\n",
            "Iteration 12720 - Loss: 0.11317317185801538\n",
            "Iteration 12730 - Loss: 0.1131395547761209\n",
            "Iteration 12740 - Loss: 0.11310598493405669\n",
            "Iteration 12750 - Loss: 0.11307246222821886\n",
            "Iteration 12760 - Loss: 0.11303898655530996\n",
            "Iteration 12770 - Loss: 0.11300555781233772\n",
            "Iteration 12780 - Loss: 0.11297217589661418\n",
            "Iteration 12790 - Loss: 0.11293884070575416\n",
            "Iteration 12800 - Loss: 0.1129055521376745\n",
            "Iteration 12810 - Loss: 0.11287231009059277\n",
            "Iteration 12820 - Loss: 0.1128391144630262\n",
            "Iteration 12830 - Loss: 0.1128059651537906\n",
            "Iteration 12840 - Loss: 0.11277286206199907\n",
            "Iteration 12850 - Loss: 0.1127398050870613\n",
            "Iteration 12860 - Loss: 0.11270679412868213\n",
            "Iteration 12870 - Loss: 0.1126738290868606\n",
            "Iteration 12880 - Loss: 0.11264090986188871\n",
            "Iteration 12890 - Loss: 0.1126080363543508\n",
            "Iteration 12900 - Loss: 0.11257520846512177\n",
            "Iteration 12910 - Loss: 0.11254242609536688\n",
            "Iteration 12920 - Loss: 0.11250968914653968\n",
            "Iteration 12930 - Loss: 0.11247699752038182\n",
            "Iteration 12940 - Loss: 0.11244435111892172\n",
            "Iteration 12950 - Loss: 0.11241174984447325\n",
            "Iteration 12960 - Loss: 0.11237919359963497\n",
            "Iteration 12970 - Loss: 0.11234668228728908\n",
            "Iteration 12980 - Loss: 0.11231421581060035\n",
            "Iteration 12990 - Loss: 0.11228179407301488\n",
            "Iteration 13000 - Loss: 0.11224941697825952\n",
            "Iteration 13010 - Loss: 0.11221708443034047\n",
            "Iteration 13020 - Loss: 0.11218479633354247\n",
            "Iteration 13030 - Loss: 0.11215255259242748\n",
            "Iteration 13040 - Loss: 0.11212035311183409\n",
            "Iteration 13050 - Loss: 0.11208819779687643\n",
            "Iteration 13060 - Loss: 0.11205608655294276\n",
            "Iteration 13070 - Loss: 0.11202401928569505\n",
            "Iteration 13080 - Loss: 0.11199199590106758\n",
            "Iteration 13090 - Loss: 0.11196001630526611\n",
            "Iteration 13100 - Loss: 0.11192808040476696\n",
            "Iteration 13110 - Loss: 0.11189618810631591\n",
            "Iteration 13120 - Loss: 0.11186433931692732\n",
            "Iteration 13130 - Loss: 0.11183253394388298\n",
            "Iteration 13140 - Loss: 0.11180077189473146\n",
            "Iteration 13150 - Loss: 0.11176905307728682\n",
            "Iteration 13160 - Loss: 0.11173737739962789\n",
            "Iteration 13170 - Loss: 0.11170574477009719\n",
            "Iteration 13180 - Loss: 0.11167415509730023\n",
            "Iteration 13190 - Loss: 0.11164260829010385\n",
            "Iteration 13200 - Loss: 0.11161110425763639\n",
            "Iteration 13210 - Loss: 0.11157964290928579\n",
            "Iteration 13220 - Loss: 0.11154822415469909\n",
            "Iteration 13230 - Loss: 0.11151684790378148\n",
            "Iteration 13240 - Loss: 0.1114855140666953\n",
            "Iteration 13250 - Loss: 0.11145422255385919\n",
            "Iteration 13260 - Loss: 0.11142297327594716\n",
            "Iteration 13270 - Loss: 0.11139176614388746\n",
            "Iteration 13280 - Loss: 0.1113606010688621\n",
            "Iteration 13290 - Loss: 0.1113294779623056\n",
            "Iteration 13300 - Loss: 0.11129839673590414\n",
            "Iteration 13310 - Loss: 0.11126735730159493\n",
            "Iteration 13320 - Loss: 0.11123635957156483\n",
            "Iteration 13330 - Loss: 0.11120540345825002\n",
            "Iteration 13340 - Loss: 0.11117448887433463\n",
            "Iteration 13350 - Loss: 0.1111436157327501\n",
            "Iteration 13360 - Loss: 0.11111278394667441\n",
            "Iteration 13370 - Loss: 0.11108199342953089\n",
            "Iteration 13380 - Loss: 0.11105124409498754\n",
            "Iteration 13390 - Loss: 0.11102053585695638\n",
            "Iteration 13400 - Loss: 0.11098986862959197\n",
            "Iteration 13410 - Loss: 0.11095924232729115\n",
            "Iteration 13420 - Loss: 0.11092865686469208\n",
            "Iteration 13430 - Loss: 0.11089811215667311\n",
            "Iteration 13440 - Loss: 0.11086760811835207\n",
            "Iteration 13450 - Loss: 0.11083714466508561\n",
            "Iteration 13460 - Loss: 0.11080672171246804\n",
            "Iteration 13470 - Loss: 0.11077633917633073\n",
            "Iteration 13480 - Loss: 0.1107459969727413\n",
            "Iteration 13490 - Loss: 0.1107156950180025\n",
            "Iteration 13500 - Loss: 0.11068543322865168\n",
            "Iteration 13510 - Loss: 0.11065521152145986\n",
            "Iteration 13520 - Loss: 0.11062502981343084\n",
            "Iteration 13530 - Loss: 0.11059488802180059\n",
            "Iteration 13540 - Loss: 0.11056478606403615\n",
            "Iteration 13550 - Loss: 0.11053472385783514\n",
            "Iteration 13560 - Loss: 0.1105047013211245\n",
            "Iteration 13570 - Loss: 0.11047471837206033\n",
            "Iteration 13580 - Loss: 0.11044477492902638\n",
            "Iteration 13590 - Loss: 0.11041487091063394\n",
            "Iteration 13600 - Loss: 0.1103850062357205\n",
            "Iteration 13610 - Loss: 0.11035518082334903\n",
            "Iteration 13620 - Loss: 0.11032539459280791\n",
            "Iteration 13630 - Loss: 0.11029564746360884\n",
            "Iteration 13640 - Loss: 0.11026593935548731\n",
            "Iteration 13650 - Loss: 0.11023627018840101\n",
            "Iteration 13660 - Loss: 0.11020663988252959\n",
            "Iteration 13670 - Loss: 0.11017704835827351\n",
            "Iteration 13680 - Loss: 0.11014749553625343\n",
            "Iteration 13690 - Loss: 0.11011798133730945\n",
            "Iteration 13700 - Loss: 0.11008850568250035\n",
            "Iteration 13710 - Loss: 0.11005906849310272\n",
            "Iteration 13720 - Loss: 0.11002966969061045\n",
            "Iteration 13730 - Loss: 0.11000030919673376\n",
            "Iteration 13740 - Loss: 0.10997098693339848\n",
            "Iteration 13750 - Loss: 0.1099417028227453\n",
            "Iteration 13760 - Loss: 0.10991245678712934\n",
            "Iteration 13770 - Loss: 0.10988324874911874\n",
            "Iteration 13780 - Loss: 0.1098540786314947\n",
            "Iteration 13790 - Loss: 0.1098249463572502\n",
            "Iteration 13800 - Loss: 0.10979585184958943\n",
            "Iteration 13810 - Loss: 0.1097667950319271\n",
            "Iteration 13820 - Loss: 0.1097377758278878\n",
            "Iteration 13830 - Loss: 0.10970879416130505\n",
            "Iteration 13840 - Loss: 0.10967984995622068\n",
            "Iteration 13850 - Loss: 0.10965094313688432\n",
            "Iteration 13860 - Loss: 0.10962207362775242\n",
            "Iteration 13870 - Loss: 0.10959324135348754\n",
            "Iteration 13880 - Loss: 0.10956444623895796\n",
            "Iteration 13890 - Loss: 0.10953568820923634\n",
            "Iteration 13900 - Loss: 0.1095069671896\n",
            "Iteration 13910 - Loss: 0.10947828310552925\n",
            "Iteration 13920 - Loss: 0.10944963588270726\n",
            "Iteration 13930 - Loss: 0.10942102544701912\n",
            "Iteration 13940 - Loss: 0.10939245172455131\n",
            "Iteration 13950 - Loss: 0.10936391464159088\n",
            "Iteration 13960 - Loss: 0.10933541412462505\n",
            "Iteration 13970 - Loss: 0.10930695010033992\n",
            "Iteration 13980 - Loss: 0.10927852249562059\n",
            "Iteration 13990 - Loss: 0.10925013123754977\n",
            "Iteration 14000 - Loss: 0.10922177625340761\n",
            "Iteration 14010 - Loss: 0.10919345747067086\n",
            "Iteration 14020 - Loss: 0.10916517481701202\n",
            "Iteration 14030 - Loss: 0.10913692822029891\n",
            "Iteration 14040 - Loss: 0.10910871760859388\n",
            "Iteration 14050 - Loss: 0.10908054291015346\n",
            "Iteration 14060 - Loss: 0.10905240405342714\n",
            "Iteration 14070 - Loss: 0.10902430096705722\n",
            "Iteration 14080 - Loss: 0.10899623357987785\n",
            "Iteration 14090 - Loss: 0.10896820182091461\n",
            "Iteration 14100 - Loss: 0.1089402056193837\n",
            "Iteration 14110 - Loss: 0.10891224490469129\n",
            "Iteration 14120 - Loss: 0.10888431960643317\n",
            "Iteration 14130 - Loss: 0.10885642965439359\n",
            "Iteration 14140 - Loss: 0.1088285749785451\n",
            "Iteration 14150 - Loss: 0.10880075550904782\n",
            "Iteration 14160 - Loss: 0.10877297117624839\n",
            "Iteration 14170 - Loss: 0.10874522191068005\n",
            "Iteration 14180 - Loss: 0.10871750764306143\n",
            "Iteration 14190 - Loss: 0.10868982830429626\n",
            "Iteration 14200 - Loss: 0.10866218382547256\n",
            "Iteration 14210 - Loss: 0.1086345741378622\n",
            "Iteration 14220 - Loss: 0.10860699917292002\n",
            "Iteration 14230 - Loss: 0.10857945886228347\n",
            "Iteration 14240 - Loss: 0.108551953137772\n",
            "Iteration 14250 - Loss: 0.10852448193138614\n",
            "Iteration 14260 - Loss: 0.10849704517530737\n",
            "Iteration 14270 - Loss: 0.10846964280189711\n",
            "Iteration 14280 - Loss: 0.10844227474369632\n",
            "Iteration 14290 - Loss: 0.10841494093342487\n",
            "Iteration 14300 - Loss: 0.10838764130398085\n",
            "Iteration 14310 - Loss: 0.10836037578844021\n",
            "Iteration 14320 - Loss: 0.10833314432005584\n",
            "Iteration 14330 - Loss: 0.10830594683225739\n",
            "Iteration 14340 - Loss: 0.10827878325865016\n",
            "Iteration 14350 - Loss: 0.10825165353301518\n",
            "Iteration 14360 - Loss: 0.10822455758930791\n",
            "Iteration 14370 - Loss: 0.10819749536165806\n",
            "Iteration 14380 - Loss: 0.10817046678436931\n",
            "Iteration 14390 - Loss: 0.108143471791918\n",
            "Iteration 14400 - Loss: 0.10811651031895307\n",
            "Iteration 14410 - Loss: 0.10808958230029544\n",
            "Iteration 14420 - Loss: 0.10806268767093737\n",
            "Iteration 14430 - Loss: 0.10803582636604162\n",
            "Iteration 14440 - Loss: 0.10800899832094166\n",
            "Iteration 14450 - Loss: 0.10798220347114028\n",
            "Iteration 14460 - Loss: 0.10795544175230927\n",
            "Iteration 14470 - Loss: 0.10792871310028923\n",
            "Iteration 14480 - Loss: 0.10790201745108866\n",
            "Iteration 14490 - Loss: 0.10787535474088335\n",
            "Iteration 14500 - Loss: 0.10784872490601614\n",
            "Iteration 14510 - Loss: 0.10782212788299604\n",
            "Iteration 14520 - Loss: 0.10779556360849818\n",
            "Iteration 14530 - Loss: 0.10776903201936246\n",
            "Iteration 14540 - Loss: 0.10774253305259389\n",
            "Iteration 14550 - Loss: 0.10771606664536135\n",
            "Iteration 14560 - Loss: 0.10768963273499754\n",
            "Iteration 14570 - Loss: 0.10766323125899814\n",
            "Iteration 14580 - Loss: 0.10763686215502148\n",
            "Iteration 14590 - Loss: 0.10761052536088787\n",
            "Iteration 14600 - Loss: 0.10758422081457918\n",
            "Iteration 14610 - Loss: 0.10755794845423804\n",
            "Iteration 14620 - Loss: 0.10753170821816783\n",
            "Iteration 14630 - Loss: 0.10750550004483168\n",
            "Iteration 14640 - Loss: 0.10747932387285211\n",
            "Iteration 14650 - Loss: 0.1074531796410106\n",
            "Iteration 14660 - Loss: 0.10742706728824707\n",
            "Iteration 14670 - Loss: 0.10740098675365899\n",
            "Iteration 14680 - Loss: 0.10737493797650166\n",
            "Iteration 14690 - Loss: 0.10734892089618676\n",
            "Iteration 14700 - Loss: 0.10732293545228257\n",
            "Iteration 14710 - Loss: 0.10729698158451303\n",
            "Iteration 14720 - Loss: 0.10727105923275757\n",
            "Iteration 14730 - Loss: 0.10724516833705018\n",
            "Iteration 14740 - Loss: 0.10721930883757941\n",
            "Iteration 14750 - Loss: 0.1071934806746875\n",
            "Iteration 14760 - Loss: 0.10716768378887005\n",
            "Iteration 14770 - Loss: 0.10714191812077546\n",
            "Iteration 14780 - Loss: 0.10711618361120448\n",
            "Iteration 14790 - Loss: 0.10709048020110963\n",
            "Iteration 14800 - Loss: 0.1070648078315948\n",
            "Iteration 14810 - Loss: 0.10703916644391497\n",
            "Iteration 14820 - Loss: 0.10701355597947512\n",
            "Iteration 14830 - Loss: 0.10698797637983042\n",
            "Iteration 14840 - Loss: 0.10696242758668521\n",
            "Iteration 14850 - Loss: 0.10693690954189303\n",
            "Iteration 14860 - Loss: 0.10691142218745564\n",
            "Iteration 14870 - Loss: 0.10688596546552291\n",
            "Iteration 14880 - Loss: 0.10686053931839226\n",
            "Iteration 14890 - Loss: 0.10683514368850798\n",
            "Iteration 14900 - Loss: 0.10680977851846107\n",
            "Iteration 14910 - Loss: 0.10678444375098851\n",
            "Iteration 14920 - Loss: 0.10675913932897296\n",
            "Iteration 14930 - Loss: 0.10673386519544223\n",
            "Iteration 14940 - Loss: 0.10670862129356903\n",
            "Iteration 14950 - Loss: 0.10668340756666982\n",
            "Iteration 14960 - Loss: 0.1066582239582054\n",
            "Iteration 14970 - Loss: 0.10663307041177951\n",
            "Iteration 14980 - Loss: 0.10660794687113885\n",
            "Iteration 14990 - Loss: 0.10658285328017265\n",
            "Iteration 15000 - Loss: 0.10655778958291191\n",
            "Iteration 15010 - Loss: 0.10653275572352922\n",
            "Iteration 15020 - Loss: 0.10650775164633833\n",
            "Iteration 15030 - Loss: 0.10648277729579333\n",
            "Iteration 15040 - Loss: 0.10645783261648885\n",
            "Iteration 15050 - Loss: 0.10643291755315884\n",
            "Iteration 15060 - Loss: 0.10640803205067684\n",
            "Iteration 15070 - Loss: 0.10638317605405498\n",
            "Iteration 15080 - Loss: 0.10635834950844403\n",
            "Iteration 15090 - Loss: 0.10633355235913251\n",
            "Iteration 15100 - Loss: 0.10630878455154669\n",
            "Iteration 15110 - Loss: 0.10628404603124963\n",
            "Iteration 15120 - Loss: 0.10625933674394125\n",
            "Iteration 15130 - Loss: 0.10623465663545757\n",
            "Iteration 15140 - Loss: 0.10621000565177063\n",
            "Iteration 15150 - Loss: 0.10618538373898762\n",
            "Iteration 15160 - Loss: 0.1061607908433507\n",
            "Iteration 15170 - Loss: 0.10613622691123673\n",
            "Iteration 15180 - Loss: 0.10611169188915637\n",
            "Iteration 15190 - Loss: 0.10608718572375414\n",
            "Iteration 15200 - Loss: 0.10606270836180784\n",
            "Iteration 15210 - Loss: 0.10603825975022808\n",
            "Iteration 15220 - Loss: 0.1060138398360578\n",
            "Iteration 15230 - Loss: 0.10598944856647198\n",
            "Iteration 15240 - Loss: 0.1059650858887773\n",
            "Iteration 15250 - Loss: 0.10594075175041151\n",
            "Iteration 15260 - Loss: 0.10591644609894313\n",
            "Iteration 15270 - Loss: 0.1058921688820711\n",
            "Iteration 15280 - Loss: 0.10586792004762426\n",
            "Iteration 15290 - Loss: 0.10584369954356104\n",
            "Iteration 15300 - Loss: 0.10581950731796895\n",
            "Iteration 15310 - Loss: 0.10579534331906436\n",
            "Iteration 15320 - Loss: 0.10577120749519185\n",
            "Iteration 15330 - Loss: 0.10574709979482406\n",
            "Iteration 15340 - Loss: 0.10572302016656114\n",
            "Iteration 15350 - Loss: 0.10569896855913016\n",
            "Iteration 15360 - Loss: 0.10567494492138532\n",
            "Iteration 15370 - Loss: 0.10565094920230694\n",
            "Iteration 15380 - Loss: 0.10562698135100139\n",
            "Iteration 15390 - Loss: 0.10560304131670059\n",
            "Iteration 15400 - Loss: 0.10557912904876163\n",
            "Iteration 15410 - Loss: 0.10555524449666644\n",
            "Iteration 15420 - Loss: 0.1055313876100212\n",
            "Iteration 15430 - Loss: 0.10550755833855642\n",
            "Iteration 15440 - Loss: 0.10548375663212597\n",
            "Iteration 15450 - Loss: 0.10545998244070715\n",
            "Iteration 15460 - Loss: 0.1054362357144\n",
            "Iteration 15470 - Loss: 0.10541251640342733\n",
            "Iteration 15480 - Loss: 0.10538882445813366\n",
            "Iteration 15490 - Loss: 0.10536515982898564\n",
            "Iteration 15500 - Loss: 0.10534152246657108\n",
            "Iteration 15510 - Loss: 0.10531791232159878\n",
            "Iteration 15520 - Loss: 0.10529432934489832\n",
            "Iteration 15530 - Loss: 0.10527077348741946\n",
            "Iteration 15540 - Loss: 0.10524724470023171\n",
            "Iteration 15550 - Loss: 0.1052237429345243\n",
            "Iteration 15560 - Loss: 0.10520026814160559\n",
            "Iteration 15570 - Loss: 0.10517682027290247\n",
            "Iteration 15580 - Loss: 0.10515339927996062\n",
            "Iteration 15590 - Loss: 0.10513000511444351\n",
            "Iteration 15600 - Loss: 0.1051066377281324\n",
            "Iteration 15610 - Loss: 0.10508329707292592\n",
            "Iteration 15620 - Loss: 0.10505998310083964\n",
            "Iteration 15630 - Loss: 0.10503669576400575\n",
            "Iteration 15640 - Loss: 0.10501343501467263\n",
            "Iteration 15650 - Loss: 0.10499020080520474\n",
            "Iteration 15660 - Loss: 0.1049669930880819\n",
            "Iteration 15670 - Loss: 0.10494381181589936\n",
            "Iteration 15680 - Loss: 0.10492065694136699\n",
            "Iteration 15690 - Loss: 0.10489752841730927\n",
            "Iteration 15700 - Loss: 0.1048744261966649\n",
            "Iteration 15710 - Loss: 0.10485135023248629\n",
            "Iteration 15720 - Loss: 0.10482830047793933\n",
            "Iteration 15730 - Loss: 0.10480527688630302\n",
            "Iteration 15740 - Loss: 0.10478227941096921\n",
            "Iteration 15750 - Loss: 0.10475930800544198\n",
            "Iteration 15760 - Loss: 0.10473636262333783\n",
            "Iteration 15770 - Loss: 0.10471344321838477\n",
            "Iteration 15780 - Loss: 0.10469054974442217\n",
            "Iteration 15790 - Loss: 0.10466768215540069\n",
            "Iteration 15800 - Loss: 0.10464484040538156\n",
            "Iteration 15810 - Loss: 0.10462202444853652\n",
            "Iteration 15820 - Loss: 0.10459923423914735\n",
            "Iteration 15830 - Loss: 0.10457646973160553\n",
            "Iteration 15840 - Loss: 0.10455373088041195\n",
            "Iteration 15850 - Loss: 0.10453101764017662\n",
            "Iteration 15860 - Loss: 0.10450832996561832\n",
            "Iteration 15870 - Loss: 0.10448566781156414\n",
            "Iteration 15880 - Loss: 0.10446303113294941\n",
            "Iteration 15890 - Loss: 0.10444041988481716\n",
            "Iteration 15900 - Loss: 0.10441783402231775\n",
            "Iteration 15910 - Loss: 0.10439527350070903\n",
            "Iteration 15920 - Loss: 0.1043727382753552\n",
            "Iteration 15930 - Loss: 0.1043502283017272\n",
            "Iteration 15940 - Loss: 0.10432774353540204\n",
            "Iteration 15950 - Loss: 0.10430528393206272\n",
            "Iteration 15960 - Loss: 0.10428284944749747\n",
            "Iteration 15970 - Loss: 0.10426044003760014\n",
            "Iteration 15980 - Loss: 0.10423805565836917\n",
            "Iteration 15990 - Loss: 0.10421569626590768\n",
            "Iteration 16000 - Loss: 0.1041933618164231\n",
            "Iteration 16010 - Loss: 0.10417105226622667\n",
            "Iteration 16020 - Loss: 0.10414876757173343\n",
            "Iteration 16030 - Loss: 0.10412650768946174\n",
            "Iteration 16040 - Loss: 0.10410427257603296\n",
            "Iteration 16050 - Loss: 0.10408206218817115\n",
            "Iteration 16060 - Loss: 0.1040598764827029\n",
            "Iteration 16070 - Loss: 0.10403771541655676\n",
            "Iteration 16080 - Loss: 0.10401557894676322\n",
            "Iteration 16090 - Loss: 0.10399346703045423\n",
            "Iteration 16100 - Loss: 0.10397137962486283\n",
            "Iteration 16110 - Loss: 0.10394931668732331\n",
            "Iteration 16120 - Loss: 0.10392727817527007\n",
            "Iteration 16130 - Loss: 0.10390526404623836\n",
            "Iteration 16140 - Loss: 0.10388327425786296\n",
            "Iteration 16150 - Loss: 0.10386130876787868\n",
            "Iteration 16160 - Loss: 0.10383936753411964\n",
            "Iteration 16170 - Loss: 0.10381745051451907\n",
            "Iteration 16180 - Loss: 0.10379555766710909\n",
            "Iteration 16190 - Loss: 0.10377368895002029\n",
            "Iteration 16200 - Loss: 0.10375184432148167\n",
            "Iteration 16210 - Loss: 0.10373002373982\n",
            "Iteration 16220 - Loss: 0.10370822716345975\n",
            "Iteration 16230 - Loss: 0.10368645455092293\n",
            "Iteration 16240 - Loss: 0.1036647058608284\n",
            "Iteration 16250 - Loss: 0.10364298105189218\n",
            "Iteration 16260 - Loss: 0.10362128008292641\n",
            "Iteration 16270 - Loss: 0.10359960291283982\n",
            "Iteration 16280 - Loss: 0.1035779495006369\n",
            "Iteration 16290 - Loss: 0.10355631980541784\n",
            "Iteration 16300 - Loss: 0.10353471378637835\n",
            "Iteration 16310 - Loss: 0.10351313140280914\n",
            "Iteration 16320 - Loss: 0.10349157261409582\n",
            "Iteration 16330 - Loss: 0.10347003737971845\n",
            "Iteration 16340 - Loss: 0.10344852565925157\n",
            "Iteration 16350 - Loss: 0.10342703741236356\n",
            "Iteration 16360 - Loss: 0.10340557259881666\n",
            "Iteration 16370 - Loss: 0.10338413117846644\n",
            "Iteration 16380 - Loss: 0.10336271311126179\n",
            "Iteration 16390 - Loss: 0.10334131835724437\n",
            "Iteration 16400 - Loss: 0.10331994687654864\n",
            "Iteration 16410 - Loss: 0.10329859862940136\n",
            "Iteration 16420 - Loss: 0.10327727357612136\n",
            "Iteration 16430 - Loss: 0.10325597167711946\n",
            "Iteration 16440 - Loss: 0.10323469289289777\n",
            "Iteration 16450 - Loss: 0.10321343718404993\n",
            "Iteration 16460 - Loss: 0.1031922045112606\n",
            "Iteration 16470 - Loss: 0.10317099483530515\n",
            "Iteration 16480 - Loss: 0.10314980811704946\n",
            "Iteration 16490 - Loss: 0.1031286443174497\n",
            "Iteration 16500 - Loss: 0.1031075033975521\n",
            "Iteration 16510 - Loss: 0.10308638531849239\n",
            "Iteration 16520 - Loss: 0.10306529004149594\n",
            "Iteration 16530 - Loss: 0.10304421752787724\n",
            "Iteration 16540 - Loss: 0.1030231677390399\n",
            "Iteration 16550 - Loss: 0.10300214063647596\n",
            "Iteration 16560 - Loss: 0.1029811361817662\n",
            "Iteration 16570 - Loss: 0.10296015433657914\n",
            "Iteration 16580 - Loss: 0.10293919506267175\n",
            "Iteration 16590 - Loss: 0.10291825832188828\n",
            "Iteration 16600 - Loss: 0.10289734407616062\n",
            "Iteration 16610 - Loss: 0.10287645228750768\n",
            "Iteration 16620 - Loss: 0.10285558291803536\n",
            "Iteration 16630 - Loss: 0.10283473592993617\n",
            "Iteration 16640 - Loss: 0.10281391128548917\n",
            "Iteration 16650 - Loss: 0.10279310894705947\n",
            "Iteration 16660 - Loss: 0.10277232887709813\n",
            "Iteration 16670 - Loss: 0.10275157103814185\n",
            "Iteration 16680 - Loss: 0.10273083539281277\n",
            "Iteration 16690 - Loss: 0.1027101219038183\n",
            "Iteration 16700 - Loss: 0.10268943053395066\n",
            "Iteration 16710 - Loss: 0.10266876124608687\n",
            "Iteration 16720 - Loss: 0.10264811400318838\n",
            "Iteration 16730 - Loss: 0.10262748876830073\n",
            "Iteration 16740 - Loss: 0.10260688550455364\n",
            "Iteration 16750 - Loss: 0.10258630417516042\n",
            "Iteration 16760 - Loss: 0.10256574474341788\n",
            "Iteration 16770 - Loss: 0.10254520717270617\n",
            "Iteration 16780 - Loss: 0.10252469142648828\n",
            "Iteration 16790 - Loss: 0.10250419746831024\n",
            "Iteration 16800 - Loss: 0.10248372526180032\n",
            "Iteration 16810 - Loss: 0.10246327477066935\n",
            "Iteration 16820 - Loss: 0.10244284595871013\n",
            "Iteration 16830 - Loss: 0.10242243878979725\n",
            "Iteration 16840 - Loss: 0.10240205322788706\n",
            "Iteration 16850 - Loss: 0.10238168923701711\n",
            "Iteration 16860 - Loss: 0.1023613467813062\n",
            "Iteration 16870 - Loss: 0.102341025824954\n",
            "Iteration 16880 - Loss: 0.10232072633224092\n",
            "Iteration 16890 - Loss: 0.10230044826752775\n",
            "Iteration 16900 - Loss: 0.1022801915952556\n",
            "Iteration 16910 - Loss: 0.10225995627994547\n",
            "Iteration 16920 - Loss: 0.10223974228619824\n",
            "Iteration 16930 - Loss: 0.10221954957869427\n",
            "Iteration 16940 - Loss: 0.10219937812219325\n",
            "Iteration 16950 - Loss: 0.1021792278815341\n",
            "Iteration 16960 - Loss: 0.10215909882163432\n",
            "Iteration 16970 - Loss: 0.10213899090749032\n",
            "Iteration 16980 - Loss: 0.10211890410417687\n",
            "Iteration 16990 - Loss: 0.102098838376847\n",
            "Iteration 17000 - Loss: 0.10207879369073168\n",
            "Iteration 17010 - Loss: 0.10205877001113968\n",
            "Iteration 17020 - Loss: 0.10203876730345704\n",
            "Iteration 17030 - Loss: 0.10201878553314771\n",
            "Iteration 17040 - Loss: 0.10199882466575226\n",
            "Iteration 17050 - Loss: 0.10197888466688841\n",
            "Iteration 17060 - Loss: 0.10195896550225048\n",
            "Iteration 17070 - Loss: 0.10193906713760922\n",
            "Iteration 17080 - Loss: 0.10191918953881171\n",
            "Iteration 17090 - Loss: 0.1018993326717811\n",
            "Iteration 17100 - Loss: 0.10187949650251628\n",
            "Iteration 17110 - Loss: 0.10185968099709174\n",
            "Iteration 17120 - Loss: 0.10183988612165741\n",
            "Iteration 17130 - Loss: 0.10182011184243858\n",
            "Iteration 17140 - Loss: 0.10180035812573526\n",
            "Iteration 17150 - Loss: 0.10178062493792234\n",
            "Iteration 17160 - Loss: 0.10176091224544939\n",
            "Iteration 17170 - Loss: 0.10174122001483993\n",
            "Iteration 17180 - Loss: 0.10172154821269232\n",
            "Iteration 17190 - Loss: 0.1017018968056782\n",
            "Iteration 17200 - Loss: 0.1016822657605433\n",
            "Iteration 17210 - Loss: 0.10166265504410689\n",
            "Iteration 17220 - Loss: 0.10164306462326134\n",
            "Iteration 17230 - Loss: 0.10162349446497233\n",
            "Iteration 17240 - Loss: 0.10160394453627844\n",
            "Iteration 17250 - Loss: 0.1015844148042909\n",
            "Iteration 17260 - Loss: 0.10156490523619352\n",
            "Iteration 17270 - Loss: 0.10154541579924234\n",
            "Iteration 17280 - Loss: 0.1015259464607657\n",
            "Iteration 17290 - Loss: 0.10150649718816364\n",
            "Iteration 17300 - Loss: 0.10148706794890801\n",
            "Iteration 17310 - Loss: 0.10146765871054216\n",
            "Iteration 17320 - Loss: 0.10144826944068085\n",
            "Iteration 17330 - Loss: 0.10142890010700982\n",
            "Iteration 17340 - Loss: 0.10140955067728574\n",
            "Iteration 17350 - Loss: 0.10139022111933618\n",
            "Iteration 17360 - Loss: 0.10137091140105912\n",
            "Iteration 17370 - Loss: 0.10135162149042284\n",
            "Iteration 17380 - Loss: 0.10133235135546577\n",
            "Iteration 17390 - Loss: 0.10131310096429647\n",
            "Iteration 17400 - Loss: 0.10129387028509305\n",
            "Iteration 17410 - Loss: 0.10127465928610321\n",
            "Iteration 17420 - Loss: 0.10125546793564426\n",
            "Iteration 17430 - Loss: 0.10123629620210238\n",
            "Iteration 17440 - Loss: 0.10121714405393288\n",
            "Iteration 17450 - Loss: 0.10119801145965995\n",
            "Iteration 17460 - Loss: 0.1011788983878763\n",
            "Iteration 17470 - Loss: 0.10115980480724308\n",
            "Iteration 17480 - Loss: 0.10114073068648974\n",
            "Iteration 17490 - Loss: 0.10112167599441368\n",
            "Iteration 17500 - Loss: 0.1011026406998803\n",
            "Iteration 17510 - Loss: 0.10108362477182255\n",
            "Iteration 17520 - Loss: 0.101064628179241\n",
            "Iteration 17530 - Loss: 0.10104565089120346\n",
            "Iteration 17540 - Loss: 0.10102669287684485\n",
            "Iteration 17550 - Loss: 0.10100775410536707\n",
            "Iteration 17560 - Loss: 0.10098883454603885\n",
            "Iteration 17570 - Loss: 0.1009699341681953\n",
            "Iteration 17580 - Loss: 0.1009510529412382\n",
            "Iteration 17590 - Loss: 0.1009321908346353\n",
            "Iteration 17600 - Loss: 0.10091334781792059\n",
            "Iteration 17610 - Loss: 0.10089452386069377\n",
            "Iteration 17620 - Loss: 0.10087571893262022\n",
            "Iteration 17630 - Loss: 0.10085693300343099\n",
            "Iteration 17640 - Loss: 0.10083816604292221\n",
            "Iteration 17650 - Loss: 0.10081941802095534\n",
            "Iteration 17660 - Loss: 0.10080068890745679\n",
            "Iteration 17670 - Loss: 0.10078197867241773\n",
            "Iteration 17680 - Loss: 0.10076328728589382\n",
            "Iteration 17690 - Loss: 0.10074461471800528\n",
            "Iteration 17700 - Loss: 0.10072596093893674\n",
            "Iteration 17710 - Loss: 0.10070732591893668\n",
            "Iteration 17720 - Loss: 0.10068870962831761\n",
            "Iteration 17730 - Loss: 0.10067011203745574\n",
            "Iteration 17740 - Loss: 0.10065153311679093\n",
            "Iteration 17750 - Loss: 0.10063297283682636\n",
            "Iteration 17760 - Loss: 0.10061443116812849\n",
            "Iteration 17770 - Loss: 0.10059590808132682\n",
            "Iteration 17780 - Loss: 0.10057740354711378\n",
            "Iteration 17790 - Loss: 0.10055891753624448\n",
            "Iteration 17800 - Loss: 0.10054045001953657\n",
            "Iteration 17810 - Loss: 0.10052200096787015\n",
            "Iteration 17820 - Loss: 0.10050357035218742\n",
            "Iteration 17830 - Loss: 0.10048515814349275\n",
            "Iteration 17840 - Loss: 0.10046676431285233\n",
            "Iteration 17850 - Loss: 0.10044838883139408\n",
            "Iteration 17860 - Loss: 0.10043003167030748\n",
            "Iteration 17870 - Loss: 0.10041169280084337\n",
            "Iteration 17880 - Loss: 0.10039337219431381\n",
            "Iteration 17890 - Loss: 0.10037506982209196\n",
            "Iteration 17900 - Loss: 0.1003567856556118\n",
            "Iteration 17910 - Loss: 0.10033851966636807\n",
            "Iteration 17920 - Loss: 0.10032027182591613\n",
            "Iteration 17930 - Loss: 0.1003020421058717\n",
            "Iteration 17940 - Loss: 0.10028383047791072\n",
            "Iteration 17950 - Loss: 0.10026563691376923\n",
            "Iteration 17960 - Loss: 0.10024746138524326\n",
            "Iteration 17970 - Loss: 0.1002293038641885\n",
            "Iteration 17980 - Loss: 0.10021116432252025\n",
            "Iteration 17990 - Loss: 0.1001930427322133\n",
            "Iteration 18000 - Loss: 0.10017493906530175\n",
            "Iteration 18010 - Loss: 0.10015685329387881\n",
            "Iteration 18020 - Loss: 0.1001387853900966\n",
            "Iteration 18030 - Loss: 0.10012073532616607\n",
            "Iteration 18040 - Loss: 0.10010270307435686\n",
            "Iteration 18050 - Loss: 0.10008468860699714\n",
            "Iteration 18060 - Loss: 0.10006669189647342\n",
            "Iteration 18070 - Loss: 0.10004871291523025\n",
            "Iteration 18080 - Loss: 0.10003075163577049\n",
            "Iteration 18090 - Loss: 0.10001280803065464\n",
            "Iteration 18100 - Loss: 0.09999488207250098\n",
            "Iteration 18110 - Loss: 0.09997697373398542\n",
            "Iteration 18120 - Loss: 0.09995908298784137\n",
            "Iteration 18130 - Loss: 0.0999412098068593\n",
            "Iteration 18140 - Loss: 0.09992335416388706\n",
            "Iteration 18150 - Loss: 0.0999055160318292\n",
            "Iteration 18160 - Loss: 0.0998876953836473\n",
            "Iteration 18170 - Loss: 0.0998698921923595\n",
            "Iteration 18180 - Loss: 0.09985210643104046\n",
            "Iteration 18190 - Loss: 0.09983433807282126\n",
            "Iteration 18200 - Loss: 0.09981658709088924\n",
            "Iteration 18210 - Loss: 0.09979885345848767\n",
            "Iteration 18220 - Loss: 0.09978113714891586\n",
            "Iteration 18230 - Loss: 0.09976343813552892\n",
            "Iteration 18240 - Loss: 0.09974575639173747\n",
            "Iteration 18250 - Loss: 0.0997280918910077\n",
            "Iteration 18260 - Loss: 0.09971044460686124\n",
            "Iteration 18270 - Loss: 0.09969281451287465\n",
            "Iteration 18280 - Loss: 0.0996752015826798\n",
            "Iteration 18290 - Loss: 0.09965760578996326\n",
            "Iteration 18300 - Loss: 0.09964002710846656\n",
            "Iteration 18310 - Loss: 0.09962246551198571\n",
            "Iteration 18320 - Loss: 0.09960492097437129\n",
            "Iteration 18330 - Loss: 0.09958739346952812\n",
            "Iteration 18340 - Loss: 0.09956988297141521\n",
            "Iteration 18350 - Loss: 0.09955238945404572\n",
            "Iteration 18360 - Loss: 0.09953491289148665\n",
            "Iteration 18370 - Loss: 0.09951745325785875\n",
            "Iteration 18380 - Loss: 0.09950001052733644\n",
            "Iteration 18390 - Loss: 0.09948258467414758\n",
            "Iteration 18400 - Loss: 0.09946517567257343\n",
            "Iteration 18410 - Loss: 0.09944778349694847\n",
            "Iteration 18420 - Loss: 0.09943040812166011\n",
            "Iteration 18430 - Loss: 0.0994130495211489\n",
            "Iteration 18440 - Loss: 0.09939570766990803\n",
            "Iteration 18450 - Loss: 0.09937838254248339\n",
            "Iteration 18460 - Loss: 0.09936107411347343\n",
            "Iteration 18470 - Loss: 0.09934378235752891\n",
            "Iteration 18480 - Loss: 0.09932650724935285\n",
            "Iteration 18490 - Loss: 0.09930924876370048\n",
            "Iteration 18500 - Loss: 0.09929200687537891\n",
            "Iteration 18510 - Loss: 0.09927478155924703\n",
            "Iteration 18520 - Loss: 0.0992575727902156\n",
            "Iteration 18530 - Loss: 0.09924038054324688\n",
            "Iteration 18540 - Loss: 0.09922320479335456\n",
            "Iteration 18550 - Loss: 0.09920604551560362\n",
            "Iteration 18560 - Loss: 0.09918890268511023\n",
            "Iteration 18570 - Loss: 0.0991717762770416\n",
            "Iteration 18580 - Loss: 0.09915466626661588\n",
            "Iteration 18590 - Loss: 0.09913757262910194\n",
            "Iteration 18600 - Loss: 0.0991204953398194\n",
            "Iteration 18610 - Loss: 0.09910343437413832\n",
            "Iteration 18620 - Loss: 0.09908638970747909\n",
            "Iteration 18630 - Loss: 0.09906936131531245\n",
            "Iteration 18640 - Loss: 0.09905234917315922\n",
            "Iteration 18650 - Loss: 0.09903535325659028\n",
            "Iteration 18660 - Loss: 0.09901837354122635\n",
            "Iteration 18670 - Loss: 0.09900141000273782\n",
            "Iteration 18680 - Loss: 0.09898446261684483\n",
            "Iteration 18690 - Loss: 0.09896753135931691\n",
            "Iteration 18700 - Loss: 0.09895061620597291\n",
            "Iteration 18710 - Loss: 0.09893371713268105\n",
            "Iteration 18720 - Loss: 0.09891683411535854\n",
            "Iteration 18730 - Loss: 0.09889996712997161\n",
            "Iteration 18740 - Loss: 0.09888311615253541\n",
            "Iteration 18750 - Loss: 0.09886628115911356\n",
            "Iteration 18760 - Loss: 0.09884946212581873\n",
            "Iteration 18770 - Loss: 0.0988326590288117\n",
            "Iteration 18780 - Loss: 0.09881587184430175\n",
            "Iteration 18790 - Loss: 0.0987991005485464\n",
            "Iteration 18800 - Loss: 0.09878234511785118\n",
            "Iteration 18810 - Loss: 0.09876560552856972\n",
            "Iteration 18820 - Loss: 0.09874888175710354\n",
            "Iteration 18830 - Loss: 0.09873217377990176\n",
            "Iteration 18840 - Loss: 0.09871548157346124\n",
            "Iteration 18850 - Loss: 0.09869880511432633\n",
            "Iteration 18860 - Loss: 0.0986821443790886\n",
            "Iteration 18870 - Loss: 0.0986654993443872\n",
            "Iteration 18880 - Loss: 0.09864886998690822\n",
            "Iteration 18890 - Loss: 0.09863225628338462\n",
            "Iteration 18900 - Loss: 0.09861565821059652\n",
            "Iteration 18910 - Loss: 0.09859907574537069\n",
            "Iteration 18920 - Loss: 0.09858250886458064\n",
            "Iteration 18930 - Loss: 0.09856595754514631\n",
            "Iteration 18940 - Loss: 0.09854942176403428\n",
            "Iteration 18950 - Loss: 0.09853290149825712\n",
            "Iteration 18960 - Loss: 0.0985163967248739\n",
            "Iteration 18970 - Loss: 0.09849990742098946\n",
            "Iteration 18980 - Loss: 0.09848343356375494\n",
            "Iteration 18990 - Loss: 0.09846697513036706\n",
            "Iteration 19000 - Loss: 0.0984505320980683\n",
            "Iteration 19010 - Loss: 0.09843410444414684\n",
            "Iteration 19020 - Loss: 0.09841769214593642\n",
            "Iteration 19030 - Loss: 0.09840129518081583\n",
            "Iteration 19040 - Loss: 0.09838491352620962\n",
            "Iteration 19050 - Loss: 0.09836854715958693\n",
            "Iteration 19060 - Loss: 0.09835219605846249\n",
            "Iteration 19070 - Loss: 0.09833586020039552\n",
            "Iteration 19080 - Loss: 0.09831953956299032\n",
            "Iteration 19090 - Loss: 0.09830323412389574\n",
            "Iteration 19100 - Loss: 0.09828694386080528\n",
            "Iteration 19110 - Loss: 0.09827066875145689\n",
            "Iteration 19120 - Loss: 0.09825440877363291\n",
            "Iteration 19130 - Loss: 0.09823816390515996\n",
            "Iteration 19140 - Loss: 0.09822193412390863\n",
            "Iteration 19150 - Loss: 0.09820571940779381\n",
            "Iteration 19160 - Loss: 0.098189519734774\n",
            "Iteration 19170 - Loss: 0.09817333508285186\n",
            "Iteration 19180 - Loss: 0.0981571654300734\n",
            "Iteration 19190 - Loss: 0.09814101075452855\n",
            "Iteration 19200 - Loss: 0.09812487103435034\n",
            "Iteration 19210 - Loss: 0.09810874624771555\n",
            "Iteration 19220 - Loss: 0.09809263637284403\n",
            "Iteration 19230 - Loss: 0.09807654138799882\n",
            "Iteration 19240 - Loss: 0.09806046127148592\n",
            "Iteration 19250 - Loss: 0.09804439600165436\n",
            "Iteration 19260 - Loss: 0.09802834555689607\n",
            "Iteration 19270 - Loss: 0.0980123099156456\n",
            "Iteration 19280 - Loss: 0.09799628905638019\n",
            "Iteration 19290 - Loss: 0.0979802829576195\n",
            "Iteration 19300 - Loss: 0.09796429159792569\n",
            "Iteration 19310 - Loss: 0.0979483149559032\n",
            "Iteration 19320 - Loss: 0.0979323530101986\n",
            "Iteration 19330 - Loss: 0.09791640573950065\n",
            "Iteration 19340 - Loss: 0.09790047312254006\n",
            "Iteration 19350 - Loss: 0.0978845551380894\n",
            "Iteration 19360 - Loss: 0.09786865176496308\n",
            "Iteration 19370 - Loss: 0.0978527629820172\n",
            "Iteration 19380 - Loss: 0.09783688876814921\n",
            "Iteration 19390 - Loss: 0.09782102910229838\n",
            "Iteration 19400 - Loss: 0.09780518396344506\n",
            "Iteration 19410 - Loss: 0.09778935333061103\n",
            "Iteration 19420 - Loss: 0.0977735371828592\n",
            "Iteration 19430 - Loss: 0.09775773549929347\n",
            "Iteration 19440 - Loss: 0.09774194825905878\n",
            "Iteration 19450 - Loss: 0.09772617544134085\n",
            "Iteration 19460 - Loss: 0.09771041702536627\n",
            "Iteration 19470 - Loss: 0.09769467299040234\n",
            "Iteration 19480 - Loss: 0.09767894331575659\n",
            "Iteration 19490 - Loss: 0.09766322798077728\n",
            "Iteration 19500 - Loss: 0.09764752696485296\n",
            "Iteration 19510 - Loss: 0.0976318402474126\n",
            "Iteration 19520 - Loss: 0.097616167807925\n",
            "Iteration 19530 - Loss: 0.09760050962589929\n",
            "Iteration 19540 - Loss: 0.09758486568088438\n",
            "Iteration 19550 - Loss: 0.09756923595246929\n",
            "Iteration 19560 - Loss: 0.09755362042028248\n",
            "Iteration 19570 - Loss: 0.09753801906399236\n",
            "Iteration 19580 - Loss: 0.09752243186330675\n",
            "Iteration 19590 - Loss: 0.09750685879797308\n",
            "Iteration 19600 - Loss: 0.09749129984777792\n",
            "Iteration 19610 - Loss: 0.09747575499254742\n",
            "Iteration 19620 - Loss: 0.0974602242121467\n",
            "Iteration 19630 - Loss: 0.09744470748647993\n",
            "Iteration 19640 - Loss: 0.09742920479549048\n",
            "Iteration 19650 - Loss: 0.09741371611916061\n",
            "Iteration 19660 - Loss: 0.09739824143751118\n",
            "Iteration 19670 - Loss: 0.09738278073060187\n",
            "Iteration 19680 - Loss: 0.09736733397853095\n",
            "Iteration 19690 - Loss: 0.0973519011614352\n",
            "Iteration 19700 - Loss: 0.09733648225949006\n",
            "Iteration 19710 - Loss: 0.09732107725290884\n",
            "Iteration 19720 - Loss: 0.09730568612194343\n",
            "Iteration 19730 - Loss: 0.0972903088468839\n",
            "Iteration 19740 - Loss: 0.09727494540805809\n",
            "Iteration 19750 - Loss: 0.09725959578583207\n",
            "Iteration 19760 - Loss: 0.09724425996060955\n",
            "Iteration 19770 - Loss: 0.09722893791283224\n",
            "Iteration 19780 - Loss: 0.09721362962297939\n",
            "Iteration 19790 - Loss: 0.09719833507156792\n",
            "Iteration 19800 - Loss: 0.09718305423915215\n",
            "Iteration 19810 - Loss: 0.09716778710632383\n",
            "Iteration 19820 - Loss: 0.09715253365371221\n",
            "Iteration 19830 - Loss: 0.09713729386198353\n",
            "Iteration 19840 - Loss: 0.09712206771184131\n",
            "Iteration 19850 - Loss: 0.09710685518402604\n",
            "Iteration 19860 - Loss: 0.09709165625931516\n",
            "Iteration 19870 - Loss: 0.09707647091852312\n",
            "Iteration 19880 - Loss: 0.09706129914250104\n",
            "Iteration 19890 - Loss: 0.09704614091213676\n",
            "Iteration 19900 - Loss: 0.09703099620835462\n",
            "Iteration 19910 - Loss: 0.09701586501211563\n",
            "Iteration 19920 - Loss: 0.09700074730441714\n",
            "Iteration 19930 - Loss: 0.09698564306629294\n",
            "Iteration 19940 - Loss: 0.0969705522788129\n",
            "Iteration 19950 - Loss: 0.09695547492308317\n",
            "Iteration 19960 - Loss: 0.09694041098024593\n",
            "Iteration 19970 - Loss: 0.09692536043147944\n",
            "Iteration 19980 - Loss: 0.09691032325799771\n",
            "Iteration 19990 - Loss: 0.0968952994410508\n",
            "Iteration 20000 - Loss: 0.0968802889619242\n",
            "Iteration 20010 - Loss: 0.09686529180193923\n",
            "Iteration 20020 - Loss: 0.09685030794245283\n",
            "Iteration 20030 - Loss: 0.09683533736485726\n",
            "Iteration 20040 - Loss: 0.09682038005058022\n",
            "Iteration 20050 - Loss: 0.09680543598108478\n",
            "Iteration 20060 - Loss: 0.09679050513786913\n",
            "Iteration 20070 - Loss: 0.09677558750246666\n",
            "Iteration 20080 - Loss: 0.09676068305644582\n",
            "Iteration 20090 - Loss: 0.09674579178141005\n",
            "Iteration 20100 - Loss: 0.0967309136589975\n",
            "Iteration 20110 - Loss: 0.09671604867088136\n",
            "Iteration 20120 - Loss: 0.09670119679876933\n",
            "Iteration 20130 - Loss: 0.09668635802440396\n",
            "Iteration 20140 - Loss: 0.09667153232956213\n",
            "Iteration 20150 - Loss: 0.09665671969605541\n",
            "Iteration 20160 - Loss: 0.0966419201057295\n",
            "Iteration 20170 - Loss: 0.09662713354046468\n",
            "Iteration 20180 - Loss: 0.0966123599821752\n",
            "Iteration 20190 - Loss: 0.09659759941280971\n",
            "Iteration 20200 - Loss: 0.09658285181435067\n",
            "Iteration 20210 - Loss: 0.09656811716881472\n",
            "Iteration 20220 - Loss: 0.09655339545825223\n",
            "Iteration 20230 - Loss: 0.09653868666474752\n",
            "Iteration 20240 - Loss: 0.09652399077041858\n",
            "Iteration 20250 - Loss: 0.09650930775741713\n",
            "Iteration 20260 - Loss: 0.09649463760792844\n",
            "Iteration 20270 - Loss: 0.09647998030417113\n",
            "Iteration 20280 - Loss: 0.09646533582839749\n",
            "Iteration 20290 - Loss: 0.09645070416289296\n",
            "Iteration 20300 - Loss: 0.09643608528997638\n",
            "Iteration 20310 - Loss: 0.09642147919199962\n",
            "Iteration 20320 - Loss: 0.09640688585134773\n",
            "Iteration 20330 - Loss: 0.09639230525043894\n",
            "Iteration 20340 - Loss: 0.09637773737172414\n",
            "Iteration 20350 - Loss: 0.0963631821976873\n",
            "Iteration 20360 - Loss: 0.09634863971084506\n",
            "Iteration 20370 - Loss: 0.09633410989374687\n",
            "Iteration 20380 - Loss: 0.09631959272897472\n",
            "Iteration 20390 - Loss: 0.09630508819914331\n",
            "Iteration 20400 - Loss: 0.0962905962868997\n",
            "Iteration 20410 - Loss: 0.09627611697492337\n",
            "Iteration 20420 - Loss: 0.09626165024592614\n",
            "Iteration 20430 - Loss: 0.09624719608265213\n",
            "Iteration 20440 - Loss: 0.09623275446787757\n",
            "Iteration 20450 - Loss: 0.09621832538441087\n",
            "Iteration 20460 - Loss: 0.09620390881509246\n",
            "Iteration 20470 - Loss: 0.09618950474279458\n",
            "Iteration 20480 - Loss: 0.09617511315042156\n",
            "Iteration 20490 - Loss: 0.0961607340209094\n",
            "Iteration 20500 - Loss: 0.09614636733722587\n",
            "Iteration 20510 - Loss: 0.09613201308237038\n",
            "Iteration 20520 - Loss: 0.09611767123937397\n",
            "Iteration 20530 - Loss: 0.09610334179129908\n",
            "Iteration 20540 - Loss: 0.0960890247212397\n",
            "Iteration 20550 - Loss: 0.0960747200123211\n",
            "Iteration 20560 - Loss: 0.09606042764769987\n",
            "Iteration 20570 - Loss: 0.09604614761056378\n",
            "Iteration 20580 - Loss: 0.09603187988413181\n",
            "Iteration 20590 - Loss: 0.09601762445165397\n",
            "Iteration 20600 - Loss: 0.09600338129641123\n",
            "Iteration 20610 - Loss: 0.09598915040171545\n",
            "Iteration 20620 - Loss: 0.09597493175090956\n",
            "Iteration 20630 - Loss: 0.09596072532736694\n",
            "Iteration 20640 - Loss: 0.09594653111449189\n",
            "Iteration 20650 - Loss: 0.09593234909571935\n",
            "Iteration 20660 - Loss: 0.09591817925451476\n",
            "Iteration 20670 - Loss: 0.09590402157437408\n",
            "Iteration 20680 - Loss: 0.09588987603882355\n",
            "Iteration 20690 - Loss: 0.09587574263142011\n",
            "Iteration 20700 - Loss: 0.09586162133575056\n",
            "Iteration 20710 - Loss: 0.09584751213543231\n",
            "Iteration 20720 - Loss: 0.09583341501411245\n",
            "Iteration 20730 - Loss: 0.0958193299554686\n",
            "Iteration 20740 - Loss: 0.09580525694320811\n",
            "Iteration 20750 - Loss: 0.09579119596106829\n",
            "Iteration 20760 - Loss: 0.09577714699281657\n",
            "Iteration 20770 - Loss: 0.09576311002224974\n",
            "Iteration 20780 - Loss: 0.09574908503319453\n",
            "Iteration 20790 - Loss: 0.09573507200950741\n",
            "Iteration 20800 - Loss: 0.09572107093507436\n",
            "Iteration 20810 - Loss: 0.09570708179381071\n",
            "Iteration 20820 - Loss: 0.09569310456966162\n",
            "Iteration 20830 - Loss: 0.0956791392466014\n",
            "Iteration 20840 - Loss: 0.09566518580863353\n",
            "Iteration 20850 - Loss: 0.0956512442397909\n",
            "Iteration 20860 - Loss: 0.09563731452413575\n",
            "Iteration 20870 - Loss: 0.09562339664575913\n",
            "Iteration 20880 - Loss: 0.09560949058878124\n",
            "Iteration 20890 - Loss: 0.09559559633735128\n",
            "Iteration 20900 - Loss: 0.0955817138756474\n",
            "Iteration 20910 - Loss: 0.09556784318787655\n",
            "Iteration 20920 - Loss: 0.09555398425827445\n",
            "Iteration 20930 - Loss: 0.09554013707110552\n",
            "Iteration 20940 - Loss: 0.09552630161066285\n",
            "Iteration 20950 - Loss: 0.09551247786126818\n",
            "Iteration 20960 - Loss: 0.09549866580727163\n",
            "Iteration 20970 - Loss: 0.09548486543305189\n",
            "Iteration 20980 - Loss: 0.09547107672301589\n",
            "Iteration 20990 - Loss: 0.09545729966159902\n",
            "Iteration 21000 - Loss: 0.09544353423326489\n",
            "Iteration 21010 - Loss: 0.09542978042250526\n",
            "Iteration 21020 - Loss: 0.09541603821384001\n",
            "Iteration 21030 - Loss: 0.09540230759181718\n",
            "Iteration 21040 - Loss: 0.09538858854101272\n",
            "Iteration 21050 - Loss: 0.09537488104603045\n",
            "Iteration 21060 - Loss: 0.09536118509150228\n",
            "Iteration 21070 - Loss: 0.0953475006620877\n",
            "Iteration 21080 - Loss: 0.09533382774247401\n",
            "Iteration 21090 - Loss: 0.09532016631737619\n",
            "Iteration 21100 - Loss: 0.09530651637153692\n",
            "Iteration 21110 - Loss: 0.09529287788972629\n",
            "Iteration 21120 - Loss: 0.09527925085674203\n",
            "Iteration 21130 - Loss: 0.09526563525740925\n",
            "Iteration 21140 - Loss: 0.0952520310765803\n",
            "Iteration 21150 - Loss: 0.09523843829913493\n",
            "Iteration 21160 - Loss: 0.09522485690998025\n",
            "Iteration 21170 - Loss: 0.09521128689405033\n",
            "Iteration 21180 - Loss: 0.09519772823630644\n",
            "Iteration 21190 - Loss: 0.095184180921737\n",
            "Iteration 21200 - Loss: 0.09517064493535729\n",
            "Iteration 21210 - Loss: 0.0951571202622096\n",
            "Iteration 21220 - Loss: 0.09514360688736306\n",
            "Iteration 21230 - Loss: 0.0951301047959137\n",
            "Iteration 21240 - Loss: 0.09511661397298415\n",
            "Iteration 21250 - Loss: 0.09510313440372381\n",
            "Iteration 21260 - Loss: 0.09508966607330872\n",
            "Iteration 21270 - Loss: 0.0950762089669415\n",
            "Iteration 21280 - Loss: 0.09506276306985117\n",
            "Iteration 21290 - Loss: 0.09504932836729338\n",
            "Iteration 21300 - Loss: 0.09503590484455005\n",
            "Iteration 21310 - Loss: 0.09502249248692941\n",
            "Iteration 21320 - Loss: 0.09500909127976602\n",
            "Iteration 21330 - Loss: 0.09499570120842063\n",
            "Iteration 21340 - Loss: 0.09498232225828022\n",
            "Iteration 21350 - Loss: 0.09496895441475774\n",
            "Iteration 21360 - Loss: 0.09495559766329215\n",
            "Iteration 21370 - Loss: 0.09494225198934854\n",
            "Iteration 21380 - Loss: 0.09492891737841788\n",
            "Iteration 21390 - Loss: 0.09491559381601682\n",
            "Iteration 21400 - Loss: 0.09490228128768813\n",
            "Iteration 21410 - Loss: 0.09488897977900003\n",
            "Iteration 21420 - Loss: 0.09487568927554652\n",
            "Iteration 21430 - Loss: 0.09486240976294734\n",
            "Iteration 21440 - Loss: 0.09484914122684758\n",
            "Iteration 21450 - Loss: 0.09483588365291815\n",
            "Iteration 21460 - Loss: 0.09482263702685508\n",
            "Iteration 21470 - Loss: 0.09480940133438\n",
            "Iteration 21480 - Loss: 0.09479617656123988\n",
            "Iteration 21490 - Loss: 0.09478296269320687\n",
            "Iteration 21500 - Loss: 0.09476975971607847\n",
            "Iteration 21510 - Loss: 0.09475656761567723\n",
            "Iteration 21520 - Loss: 0.09474338637785092\n",
            "Iteration 21530 - Loss: 0.09473021598847232\n",
            "Iteration 21540 - Loss: 0.09471705643343926\n",
            "Iteration 21550 - Loss: 0.09470390769867444\n",
            "Iteration 21560 - Loss: 0.0946907697701255\n",
            "Iteration 21570 - Loss: 0.09467764263376484\n",
            "Iteration 21580 - Loss: 0.09466452627558983\n",
            "Iteration 21590 - Loss: 0.09465142068162237\n",
            "Iteration 21600 - Loss: 0.09463832583790918\n",
            "Iteration 21610 - Loss: 0.09462524173052146\n",
            "Iteration 21620 - Loss: 0.09461216834555504\n",
            "Iteration 21630 - Loss: 0.09459910566913025\n",
            "Iteration 21640 - Loss: 0.09458605368739192\n",
            "Iteration 21650 - Loss: 0.09457301238650925\n",
            "Iteration 21660 - Loss: 0.09455998175267571\n",
            "Iteration 21670 - Loss: 0.09454696177210915\n",
            "Iteration 21680 - Loss: 0.09453395243105157\n",
            "Iteration 21690 - Loss: 0.09452095371576925\n",
            "Iteration 21700 - Loss: 0.09450796561255259\n",
            "Iteration 21710 - Loss: 0.09449498810771602\n",
            "Iteration 21720 - Loss: 0.09448202118759788\n",
            "Iteration 21730 - Loss: 0.09446906483856073\n",
            "Iteration 21740 - Loss: 0.09445611904699083\n",
            "Iteration 21750 - Loss: 0.09444318379929849\n",
            "Iteration 21760 - Loss: 0.09443025908191766\n",
            "Iteration 21770 - Loss: 0.09441734488130614\n",
            "Iteration 21780 - Loss: 0.09440444118394536\n",
            "Iteration 21790 - Loss: 0.09439154797634054\n",
            "Iteration 21800 - Loss: 0.09437866524502032\n",
            "Iteration 21810 - Loss: 0.09436579297653701\n",
            "Iteration 21820 - Loss: 0.09435293115746644\n",
            "Iteration 21830 - Loss: 0.0943400797744078\n",
            "Iteration 21840 - Loss: 0.09432723881398365\n",
            "Iteration 21850 - Loss: 0.09431440826283999\n",
            "Iteration 21860 - Loss: 0.09430158810764613\n",
            "Iteration 21870 - Loss: 0.09428877833509439\n",
            "Iteration 21880 - Loss: 0.0942759789319006\n",
            "Iteration 21890 - Loss: 0.09426318988480346\n",
            "Iteration 21900 - Loss: 0.09425041118056501\n",
            "Iteration 21910 - Loss: 0.09423764280596994\n",
            "Iteration 21920 - Loss: 0.09422488474782643\n",
            "Iteration 21930 - Loss: 0.09421213699296517\n",
            "Iteration 21940 - Loss: 0.09419939952823993\n",
            "Iteration 21950 - Loss: 0.09418667234052722\n",
            "Iteration 21960 - Loss: 0.09417395541672641\n",
            "Iteration 21970 - Loss: 0.0941612487437597\n",
            "Iteration 21980 - Loss: 0.09414855230857172\n",
            "Iteration 21990 - Loss: 0.0941358660981299\n",
            "Iteration 22000 - Loss: 0.09412319009942426\n",
            "Iteration 22010 - Loss: 0.09411052429946731\n",
            "Iteration 22020 - Loss: 0.09409786868529404\n",
            "Iteration 22030 - Loss: 0.09408522324396193\n",
            "Iteration 22040 - Loss: 0.09407258796255077\n",
            "Iteration 22050 - Loss: 0.09405996282816272\n",
            "Iteration 22060 - Loss: 0.09404734782792223\n",
            "Iteration 22070 - Loss: 0.09403474294897617\n",
            "Iteration 22080 - Loss: 0.09402214817849328\n",
            "Iteration 22090 - Loss: 0.09400956350366457\n",
            "Iteration 22100 - Loss: 0.09399698891170329\n",
            "Iteration 22110 - Loss: 0.0939844243898445\n",
            "Iteration 22120 - Loss: 0.09397186992534552\n",
            "Iteration 22130 - Loss: 0.09395932550548546\n",
            "Iteration 22140 - Loss: 0.09394679111756529\n",
            "Iteration 22150 - Loss: 0.09393426674890804\n",
            "Iteration 22160 - Loss: 0.09392175238685832\n",
            "Iteration 22170 - Loss: 0.09390924801878262\n",
            "Iteration 22180 - Loss: 0.09389675363206919\n",
            "Iteration 22190 - Loss: 0.09388426921412779\n",
            "Iteration 22200 - Loss: 0.09387179475239005\n",
            "Iteration 22210 - Loss: 0.09385933023430885\n",
            "Iteration 22220 - Loss: 0.09384687564735898\n",
            "Iteration 22230 - Loss: 0.0938344309790364\n",
            "Iteration 22240 - Loss: 0.09382199621685859\n",
            "Iteration 22250 - Loss: 0.09380957134836442\n",
            "Iteration 22260 - Loss: 0.09379715636111427\n",
            "Iteration 22270 - Loss: 0.09378475124268948\n",
            "Iteration 22280 - Loss: 0.09377235598069295\n",
            "Iteration 22290 - Loss: 0.09375997056274857\n",
            "Iteration 22300 - Loss: 0.0937475949765016\n",
            "Iteration 22310 - Loss: 0.09373522920961824\n",
            "Iteration 22320 - Loss: 0.09372287324978583\n",
            "Iteration 22330 - Loss: 0.0937105270847127\n",
            "Iteration 22340 - Loss: 0.09369819070212815\n",
            "Iteration 22350 - Loss: 0.09368586408978243\n",
            "Iteration 22360 - Loss: 0.09367354723544669\n",
            "Iteration 22370 - Loss: 0.09366124012691299\n",
            "Iteration 22380 - Loss: 0.09364894275199401\n",
            "Iteration 22390 - Loss: 0.0936366550985233\n",
            "Iteration 22400 - Loss: 0.09362437715435507\n",
            "Iteration 22410 - Loss: 0.09361210890736436\n",
            "Iteration 22420 - Loss: 0.09359985034544652\n",
            "Iteration 22430 - Loss: 0.0935876014565177\n",
            "Iteration 22440 - Loss: 0.09357536222851455\n",
            "Iteration 22450 - Loss: 0.0935631326493942\n",
            "Iteration 22460 - Loss: 0.09355091270713412\n",
            "Iteration 22470 - Loss: 0.09353870238973233\n",
            "Iteration 22480 - Loss: 0.09352650168520708\n",
            "Iteration 22490 - Loss: 0.09351431058159704\n",
            "Iteration 22500 - Loss: 0.09350212906696106\n",
            "Iteration 22510 - Loss: 0.09348995712937831\n",
            "Iteration 22520 - Loss: 0.09347779475694803\n",
            "Iteration 22530 - Loss: 0.09346564193778963\n",
            "Iteration 22540 - Loss: 0.09345349866004268\n",
            "Iteration 22550 - Loss: 0.0934413649118667\n",
            "Iteration 22560 - Loss: 0.09342924068144132\n",
            "Iteration 22570 - Loss: 0.09341712595696611\n",
            "Iteration 22580 - Loss: 0.0934050207266605\n",
            "Iteration 22590 - Loss: 0.09339292497876386\n",
            "Iteration 22600 - Loss: 0.0933808387015354\n",
            "Iteration 22610 - Loss: 0.09336876188325416\n",
            "Iteration 22620 - Loss: 0.09335669451221881\n",
            "Iteration 22630 - Loss: 0.09334463657674785\n",
            "Iteration 22640 - Loss: 0.09333258806517938\n",
            "Iteration 22650 - Loss: 0.09332054896587125\n",
            "Iteration 22660 - Loss: 0.09330851926720082\n",
            "Iteration 22670 - Loss: 0.09329649895756484\n",
            "Iteration 22680 - Loss: 0.09328448802537985\n",
            "Iteration 22690 - Loss: 0.0932724864590816\n",
            "Iteration 22700 - Loss: 0.09326049424712553\n",
            "Iteration 22710 - Loss: 0.0932485113779862\n",
            "Iteration 22720 - Loss: 0.09323653784015762\n",
            "Iteration 22730 - Loss: 0.09322457362215322\n",
            "Iteration 22740 - Loss: 0.09321261871250536\n",
            "Iteration 22750 - Loss: 0.09320067309976601\n",
            "Iteration 22760 - Loss: 0.09318873677250608\n",
            "Iteration 22770 - Loss: 0.09317680971931559\n",
            "Iteration 22780 - Loss: 0.09316489192880381\n",
            "Iteration 22790 - Loss: 0.09315298338959896\n",
            "Iteration 22800 - Loss: 0.09314108409034827\n",
            "Iteration 22810 - Loss: 0.09312919401971803\n",
            "Iteration 22820 - Loss: 0.09311731316639343\n",
            "Iteration 22830 - Loss: 0.09310544151907844\n",
            "Iteration 22840 - Loss: 0.09309357906649612\n",
            "Iteration 22850 - Loss: 0.09308172579738805\n",
            "Iteration 22860 - Loss: 0.09306988170051493\n",
            "Iteration 22870 - Loss: 0.09305804676465584\n",
            "Iteration 22880 - Loss: 0.09304622097860883\n",
            "Iteration 22890 - Loss: 0.09303440433119042\n",
            "Iteration 22900 - Loss: 0.09302259681123591\n",
            "Iteration 22910 - Loss: 0.09301079840759907\n",
            "Iteration 22920 - Loss: 0.09299900910915224\n",
            "Iteration 22930 - Loss: 0.09298722890478624\n",
            "Iteration 22940 - Loss: 0.09297545778341045\n",
            "Iteration 22950 - Loss: 0.09296369573395254\n",
            "Iteration 22960 - Loss: 0.09295194274535856\n",
            "Iteration 22970 - Loss: 0.09294019880659314\n",
            "Iteration 22980 - Loss: 0.09292846390663892\n",
            "Iteration 22990 - Loss: 0.09291673803449703\n",
            "Iteration 23000 - Loss: 0.09290502117918664\n",
            "Iteration 23010 - Loss: 0.09289331332974525\n",
            "Iteration 23020 - Loss: 0.09288161447522861\n",
            "Iteration 23030 - Loss: 0.0928699246047102\n",
            "Iteration 23040 - Loss: 0.09285824370728207\n",
            "Iteration 23050 - Loss: 0.09284657177205391\n",
            "Iteration 23060 - Loss: 0.09283490878815372\n",
            "Iteration 23070 - Loss: 0.09282325474472726\n",
            "Iteration 23080 - Loss: 0.09281160963093826\n",
            "Iteration 23090 - Loss: 0.09279997343596849\n",
            "Iteration 23100 - Loss: 0.09278834614901735\n",
            "Iteration 23110 - Loss: 0.09277672775930215\n",
            "Iteration 23120 - Loss: 0.09276511825605802\n",
            "Iteration 23130 - Loss: 0.09275351762853788\n",
            "Iteration 23140 - Loss: 0.09274192586601217\n",
            "Iteration 23150 - Loss: 0.09273034295776916\n",
            "Iteration 23160 - Loss: 0.09271876889311471\n",
            "Iteration 23170 - Loss: 0.09270720366137229\n",
            "Iteration 23180 - Loss: 0.09269564725188285\n",
            "Iteration 23190 - Loss: 0.09268409965400495\n",
            "Iteration 23200 - Loss: 0.09267256085711464\n",
            "Iteration 23210 - Loss: 0.09266103085060536\n",
            "Iteration 23220 - Loss: 0.09264950962388796\n",
            "Iteration 23230 - Loss: 0.09263799716639086\n",
            "Iteration 23240 - Loss: 0.09262649346755944\n",
            "Iteration 23250 - Loss: 0.09261499851685669\n",
            "Iteration 23260 - Loss: 0.09260351230376292\n",
            "Iteration 23270 - Loss: 0.09259203481777539\n",
            "Iteration 23280 - Loss: 0.09258056604840882\n",
            "Iteration 23290 - Loss: 0.09256910598519487\n",
            "Iteration 23300 - Loss: 0.0925576546176825\n",
            "Iteration 23310 - Loss: 0.09254621193543774\n",
            "Iteration 23320 - Loss: 0.09253477792804368\n",
            "Iteration 23330 - Loss: 0.09252335258510036\n",
            "Iteration 23340 - Loss: 0.09251193589622472\n",
            "Iteration 23350 - Loss: 0.09250052785105098\n",
            "Iteration 23360 - Loss: 0.09248912843922999\n",
            "Iteration 23370 - Loss: 0.09247773765042953\n",
            "Iteration 23380 - Loss: 0.09246635547433431\n",
            "Iteration 23390 - Loss: 0.09245498190064579\n",
            "Iteration 23400 - Loss: 0.09244361691908225\n",
            "Iteration 23410 - Loss: 0.09243226051937871\n",
            "Iteration 23420 - Loss: 0.09242091269128679\n",
            "Iteration 23430 - Loss: 0.09240957342457497\n",
            "Iteration 23440 - Loss: 0.09239824270902819\n",
            "Iteration 23450 - Loss: 0.09238692053444814\n",
            "Iteration 23460 - Loss: 0.09237560689065298\n",
            "Iteration 23470 - Loss: 0.09236430176747747\n",
            "Iteration 23480 - Loss: 0.09235300515477292\n",
            "Iteration 23490 - Loss: 0.0923417170424069\n",
            "Iteration 23500 - Loss: 0.09233043742026374\n",
            "Iteration 23510 - Loss: 0.0923191662782439\n",
            "Iteration 23520 - Loss: 0.09230790360626427\n",
            "Iteration 23530 - Loss: 0.09229664939425825\n",
            "Iteration 23540 - Loss: 0.09228540363217524\n",
            "Iteration 23550 - Loss: 0.09227416630998121\n",
            "Iteration 23560 - Loss: 0.09226293741765816\n",
            "Iteration 23570 - Loss: 0.09225171694520438\n",
            "Iteration 23580 - Loss: 0.0922405048826343\n",
            "Iteration 23590 - Loss: 0.09222930121997855\n",
            "Iteration 23600 - Loss: 0.09221810594728377\n",
            "Iteration 23610 - Loss: 0.0922069190546128\n",
            "Iteration 23620 - Loss: 0.09219574053204427\n",
            "Iteration 23630 - Loss: 0.09218457036967315\n",
            "Iteration 23640 - Loss: 0.09217340855761014\n",
            "Iteration 23650 - Loss: 0.09216225508598193\n",
            "Iteration 23660 - Loss: 0.09215110994493123\n",
            "Iteration 23670 - Loss: 0.09213997312461641\n",
            "Iteration 23680 - Loss: 0.09212884461521192\n",
            "Iteration 23690 - Loss: 0.09211772440690796\n",
            "Iteration 23700 - Loss: 0.09210661248991028\n",
            "Iteration 23710 - Loss: 0.09209550885444076\n",
            "Iteration 23720 - Loss: 0.09208441349073669\n",
            "Iteration 23730 - Loss: 0.09207332638905116\n",
            "Iteration 23740 - Loss: 0.09206224753965292\n",
            "Iteration 23750 - Loss: 0.09205117693282631\n",
            "Iteration 23760 - Loss: 0.09204011455887122\n",
            "Iteration 23770 - Loss: 0.09202906040810323\n",
            "Iteration 23780 - Loss: 0.09201801447085331\n",
            "Iteration 23790 - Loss: 0.09200697673746804\n",
            "Iteration 23800 - Loss: 0.09199594719830934\n",
            "Iteration 23810 - Loss: 0.09198492584375459\n",
            "Iteration 23820 - Loss: 0.09197391266419665\n",
            "Iteration 23830 - Loss: 0.09196290765004365\n",
            "Iteration 23840 - Loss: 0.09195191079171919\n",
            "Iteration 23850 - Loss: 0.09194092207966195\n",
            "Iteration 23860 - Loss: 0.0919299415043262\n",
            "Iteration 23870 - Loss: 0.0919189690561812\n",
            "Iteration 23880 - Loss: 0.09190800472571148\n",
            "Iteration 23890 - Loss: 0.09189704850341679\n",
            "Iteration 23900 - Loss: 0.09188610037981203\n",
            "Iteration 23910 - Loss: 0.09187516034542725\n",
            "Iteration 23920 - Loss: 0.09186422839080748\n",
            "Iteration 23930 - Loss: 0.09185330450651302\n",
            "Iteration 23940 - Loss: 0.09184238868311892\n",
            "Iteration 23950 - Loss: 0.09183148091121547\n",
            "Iteration 23960 - Loss: 0.09182058118140785\n",
            "Iteration 23970 - Loss: 0.09180968948431609\n",
            "Iteration 23980 - Loss: 0.0917988058105753\n",
            "Iteration 23990 - Loss: 0.09178793015083536\n",
            "Iteration 24000 - Loss: 0.091777062495761\n",
            "Iteration 24010 - Loss: 0.09176620283603189\n",
            "Iteration 24020 - Loss: 0.09175535116234226\n",
            "Iteration 24030 - Loss: 0.09174450746540139\n",
            "Iteration 24040 - Loss: 0.09173367173593297\n",
            "Iteration 24050 - Loss: 0.09172284396467573\n",
            "Iteration 24060 - Loss: 0.09171202414238284\n",
            "Iteration 24070 - Loss: 0.09170121225982213\n",
            "Iteration 24080 - Loss: 0.09169040830777621\n",
            "Iteration 24090 - Loss: 0.09167961227704212\n",
            "Iteration 24100 - Loss: 0.0916688241584315\n",
            "Iteration 24110 - Loss: 0.09165804394277052\n",
            "Iteration 24120 - Loss: 0.09164727162089989\n",
            "Iteration 24130 - Loss: 0.09163650718367465\n",
            "Iteration 24140 - Loss: 0.09162575062196439\n",
            "Iteration 24150 - Loss: 0.0916150019266533\n",
            "Iteration 24160 - Loss: 0.09160426108863946\n",
            "Iteration 24170 - Loss: 0.09159352809883575\n",
            "Iteration 24180 - Loss: 0.09158280294816923\n",
            "Iteration 24190 - Loss: 0.09157208562758126\n",
            "Iteration 24200 - Loss: 0.0915613761280274\n",
            "Iteration 24210 - Loss: 0.09155067444047744\n",
            "Iteration 24220 - Loss: 0.0915399805559157\n",
            "Iteration 24230 - Loss: 0.09152929446534028\n",
            "Iteration 24240 - Loss: 0.09151861615976352\n",
            "Iteration 24250 - Loss: 0.09150794563021213\n",
            "Iteration 24260 - Loss: 0.09149728286772656\n",
            "Iteration 24270 - Loss: 0.09148662786336165\n",
            "Iteration 24280 - Loss: 0.09147598060818617\n",
            "Iteration 24290 - Loss: 0.09146534109328279\n",
            "Iteration 24300 - Loss: 0.09145470930974837\n",
            "Iteration 24310 - Loss: 0.0914440852486936\n",
            "Iteration 24320 - Loss: 0.09143346890124306\n",
            "Iteration 24330 - Loss: 0.09142286025853542\n",
            "Iteration 24340 - Loss: 0.09141225931172305\n",
            "Iteration 24350 - Loss: 0.09140166605197225\n",
            "Iteration 24360 - Loss: 0.0913910804704631\n",
            "Iteration 24370 - Loss: 0.09138050255838953\n",
            "Iteration 24380 - Loss: 0.09136993230695921\n",
            "Iteration 24390 - Loss: 0.09135936970739354\n",
            "Iteration 24400 - Loss: 0.09134881475092771\n",
            "Iteration 24410 - Loss: 0.09133826742881046\n",
            "Iteration 24420 - Loss: 0.09132772773230426\n",
            "Iteration 24430 - Loss: 0.0913171956526853\n",
            "Iteration 24440 - Loss: 0.09130667118124319\n",
            "Iteration 24450 - Loss: 0.09129615430928124\n",
            "Iteration 24460 - Loss: 0.09128564502811638\n",
            "Iteration 24470 - Loss: 0.09127514332907893\n",
            "Iteration 24480 - Loss: 0.09126464920351277\n",
            "Iteration 24490 - Loss: 0.09125416264277518\n",
            "Iteration 24500 - Loss: 0.09124368363823704\n",
            "Iteration 24510 - Loss: 0.09123321218128246\n",
            "Iteration 24520 - Loss: 0.09122274826330914\n",
            "Iteration 24530 - Loss: 0.09121229187572805\n",
            "Iteration 24540 - Loss: 0.09120184300996335\n",
            "Iteration 24550 - Loss: 0.0911914016574529\n",
            "Iteration 24560 - Loss: 0.09118096780964743\n",
            "Iteration 24570 - Loss: 0.09117054145801118\n",
            "Iteration 24580 - Loss: 0.09116012259402156\n",
            "Iteration 24590 - Loss: 0.09114971120916919\n",
            "Iteration 24600 - Loss: 0.09113930729495782\n",
            "Iteration 24610 - Loss: 0.09112891084290466\n",
            "Iteration 24620 - Loss: 0.09111852184453952\n",
            "Iteration 24630 - Loss: 0.09110814029140572\n",
            "Iteration 24640 - Loss: 0.09109776617505956\n",
            "Iteration 24650 - Loss: 0.09108739948707038\n",
            "Iteration 24660 - Loss: 0.09107704021902056\n",
            "Iteration 24670 - Loss: 0.09106668836250549\n",
            "Iteration 24680 - Loss: 0.0910563439091335\n",
            "Iteration 24690 - Loss: 0.09104600685052601\n",
            "Iteration 24700 - Loss: 0.09103567717831715\n",
            "Iteration 24710 - Loss: 0.09102535488415411\n",
            "Iteration 24720 - Loss: 0.09101503995969695\n",
            "Iteration 24730 - Loss: 0.09100473239661855\n",
            "Iteration 24740 - Loss: 0.09099443218660455\n",
            "Iteration 24750 - Loss: 0.09098413932135359\n",
            "Iteration 24760 - Loss: 0.0909738537925769\n",
            "Iteration 24770 - Loss: 0.09096357559199857\n",
            "Iteration 24780 - Loss: 0.09095330471135533\n",
            "Iteration 24790 - Loss: 0.09094304114239672\n",
            "Iteration 24800 - Loss: 0.09093278487688483\n",
            "Iteration 24810 - Loss: 0.09092253590659458\n",
            "Iteration 24820 - Loss: 0.0909122942233134\n",
            "Iteration 24830 - Loss: 0.09090205981884131\n",
            "Iteration 24840 - Loss: 0.09089183268499096\n",
            "Iteration 24850 - Loss: 0.09088161281358752\n",
            "Iteration 24860 - Loss: 0.09087140019646885\n",
            "Iteration 24870 - Loss: 0.09086119482548516\n",
            "Iteration 24880 - Loss: 0.090850996692499\n",
            "Iteration 24890 - Loss: 0.09084080578938578\n",
            "Iteration 24900 - Loss: 0.09083062210803297\n",
            "Iteration 24910 - Loss: 0.09082044564034077\n",
            "Iteration 24920 - Loss: 0.09081027637822144\n",
            "Iteration 24930 - Loss: 0.09080011431359981\n",
            "Iteration 24940 - Loss: 0.09078995943841316\n",
            "Iteration 24950 - Loss: 0.09077981174461067\n",
            "Iteration 24960 - Loss: 0.09076967122415439\n",
            "Iteration 24970 - Loss: 0.09075953786901805\n",
            "Iteration 24980 - Loss: 0.09074941167118811\n",
            "Iteration 24990 - Loss: 0.09073929262266295\n",
            "Iteration 25000 - Loss: 0.09072918071545327\n",
            "Iteration 25010 - Loss: 0.0907190759415819\n",
            "Iteration 25020 - Loss: 0.09070897829308391\n",
            "Iteration 25030 - Loss: 0.09069888776200634\n",
            "Iteration 25040 - Loss: 0.09068880434040855\n",
            "Iteration 25050 - Loss: 0.09067872802036181\n",
            "Iteration 25060 - Loss: 0.09066865879394932\n",
            "Iteration 25070 - Loss: 0.09065859665326677\n",
            "Iteration 25080 - Loss: 0.09064854159042145\n",
            "Iteration 25090 - Loss: 0.09063849359753283\n",
            "Iteration 25100 - Loss: 0.09062845266673217\n",
            "Iteration 25110 - Loss: 0.09061841879016294\n",
            "Iteration 25120 - Loss: 0.09060839195998031\n",
            "Iteration 25130 - Loss: 0.09059837216835143\n",
            "Iteration 25140 - Loss: 0.09058835940745533\n",
            "Iteration 25150 - Loss: 0.09057835366948284\n",
            "Stopping training as loss change is smaller than the stopping threshold.\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.896\n",
            "Confusion Matrix:\n",
            "[[ 958    0    1    1    0    8    8    1    2    1]\n",
            " [   0 1104    3    3    1    1    5    0   18    0]\n",
            " [  13   21  887   20   11    0   17   21   37    5]\n",
            " [   5    7   25  894    3   29    5   17   17    8]\n",
            " [   0   10    7    1  911    2    7    5    5   34]\n",
            " [  16    8    2   36   19  741   21   13   25   11]\n",
            " [  11    8    9    0   17   18  895    0    0    0]\n",
            " [   4   26   19    7   11    1    1  923    0   36]\n",
            " [  15   28    9   22   21   31   14   19  800   15]\n",
            " [  16    9    1   16   57    9    0   43    6  852]]\n",
            "Close app?yes\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yes'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Read, Normalize and Split Data\n",
        "#########################################################################\n",
        "\n",
        "def load_and_process_data(file_name):\n",
        "    # Read data from CSV file and preprocess it\n",
        "    with open(file_name) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        X = []  # Initialize a list to store input data (pixel values)\n",
        "        y = []  # Initialize a list to store labels\n",
        "        for row in csv_reader:\n",
        "            y.append(int(row[0]))\n",
        "            temp = [float(i) / 255.0 for i in row[1:]]  # Normalize pixel values\n",
        "            X.append(temp)\n",
        "\n",
        "    # Convert data to NumPy arrays for further processing\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    # Normalize the pixel values using Min-Max scaling\n",
        "    '''\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Normalize the pixel values using Z-score (standardization)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "    # Add a bias term (1) to the input data\n",
        "    X = np.append(X, np.ones((X.shape[0], 1), np.float64), axis=1)\n",
        "\n",
        "    # Convert labels (y) to one-hot encoding\n",
        "    num_classes = len(np.unique(y))\n",
        "    y = np.eye(num_classes)[y]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Read Training Data\n",
        "X_train, y_train = load_and_process_data('mnist_train.csv')\n",
        "\n",
        "# Read Test Data\n",
        "X_test, y_test = load_and_process_data('mnist_test.csv')\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#   Logistic regression\n",
        "#########################################################################\n",
        "\n",
        "def compute_loss(y_true, y_pred, weights, lambda_reg):\n",
        "    # Compute the binary cross-entropy loss with L2 regularization\n",
        "    epsilon = 1e-9\n",
        "    y1 = y_true * np.log(y_pred + epsilon)\n",
        "    y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
        "    regularization_term = (lambda_reg / (2 * len(y_true))) * np.sum(weights ** 2)\n",
        "    return -np.mean(y1 + y2) + regularization_term\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Sigmoid activation function\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def feed_forward(X, weights, bias):\n",
        "    # Perform feedforward operation\n",
        "    z = np.dot(X, weights) + bias\n",
        "    A = sigmoid(z)\n",
        "    return A\n",
        "\n",
        "\n",
        "def fit(X, y, lr, lambda_reg, stopping_threshold):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    weights = np.zeros((n_features, n_classes))\n",
        "    bias = np.zeros(n_classes)\n",
        "    losses = []  # To store loss at each iteration\n",
        "\n",
        "    previous_loss = float('inf')  # Set to a large value initially\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        A = feed_forward(X, weights, bias)\n",
        "        loss = compute_loss(y, A, weights, lambda_reg)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if abs(loss - previous_loss) < stopping_threshold:\n",
        "            print(\"Stopping training as loss change is smaller than the stopping threshold.\")\n",
        "            break\n",
        "\n",
        "        dz = A - y\n",
        "        dw = (1 / n_samples) * (np.dot(X.T, dz) + lambda_reg * weights)\n",
        "        db = (1 / n_samples) * np.sum(dz, axis=0)\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        previous_loss = loss\n",
        "\n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration} - Loss: {loss}\")\n",
        "\n",
        "    return weights, bias, losses, iteration\n",
        "\n",
        "# Call the fit function with the stopping_threshold\n",
        "learning_rate = 0.001\n",
        "lambda_reg = 0.1\n",
        "stopping_threshold = 1e-6\n",
        "\n",
        "weights, bias, losses,n_iters = fit(X_train, y_train, learning_rate, lambda_reg, stopping_threshold)\n",
        "\n",
        "\n",
        "\n",
        "# Plot the loss over iterations\n",
        "plt.figure(1)\n",
        "plt.plot(range(n_iters), losses, '-g', label='Logistic Regression')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = feed_forward(X_test, weights, bias)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute and display the confusion matrix and test accuracy\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), predicted_classes)\n",
        "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm)) / np.sum(cm)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(np.array(cm))\n",
        "\n",
        "input('Close app?')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Hupso_TUQXuwsCbMeb6z8ss7nqaDsr8o",
      "authorship_tag": "ABX9TyOm0BUz2ZotYlNb2mYEXpXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}